{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c071de1c-5e46-46e3-b98a-99b1f69593f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HTTP_PROXY\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"HTTPS_PROXY\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"http_proxy\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"https_proxy\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"ftp_proxy\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8870798-fe67-4265-8590-2e5415dff7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.12.2 when it was built against 1.12.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"My dog really wanted to do this\\n\\nBut he couldn't. He was so depressed.\\n\"},\n",
       " {'generated_text': 'My dog really wanted to call him a cheetah. [The dog] told her that che'},\n",
       " {'generated_text': 'My dog really wanted to be with me and she had a good time with me,\" Kipnis'},\n",
       " {'generated_text': \"My dog really wanted to keep him, because I thought he'd really love me, or just give\"},\n",
       " {'generated_text': 'My dog really wanted to walk me to my room and I thought about him every day, the same'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM, AutoConfig\n",
    "input_text = \"My dog really wanted to\"\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
    "generator(input_text, max_length=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb41fbe-149c-4183-b113-300b55c0cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/tnguyent/.local/lib/python3.10/site-packages (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from accelerate) (0.16.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from accelerate) (0.3.3)\n",
      "Requirement already satisfied: filelock in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42614b10-bb97-4b80-951a-196865a6d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128) \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4398dec0-202a-45e0-8a17-347eb16ab81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/home/tnguyent/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.3.18, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset('dataset/train_input.txt','dataset/test_input.txt', tokenizer)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 40, # Number of update steps between two evaluations.\n",
    "    save_steps=80, # after # steps model is saved \n",
    "    warmup_steps=50,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e906bb-e214-41f2-b6a7-35b2093e63f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('openai-community/gpt2')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6571802-7d89-475c-9e81-89f18760a464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14f75d3a59b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1376f8-dd6b-4dc5-951f-df02fa2e4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 32 # channels\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0723d85b-9923-4bc1-8c64-a60ffec59efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Here we want the wei to be data dependent - ie gather info from the past but in a data dependant way\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16) # each token here (totally B*T) produce a key and query in parallel and independently\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x)\n",
    "\n",
    "wei =  q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T). #\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and normalize giving a nice distibution that sums to 1 and\n",
    "                             # now it tells us that in a data dependent manner how much of info to aggregate from\n",
    "\n",
    "out = wei @ v # aggregate the attention scores and value vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a443de6b-a5b0-4f88-9e6e-4b7d06fff9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0618, -0.0091, -0.3488,  0.3208,  0.2971, -0.1573, -0.0561,  0.1068,\n",
      "          0.0368,  0.0139, -0.0017,  0.3110,  0.1404, -0.0158,  0.1853,  0.4290],\n",
      "        [ 0.1578, -0.0971, -0.4256,  0.3538,  0.3621, -0.2392, -0.0536,  0.1759,\n",
      "          0.1115,  0.0282, -0.0649,  0.3641,  0.1928,  0.0261,  0.2162,  0.3758],\n",
      "        [ 0.1293,  0.0759, -0.2946,  0.2292,  0.2215, -0.0710, -0.0107,  0.1616,\n",
      "         -0.0930, -0.0877,  0.0567,  0.1899,  0.0311, -0.0894,  0.0309,  0.5471],\n",
      "        [ 0.1247,  0.1400, -0.2436,  0.1819,  0.1976,  0.0338, -0.0028,  0.1124,\n",
      "         -0.1477, -0.0748,  0.0650,  0.1392, -0.0314, -0.0989,  0.0613,  0.5433],\n",
      "        [ 0.0667,  0.1845, -0.2135,  0.2813,  0.2064,  0.0873,  0.0084,  0.2055,\n",
      "         -0.1130, -0.1466,  0.0459,  0.1923, -0.0275, -0.1107,  0.0065,  0.4674],\n",
      "        [ 0.1924,  0.1693, -0.1568,  0.2284,  0.1620,  0.0737,  0.0443,  0.2519,\n",
      "         -0.1912, -0.1979,  0.0832,  0.0713, -0.0826, -0.0848, -0.1047,  0.6089],\n",
      "        [ 0.1184,  0.0884, -0.2652,  0.2560,  0.1840,  0.0284, -0.0621,  0.1181,\n",
      "         -0.0880,  0.0104,  0.1123,  0.1850,  0.0369, -0.0730,  0.0663,  0.5242],\n",
      "        [ 0.1243,  0.0453, -0.3412,  0.2709,  0.2335, -0.0948, -0.0421,  0.2143,\n",
      "         -0.0330, -0.0313,  0.0520,  0.2378,  0.1084, -0.0959,  0.0300,  0.4707]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08046848-b0c6-4f53-b980-141c8b795577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bertviz in /home/tnguyent/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: transformers>=2.0 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from bertviz) (4.34.0)\n",
      "Requirement already satisfied: torch>=1.0 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from bertviz) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from bertviz) (4.65.0)\n",
      "Requirement already satisfied: boto3 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from bertviz) (1.28.60)\n",
      "Requirement already satisfied: requests in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from bertviz) (2.31.0)\n",
      "Requirement already satisfied: regex in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from bertviz) (2023.10.3)\n",
      "Requirement already satisfied: sentencepiece in /home/tnguyent/.local/lib/python3.10/site-packages (from bertviz) (0.2.0)\n",
      "Requirement already satisfied: filelock in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.0->bertviz) (4.10.0)\n",
      "Requirement already satisfied: sympy in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.0->bertviz) (1.12)\n",
      "Requirement already satisfied: networkx in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.1)\n",
      "Requirement already satisfied: jinja2 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.60 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from boto3->bertviz) (1.31.60)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from boto3->bertviz) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from boto3->bertviz) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from requests->bertviz) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from requests->bertviz) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from requests->bertviz) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from requests->bertviz) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.60->boto3->bertviz) (2.8.2)\n",
      "Requirement already satisfied: fsspec in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=2.0->bertviz) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from jinja2->torch>=1.0->bertviz) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /soft/datascience/conda/2023-10-04/mconda3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.60->boto3->bertviz) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa5616a-3a54-41d9-93fd-2adff08ac81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-df8d67b4ace847b4a67aa800cdd9cb83\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/**\n",
       " * @fileoverview Transformer Visualization D3 javascript code.\n",
       " *\n",
       " * Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n",
       " *\n",
       " * Change log:\n",
       " *\n",
       " * 02/01/19  Jesse Vig   Initial implementation\n",
       " * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n",
       " * 01/19/21  Jesse Vig   Support light/dark modes\n",
       " * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n",
       " * 05/03/21  Jesse Vig   Adjust visualization height dynamically\n",
       " * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n",
       " **/\n",
       "\n",
       "require.config({\n",
       "  paths: {\n",
       "      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
       "    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
       "  }\n",
       "});\n",
       "\n",
       "requirejs(['jquery', 'd3'], function($, d3) {\n",
       "\n",
       "        const params = {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.961219847202301, 0.038780149072408676, 0.0, 0.0, 0.0, 0.0], [0.7466979026794434, 0.11987314373254776, 0.1334289014339447, 0.0, 0.0, 0.0], [0.5885030031204224, 0.13792067766189575, 0.212137371301651, 0.06143897399306297, 0.0, 0.0], [0.6570857763290405, 0.08996301889419556, 0.12751281261444092, 0.08361563086509705, 0.041822850704193115, 0.0], [0.2728874385356903, 0.11203353852033615, 0.1663985401391983, 0.08467111736536026, 0.16952736675739288, 0.19448210299015045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010616563260555267, 0.9893833994865417, 0.0, 0.0, 0.0, 0.0], [0.0024677535984665155, 0.008448007516562939, 0.9890841841697693, 0.0, 0.0, 0.0], [0.0001232847134815529, 0.0018733182223513722, 0.013126976788043976, 0.9848763942718506, 0.0, 0.0], [0.0010669564362615347, 0.001136627048254013, 0.003034998197108507, 0.0015735096530988812, 0.9931879043579102, 0.0], [0.00019791982776951045, 0.0010528112761676311, 0.0015437351539731026, 0.0009642760851420462, 3.4924432839034125e-05, 0.9962062835693359]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47578439116477966, 0.524215579032898, 0.0, 0.0, 0.0, 0.0], [0.5906045436859131, 0.2486611008644104, 0.16073434054851532, 0.0, 0.0, 0.0], [0.5529289841651917, 0.18856702744960785, 0.14457571506500244, 0.11392831057310104, 0.0, 0.0], [0.45094072818756104, 0.16486799716949463, 0.17318038642406464, 0.11748014390468597, 0.09353074431419373, 0.0], [0.4257245659828186, 0.1732865273952484, 0.15651953220367432, 0.07022649794816971, 0.0808701142668724, 0.09337282180786133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6133623123168945, 0.38663768768310547, 0.0, 0.0, 0.0, 0.0], [0.06098509579896927, 0.03253461793065071, 0.9064802527427673, 0.0, 0.0, 0.0], [0.006717085838317871, 0.0004012881254311651, 0.7572958469390869, 0.23558568954467773, 0.0, 0.0], [0.03722766041755676, 0.002948855282738805, 0.10081092268228531, 0.04142269119620323, 0.8175898790359497, 0.0], [0.04989781975746155, 0.00030758307548239827, 0.0024198265746235847, 0.0034334994852542877, 0.0006823898293077946, 0.9432588815689087]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9489555954933167, 0.051044441759586334, 0.0, 0.0, 0.0, 0.0], [0.6821408867835999, 0.1395241767168045, 0.17833495140075684, 0.0, 0.0, 0.0], [0.20366324484348297, 0.05641487240791321, 0.06399301439523697, 0.6759288311004639, 0.0, 0.0], [0.3419547975063324, 0.06725440919399261, 0.07926183938980103, 0.1783619523048401, 0.3331669867038727, 0.0], [0.09464015811681747, 0.0074282134883105755, 0.006983973551541567, 0.0071843694895505905, 0.018724264577031136, 0.865039050579071]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33834606409072876, 0.6616539359092712, 0.0, 0.0, 0.0, 0.0], [0.07855993509292603, 0.006165449041873217, 0.9152746200561523, 0.0, 0.0, 0.0], [0.01677597686648369, 0.0004037705948576331, 0.003340460592880845, 0.9794798493385315, 0.0, 0.0], [0.027600426226854324, 0.00044415233423933387, 0.0006541680195368826, 0.0002266185765620321, 0.971074640750885, 0.0], [0.010248198173940182, 3.701553578139283e-05, 0.00016064041119534522, 2.7341819077264518e-05, 1.0187304724240676e-05, 0.98951655626297]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.982503354549408, 0.017496665939688683, 0.0, 0.0, 0.0, 0.0], [0.8874197006225586, 0.05467939004302025, 0.05790085718035698, 0.0, 0.0, 0.0], [0.6849910616874695, 0.1228068619966507, 0.04972026124596596, 0.14248186349868774, 0.0, 0.0], [0.6015856862068176, 0.09881888329982758, 0.07070108503103256, 0.16652540862560272, 0.06236903741955757, 0.0], [0.3232504427433014, 0.12567411363124847, 0.04432179778814316, 0.07076980918645859, 0.06606649607419968, 0.36991727352142334]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9191647171974182, 0.0808352455496788, 0.0, 0.0, 0.0, 0.0], [0.45986413955688477, 0.39703112840652466, 0.14310479164123535, 0.0, 0.0, 0.0], [0.3003872334957123, 0.22181738913059235, 0.38161516189575195, 0.09618020057678223, 0.0, 0.0], [0.18963925540447235, 0.1376371532678604, 0.20173484086990356, 0.23632164299488068, 0.23466713726520538, 0.0], [0.15410441160202026, 0.09489496797323227, 0.11902562528848648, 0.10277965664863586, 0.4317220449447632, 0.09747327119112015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.364999920129776, 0.6350001096725464, 0.0, 0.0, 0.0, 0.0], [0.24595215916633606, 0.5519201755523682, 0.20212766528129578, 0.0, 0.0, 0.0], [0.2721358835697174, 0.40738627314567566, 0.25186213850975037, 0.06861574947834015, 0.0, 0.0], [0.10242555290460587, 0.16683615744113922, 0.524804949760437, 0.05445462837815285, 0.15147870779037476, 0.0], [0.25029507279396057, 0.22198128700256348, 0.18899968266487122, 0.10677118599414825, 0.1303267478942871, 0.10162602365016937]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6990506649017334, 0.3009493350982666, 0.0, 0.0, 0.0, 0.0], [0.5107942819595337, 0.2948642075061798, 0.1943415403366089, 0.0, 0.0, 0.0], [0.4604707360267639, 0.2805190980434418, 0.19174803793430328, 0.0672621801495552, 0.0, 0.0], [0.37648412585258484, 0.21120662987232208, 0.20214538276195526, 0.10207021236419678, 0.10809355974197388, 0.0], [0.30138441920280457, 0.20456179976463318, 0.18250338733196259, 0.11019382625818253, 0.1629127413034439, 0.03844383731484413]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7131582498550415, 0.2868417799472809, 0.0, 0.0, 0.0, 0.0], [0.4058799147605896, 0.18063297867774963, 0.41348710656166077, 0.0, 0.0, 0.0], [0.265546053647995, 0.1698586493730545, 0.3358593285083771, 0.228736013174057, 0.0, 0.0], [0.31385406851768494, 0.1831669807434082, 0.14928358793258667, 0.05377671495079994, 0.29991865158081055, 0.0], [0.20466560125350952, 0.18731118738651276, 0.15959151089191437, 0.06381776183843613, 0.03642302006483078, 0.34819093346595764]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6586242914199829, 0.3413757383823395, 0.0, 0.0, 0.0, 0.0], [0.5917776226997375, 0.3160035014152527, 0.0922188088297844, 0.0, 0.0, 0.0], [0.5477152466773987, 0.23586955666542053, 0.061456020921468735, 0.1549593061208725, 0.0, 0.0], [0.4587061107158661, 0.22439992427825928, 0.07887422293424606, 0.0992034301161766, 0.13881628215312958, 0.0], [0.32743722200393677, 0.19600819051265717, 0.068057119846344, 0.0892510637640953, 0.11618079245090485, 0.20306548476219177]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9961552023887634, 0.0038448425475507975, 0.0, 0.0, 0.0, 0.0], [0.8594854474067688, 0.06906110048294067, 0.07145342975854874, 0.0, 0.0, 0.0], [0.3800053000450134, 0.04127567633986473, 0.5496612787246704, 0.029057776555418968, 0.0, 0.0], [0.21445226669311523, 0.05088742449879646, 0.4317440092563629, 0.25869303941726685, 0.044223275035619736, 0.0], [0.11175256222486496, 0.017593080177903175, 0.027507441118359566, 0.04086771607398987, 0.7754669785499573, 0.026812179014086723]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9285967946052551, 0.07140326499938965, 0.0, 0.0, 0.0, 0.0], [0.6077286005020142, 0.3121427297592163, 0.08012867718935013, 0.0, 0.0, 0.0], [0.4942909777164459, 0.28503698110580444, 0.11849315464496613, 0.10217894613742828, 0.0, 0.0], [0.4183879494667053, 0.23117904365062714, 0.0834062322974205, 0.11365949362516403, 0.1533672958612442, 0.0], [0.42215850949287415, 0.12917140126228333, 0.08740927278995514, 0.1016375944018364, 0.21230268478393555, 0.04732053726911545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9786475896835327, 0.02135237120091915, 0.0, 0.0, 0.0, 0.0], [0.7749121785163879, 0.06510371714830399, 0.15998409688472748, 0.0, 0.0, 0.0], [0.6484923362731934, 0.07483134418725967, 0.14751605689525604, 0.12916021049022675, 0.0, 0.0], [0.5224639773368835, 0.06921815127134323, 0.13823404908180237, 0.1110658198595047, 0.15901805460453033, 0.0], [0.3964517116546631, 0.07325823605060577, 0.12938153743743896, 0.1064242571592331, 0.14864002168178558, 0.1458442211151123]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5525906085968018, 0.44740936160087585, 0.0, 0.0, 0.0, 0.0], [0.5585009455680847, 0.2176259458065033, 0.22387312352657318, 0.0, 0.0, 0.0], [0.5143128633499146, 0.15964674949645996, 0.15491968393325806, 0.1711207628250122, 0.0, 0.0], [0.5039961338043213, 0.11401888728141785, 0.11974027007818222, 0.12552587687969208, 0.13671889901161194, 0.0], [0.5061842799186707, 0.08567393571138382, 0.08903021365404129, 0.09759818762540817, 0.1027572825551033, 0.11875619739294052]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9242545366287231, 0.07574543356895447, 0.0, 0.0, 0.0, 0.0], [0.8257425427436829, 0.07932533323764801, 0.09493216127157211, 0.0, 0.0, 0.0], [0.7306380271911621, 0.0857183039188385, 0.08043931424617767, 0.10320431739091873, 0.0, 0.0], [0.6383238434791565, 0.07886394113302231, 0.07815027981996536, 0.08758097141981125, 0.1170809343457222, 0.0], [0.5552157163619995, 0.07409121096134186, 0.06834889203310013, 0.07778600603342056, 0.09999319165945053, 0.12456497550010681]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8578913807868958, 0.14210854470729828, 0.0, 0.0, 0.0, 0.0], [0.6423038244247437, 0.166290283203125, 0.19140593707561493, 0.0, 0.0, 0.0], [0.5530979633331299, 0.10609274357557297, 0.07821257412433624, 0.26259663701057434, 0.0, 0.0], [0.40121692419052124, 0.12223611027002335, 0.1934729963541031, 0.14164622128009796, 0.14142780005931854, 0.0], [0.40212565660476685, 0.18450751900672913, 0.07516805827617645, 0.05849048122763634, 0.1444634348154068, 0.13524490594863892]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791558980941772, 0.020844051614403725, 0.0, 0.0, 0.0, 0.0], [0.8829841613769531, 0.06233249977231026, 0.05468335747718811, 0.0, 0.0, 0.0], [0.8105455040931702, 0.08617085963487625, 0.07321777194738388, 0.03006584383547306, 0.0, 0.0], [0.6819812059402466, 0.04990820586681366, 0.08296552300453186, 0.08369525521993637, 0.10144983977079391, 0.0], [0.4056689441204071, 0.07337666302919388, 0.08601408451795578, 0.061709366738796234, 0.13226434588432312, 0.2409665435552597]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9670190811157227, 0.03298088163137436, 0.0, 0.0, 0.0, 0.0], [0.8449064493179321, 0.0851450264453888, 0.06994850933551788, 0.0, 0.0, 0.0], [0.7123572826385498, 0.07896047830581665, 0.055410757660865784, 0.15327158570289612, 0.0, 0.0], [0.6402613520622253, 0.0739755630493164, 0.044393062591552734, 0.14322125911712646, 0.09814881533384323, 0.0], [0.5073903799057007, 0.07523059099912643, 0.07754647731781006, 0.11362491548061371, 0.13947951793670654, 0.08672808855772018]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8487569093704224, 0.1512431502342224, 0.0, 0.0, 0.0, 0.0], [0.8415648937225342, 0.12107233703136444, 0.03736274689435959, 0.0, 0.0, 0.0], [0.7505517601966858, 0.11348944902420044, 0.06179959326982498, 0.07415912300348282, 0.0, 0.0], [0.6614719033241272, 0.10242646187543869, 0.052934251725673676, 0.07529708743095398, 0.10787025839090347, 0.0], [0.6014202237129211, 0.11340376734733582, 0.05631929263472557, 0.07096721231937408, 0.10906282067298889, 0.04882663115859032]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9445484280586243, 0.05545158311724663, 0.0, 0.0, 0.0, 0.0], [0.8874568939208984, 0.05474215745925903, 0.0578010231256485, 0.0, 0.0, 0.0], [0.8281888961791992, 0.06895001977682114, 0.059034693986177444, 0.0438263975083828, 0.0, 0.0], [0.6429892778396606, 0.0674755647778511, 0.11629703640937805, 0.05417950078845024, 0.11905858665704727, 0.0], [0.7367823719978333, 0.056119054555892944, 0.06857288628816605, 0.034219540655612946, 0.0787537544965744, 0.02555238828063011]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002913394710049033, 0.9997085928916931, 0.0, 0.0, 0.0, 0.0], [0.0007981209782883525, 0.5288336873054504, 0.4703682065010071, 0.0, 0.0, 0.0], [0.0007648481405340135, 0.34519824385643005, 0.3085267245769501, 0.34551018476486206, 0.0, 0.0], [0.0010283143492415547, 0.241359144449234, 0.23320138454437256, 0.2555713355541229, 0.2688397467136383, 0.0], [0.0009746829164214432, 0.17789699137210846, 0.16743157804012299, 0.1858760118484497, 0.18734444677829742, 0.28047630190849304]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.824492871761322, 0.17550717294216156, 0.0, 0.0, 0.0, 0.0], [0.12386877834796906, 0.044499922543764114, 0.8316312432289124, 0.0, 0.0, 0.0], [0.07924355566501617, 0.01296587660908699, 0.0015277155907824636, 0.9062628149986267, 0.0, 0.0], [0.08806384354829788, 0.0213409923017025, 0.0028886159416288137, 0.002845379989594221, 0.884861171245575, 0.0], [0.09983218461275101, 0.03363388776779175, 0.0054999832063913345, 0.002433052286505699, 0.0015082412865012884, 0.8570926189422607]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9646892547607422, 0.03531072288751602, 0.0, 0.0, 0.0, 0.0], [0.7529157400131226, 0.08733473718166351, 0.15974950790405273, 0.0, 0.0, 0.0], [0.4202282726764679, 0.09195102006196976, 0.23549850285053253, 0.25232216715812683, 0.0, 0.0], [0.30848920345306396, 0.05908140912652016, 0.38391315937042236, 0.15659146010875702, 0.09192468225955963, 0.0], [0.44790443778038025, 0.04329312965273857, 0.0796918049454689, 0.11081931740045547, 0.22124572098255157, 0.09704558551311493]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.991096019744873, 0.008904009126126766, 0.0, 0.0, 0.0, 0.0], [0.9697675704956055, 0.026084503158926964, 0.004147922620177269, 0.0, 0.0, 0.0], [0.9082901477813721, 0.033206019550561905, 0.00942116230726242, 0.049082688987255096, 0.0, 0.0], [0.8949133157730103, 0.05544555187225342, 0.005577624775469303, 0.03150692582130432, 0.012556522153317928, 0.0], [0.8497740030288696, 0.028890123590826988, 0.0036647915840148926, 0.03751987963914871, 0.038427725434303284, 0.04172350466251373]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9984525442123413, 0.0015474462416023016, 0.0, 0.0, 0.0, 0.0], [0.48947831988334656, 0.4812193810939789, 0.029302269220352173, 0.0, 0.0, 0.0], [0.11772153526544571, 0.13121186196804047, 0.6702314615249634, 0.08083520829677582, 0.0, 0.0], [0.13043689727783203, 0.04068669304251671, 0.2652038037776947, 0.4114362895488739, 0.15223638713359833, 0.0], [0.12661904096603394, 0.03275119513273239, 0.03567872568964958, 0.06039190664887428, 0.6021825075149536, 0.1423766165971756]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9805176854133606, 0.019482342526316643, 0.0, 0.0, 0.0, 0.0], [0.7948849201202393, 0.12061909586191177, 0.08449601382017136, 0.0, 0.0, 0.0], [0.5612356066703796, 0.15743127465248108, 0.20339730381965637, 0.0779358446598053, 0.0, 0.0], [0.42583736777305603, 0.10742014646530151, 0.15123659372329712, 0.08755031228065491, 0.22795552015304565, 0.0], [0.24752654135227203, 0.024188270792365074, 0.03039524517953396, 0.08586956560611725, 0.5714336633682251, 0.040586672723293304]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887767434120178, 0.011223225854337215, 0.0, 0.0, 0.0, 0.0], [0.7572693228721619, 0.22317346930503845, 0.019557112827897072, 0.0, 0.0, 0.0], [0.5341880321502686, 0.22107566893100739, 0.1762184202671051, 0.06851787120103836, 0.0, 0.0], [0.17095312476158142, 0.0822940468788147, 0.576022207736969, 0.11097585409879684, 0.059754710644483566, 0.0], [0.2487109899520874, 0.08880793303251266, 0.08980197459459305, 0.09729334712028503, 0.4413093626499176, 0.03407646715641022]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8422133326530457, 0.15778663754463196, 0.0, 0.0, 0.0, 0.0], [0.468412846326828, 0.46105360984802246, 0.07053359597921371, 0.0, 0.0, 0.0], [0.2588140666484833, 0.4635888636112213, 0.18503506481647491, 0.09256205707788467, 0.0, 0.0], [0.18399578332901, 0.29154160618782043, 0.17031098902225494, 0.27173006534576416, 0.08242159336805344, 0.0], [0.1646990180015564, 0.2472696155309677, 0.08770562708377838, 0.22575001418590546, 0.1774536371231079, 0.09712201356887817]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919946193695068, 0.008005390875041485, 0.0, 0.0, 0.0, 0.0], [0.9068724513053894, 0.044065121561288834, 0.04906242713332176, 0.0, 0.0, 0.0], [0.8582221865653992, 0.055348269641399384, 0.040419407188892365, 0.046010036021471024, 0.0, 0.0], [0.7855252623558044, 0.041242364794015884, 0.08369296044111252, 0.04887620359659195, 0.040663279592990875, 0.0], [0.7856317162513733, 0.05014643445611, 0.04751267284154892, 0.027365952730178833, 0.05614755302667618, 0.03319567069411278]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9041035175323486, 0.09589648246765137, 0.0, 0.0, 0.0, 0.0], [0.5862312912940979, 0.07199832051992416, 0.34177035093307495, 0.0, 0.0, 0.0], [0.3878960907459259, 0.04660807177424431, 0.20278996229171753, 0.36270591616630554, 0.0, 0.0], [0.2665242552757263, 0.024533024057745934, 0.12211935967206955, 0.20041218400001526, 0.386411190032959, 0.0], [0.23357485234737396, 0.02053728699684143, 0.09610321372747421, 0.13062246143817902, 0.22990450263023376, 0.289257675409317]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639912247657776, 0.036008793860673904, 0.0, 0.0, 0.0, 0.0], [0.7075552344322205, 0.2542775869369507, 0.038167137652635574, 0.0, 0.0, 0.0], [0.2566526234149933, 0.20589298009872437, 0.01665665954351425, 0.5207977294921875, 0.0, 0.0], [0.1037939190864563, 0.04639088362455368, 0.008698614314198494, 0.7866851687431335, 0.05443140119314194, 0.0], [0.2214341163635254, 0.03379744663834572, 0.029023902490735054, 0.541292130947113, 0.15286092460155487, 0.021591555327177048]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891703724861145, 0.010829661041498184, 0.0, 0.0, 0.0, 0.0], [0.7913155555725098, 0.12309625744819641, 0.08558809012174606, 0.0, 0.0, 0.0], [0.2954600155353546, 0.15808308124542236, 0.4217240810394287, 0.1247328370809555, 0.0, 0.0], [0.23440983891487122, 0.09886523336172104, 0.33160170912742615, 0.1971396654844284, 0.1379835456609726, 0.0], [0.19728390872478485, 0.05741839483380318, 0.06909029185771942, 0.16469819843769073, 0.2797277867794037, 0.23178131878376007]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9359127879142761, 0.0640871673822403, 0.0, 0.0, 0.0, 0.0], [0.7888627648353577, 0.08673475682735443, 0.12440246343612671, 0.0, 0.0, 0.0], [0.6535118818283081, 0.07573551684617996, 0.09732568264007568, 0.17342689633369446, 0.0, 0.0], [0.522276759147644, 0.058278825134038925, 0.09920477122068405, 0.17020836472511292, 0.15003129839897156, 0.0], [0.4108840823173523, 0.047306034713983536, 0.07265672832727432, 0.10560744255781174, 0.10550004243850708, 0.25804558396339417]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9683833122253418, 0.03161672502756119, 0.0, 0.0, 0.0, 0.0], [0.8965396881103516, 0.038870569318532944, 0.06458976864814758, 0.0, 0.0, 0.0], [0.8264952898025513, 0.03213464096188545, 0.05196719989180565, 0.0894029513001442, 0.0, 0.0], [0.7718173265457153, 0.030402837321162224, 0.045827414840459824, 0.07118473201990128, 0.08076759427785873, 0.0], [0.7292331457138062, 0.021699821576476097, 0.033074747771024704, 0.04720093309879303, 0.06474557518959045, 0.10404567420482635]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9979567527770996, 0.0020432830788195133, 0.0, 0.0, 0.0, 0.0], [0.955294132232666, 0.00802531372755766, 0.03668047487735748, 0.0, 0.0, 0.0], [0.9254710078239441, 0.002755576279014349, 0.0020629852078855038, 0.06971040368080139, 0.0, 0.0], [0.8660576939582825, 0.0038883681409060955, 0.0006785982404835522, 0.0006981453043408692, 0.1286771297454834, 0.0], [0.8455929160118103, 0.0037804055027663708, 0.000253423087997362, 6.0270751419011503e-05, 0.00011820747749879956, 0.15019479393959045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9262455105781555, 0.07375453412532806, 0.0, 0.0, 0.0, 0.0], [0.7717157006263733, 0.16241952776908875, 0.06586471945047379, 0.0, 0.0, 0.0], [0.8167637586593628, 0.07807160913944244, 0.06324034929275513, 0.041924238204956055, 0.0, 0.0], [0.6867184638977051, 0.07755157351493835, 0.10056912153959274, 0.05955080687999725, 0.07561002671718597, 0.0], [0.6421161890029907, 0.11014898866415024, 0.07688194513320923, 0.054033469408750534, 0.10333634912967682, 0.013483096845448017]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9395954608917236, 0.060404520481824875, 0.0, 0.0, 0.0, 0.0], [0.23004619777202606, 0.6617380380630493, 0.1082158014178276, 0.0, 0.0, 0.0], [0.2670227289199829, 0.3607950508594513, 0.3249626159667969, 0.047219593077898026, 0.0, 0.0], [0.595201313495636, 0.12269274890422821, 0.06302059441804886, 0.08916817605495453, 0.12991715967655182, 0.0], [0.10284596681594849, 0.02938011661171913, 0.013739082030951977, 0.045860596001148224, 0.7698501348495483, 0.03832406550645828]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9040980935096741, 0.09590194374322891, 0.0, 0.0, 0.0, 0.0], [0.357237845659256, 0.6274612545967102, 0.015300876460969448, 0.0, 0.0, 0.0], [0.5917996764183044, 0.2764042019844055, 0.10476048290729523, 0.027035649865865707, 0.0, 0.0], [0.7254403829574585, 0.04983152449131012, 0.014982940629124641, 0.1778142899274826, 0.031930916011333466, 0.0], [0.7612743973731995, 0.06158972904086113, 0.005942251533269882, 0.01642685756087303, 0.1267806589603424, 0.0279861893504858]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947587847709656, 0.005241230130195618, 0.0, 0.0, 0.0, 0.0], [0.9632415771484375, 0.017816413193941116, 0.018942030146718025, 0.0, 0.0, 0.0], [0.9671078324317932, 0.008509586565196514, 0.00856222677975893, 0.015820473432540894, 0.0, 0.0], [0.9340996146202087, 0.011952387169003487, 0.02018021047115326, 0.02675083465874195, 0.0070168930105865, 0.0], [0.9587237238883972, 0.004657115787267685, 0.003326789475977421, 0.006545313633978367, 0.010182461701333523, 0.016564540565013885]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769991040229797, 0.023000910878181458, 0.0, 0.0, 0.0, 0.0], [0.7917609214782715, 0.1753319948911667, 0.032907065004110336, 0.0, 0.0, 0.0], [0.7949192523956299, 0.10531841963529587, 0.040218502283096313, 0.05954383686184883, 0.0, 0.0], [0.7097718715667725, 0.10552527755498886, 0.06597573310136795, 0.05765606462955475, 0.061070989817380905, 0.0], [0.7506601214408875, 0.026514461264014244, 0.021576043218374252, 0.034296683967113495, 0.08494450151920319, 0.08200812339782715]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983751654624939, 0.016248304396867752, 0.0, 0.0, 0.0, 0.0], [0.5615494847297668, 0.08956841379404068, 0.3488820493221283, 0.0, 0.0, 0.0], [0.32929039001464844, 0.024114903062582016, 0.5428059697151184, 0.10378880053758621, 0.0, 0.0], [0.34330207109451294, 0.01308644749224186, 0.5121983289718628, 0.11146228760480881, 0.019950881600379944, 0.0], [0.4792812764644623, 0.01733359508216381, 0.1180536150932312, 0.06130281835794449, 0.20071913301944733, 0.12330964207649231]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908847212791443, 0.009115329943597317, 0.0, 0.0, 0.0, 0.0], [0.5282707214355469, 0.3292262554168701, 0.1425030380487442, 0.0, 0.0, 0.0], [0.48788541555404663, 0.23368670046329498, 0.17578084766864777, 0.10264702141284943, 0.0, 0.0], [0.31444698572158813, 0.18065163493156433, 0.168714240193367, 0.09506598114967346, 0.24112118780612946, 0.0], [0.5168765187263489, 0.035897161811590195, 0.026188155636191368, 0.04039734974503517, 0.18791745603084564, 0.1927233189344406]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8750308156013489, 0.12496919929981232, 0.0, 0.0, 0.0, 0.0], [0.4550614655017853, 0.4900427758693695, 0.05489582195878029, 0.0, 0.0, 0.0], [0.2933720052242279, 0.5449907183647156, 0.09444297850131989, 0.06719419360160828, 0.0, 0.0], [0.489708811044693, 0.2720997631549835, 0.06861965358257294, 0.14694802463054657, 0.022623788565397263, 0.0], [0.4729066491127014, 0.08103099465370178, 0.016052134335041046, 0.30672287940979004, 0.10120721161365509, 0.022080255672335625]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9630220532417297, 0.03697792813181877, 0.0, 0.0, 0.0, 0.0], [0.7557195425033569, 0.16436372697353363, 0.07991670072078705, 0.0, 0.0, 0.0], [0.6947705745697021, 0.08409853279590607, 0.0638260766863823, 0.15730486810207367, 0.0, 0.0], [0.5821147561073303, 0.03297805413603783, 0.07936596870422363, 0.19441406428813934, 0.11112712323665619, 0.0], [0.5974540710449219, 0.04261096194386482, 0.06919723749160767, 0.14563441276550293, 0.12481734901666641, 0.020285936072468758]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957822561264038, 0.004217816516757011, 0.0, 0.0, 0.0, 0.0], [0.9312832951545715, 0.010560247115790844, 0.05815650522708893, 0.0, 0.0, 0.0], [0.8435326814651489, 0.015695005655288696, 0.045751139521598816, 0.09502115100622177, 0.0, 0.0], [0.772409975528717, 0.011981245130300522, 0.03504609689116478, 0.03876771405339241, 0.14179500937461853, 0.0], [0.7642908692359924, 0.009868789464235306, 0.00812275055795908, 0.013314393348991871, 0.04824395477771759, 0.15615922212600708]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9701177477836609, 0.02988232672214508, 0.0, 0.0, 0.0, 0.0], [0.6564007997512817, 0.22506150603294373, 0.11853761970996857, 0.0, 0.0, 0.0], [0.6958062648773193, 0.14701850712299347, 0.07145983725786209, 0.08571550250053406, 0.0, 0.0], [0.6353274583816528, 0.1346064656972885, 0.030994214117527008, 0.056916315108537674, 0.1421555131673813, 0.0], [0.6779401898384094, 0.053654152899980545, 0.01800631172955036, 0.06284520775079727, 0.1103820651769638, 0.07717210054397583]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822334051132202, 0.017766647040843964, 0.0, 0.0, 0.0, 0.0], [0.9037663340568542, 0.06541544198989868, 0.03081829659640789, 0.0, 0.0, 0.0], [0.8119193911552429, 0.03679030388593674, 0.060560714453458786, 0.09072960168123245, 0.0, 0.0], [0.40546438097953796, 0.10383912175893784, 0.10211236774921417, 0.35434210300445557, 0.03424208238720894, 0.0], [0.22824221849441528, 0.017278727144002914, 0.05055465176701546, 0.6015752553939819, 0.09411764144897461, 0.008231506682932377]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873148202896118, 0.012685136869549751, 0.0, 0.0, 0.0, 0.0], [0.35445743799209595, 0.5317603349685669, 0.11378221958875656, 0.0, 0.0, 0.0], [0.07823363691568375, 0.7221359014511108, 0.10936623811721802, 0.090264230966568, 0.0, 0.0], [0.21967869997024536, 0.4048435091972351, 0.12358088046312332, 0.20018866658210754, 0.051708199083805084, 0.0], [0.36089760065078735, 0.10459021478891373, 0.06983799487352371, 0.2976483404636383, 0.13869903981685638, 0.02832675166428089]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732162356376648, 0.0267837755382061, 0.0, 0.0, 0.0, 0.0], [0.9167553782463074, 0.061452705413103104, 0.02179192565381527, 0.0, 0.0, 0.0], [0.8543081283569336, 0.08049600571393967, 0.030334919691085815, 0.03486092761158943, 0.0, 0.0], [0.8919214606285095, 0.04280779883265495, 0.022045055404305458, 0.023470671847462654, 0.01975487545132637, 0.0], [0.8116763234138489, 0.03413533419370651, 0.03567665070295334, 0.04748587682843208, 0.0253971628844738, 0.04562860727310181]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502761960029602, 0.04972382262349129, 0.0, 0.0, 0.0, 0.0], [0.7637454271316528, 0.2007361352443695, 0.03551840782165527, 0.0, 0.0, 0.0], [0.6279097199440002, 0.03768139332532883, 0.1994536966085434, 0.13495522737503052, 0.0, 0.0], [0.6397060751914978, 0.027007432654500008, 0.09082036465406418, 0.20653828978538513, 0.03592785820364952, 0.0], [0.4559425115585327, 0.021641194820404053, 0.12939567863941193, 0.21800927817821503, 0.10379841923713684, 0.07121295481920242]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498406648635864, 0.050159383565187454, 0.0, 0.0, 0.0, 0.0], [0.8688724637031555, 0.0872218981385231, 0.043905653059482574, 0.0, 0.0, 0.0], [0.6937950253486633, 0.06359200924634933, 0.091790571808815, 0.15082231163978577, 0.0, 0.0], [0.7266597151756287, 0.04389883577823639, 0.04683985933661461, 0.09851823002099991, 0.08408336341381073, 0.0], [0.7848998308181763, 0.037147827446460724, 0.012907838448882103, 0.01053939200937748, 0.12079165875911713, 0.03371351957321167]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891054034233093, 0.01089458167552948, 0.0, 0.0, 0.0, 0.0], [0.8929519653320312, 0.08700055629014969, 0.02004752680659294, 0.0, 0.0, 0.0], [0.7891124486923218, 0.09797251224517822, 0.08633202314376831, 0.026582980528473854, 0.0, 0.0], [0.8850635886192322, 0.03645012155175209, 0.05395457148551941, 0.01237727515399456, 0.012154522351920605, 0.0], [0.6861329674720764, 0.05720378831028938, 0.011636304669082165, 0.021660611033439636, 0.1748800277709961, 0.048486363142728806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9396191835403442, 0.06038080155849457, 0.0, 0.0, 0.0, 0.0], [0.7851794958114624, 0.19751444458961487, 0.017306052148342133, 0.0, 0.0, 0.0], [0.7660509943962097, 0.15444670617580414, 0.03188290074467659, 0.04761936888098717, 0.0, 0.0], [0.703522801399231, 0.05171430483460426, 0.07760990411043167, 0.1533905267715454, 0.013762423768639565, 0.0], [0.7121888399124146, 0.04994234815239906, 0.03772548958659172, 0.08649132400751114, 0.06541401147842407, 0.04823806509375572]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.974072277545929, 0.025927715003490448, 0.0, 0.0, 0.0, 0.0], [0.792539656162262, 0.01171559002250433, 0.19574476778507233, 0.0, 0.0, 0.0], [0.5106770992279053, 0.007296787109225988, 0.039619915187358856, 0.4424062669277191, 0.0, 0.0], [0.5862472057342529, 0.012099712155759335, 0.024585209786891937, 0.06737840175628662, 0.30968940258026123, 0.0], [0.30196306109428406, 0.007724012713879347, 0.011518122628331184, 0.046947259455919266, 0.22146707773208618, 0.41038045287132263]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9744554162025452, 0.02554464340209961, 0.0, 0.0, 0.0, 0.0], [0.9769195318222046, 0.015048524364829063, 0.008031901903450489, 0.0, 0.0, 0.0], [0.9060619473457336, 0.025875424966216087, 0.025954782962799072, 0.04210779070854187, 0.0, 0.0], [0.9400081038475037, 0.00555665697902441, 0.005828304681926966, 0.031757812947034836, 0.016849134117364883, 0.0], [0.9105738401412964, 0.0019752182997763157, 0.008646721951663494, 0.013360846787691116, 0.03543964773416519, 0.030003678053617477]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791666865348816, 0.020833350718021393, 0.0, 0.0, 0.0, 0.0], [0.8444858193397522, 0.13507869839668274, 0.020435383543372154, 0.0, 0.0, 0.0], [0.7903086543083191, 0.14559169113636017, 0.037529975175857544, 0.026569725945591927, 0.0, 0.0], [0.7298924326896667, 0.056496407836675644, 0.032735615968704224, 0.10400459170341492, 0.07687094807624817, 0.0], [0.5684185028076172, 0.04388832300901413, 0.026293467730283737, 0.0811714455485344, 0.24314835667610168, 0.037079911679029465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9499868154525757, 0.05001320689916611, 0.0, 0.0, 0.0, 0.0], [0.9336170554161072, 0.05848868936300278, 0.007894262671470642, 0.0, 0.0, 0.0], [0.7897834181785583, 0.11071821302175522, 0.05360178276896477, 0.04589657858014107, 0.0, 0.0], [0.885930061340332, 0.05752986669540405, 0.01374326553195715, 0.0033877466339617968, 0.03940902277827263, 0.0], [0.9337607622146606, 0.02647063508629799, 0.004523396957665682, 0.0061904797330498695, 0.014132906682789326, 0.014921708963811398]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.8521224554035598e-09, 0.0, 0.0, 0.0, 0.0], [6.6758907451003324e-06, 0.9999804496765137, 1.2841281204600818e-05, 0.0, 0.0, 0.0], [2.2194194926328237e-08, 2.6684581211355862e-09, 0.9999971389770508, 2.8136880700913025e-06, 0.0, 0.0], [1.0145409987671883e-06, 4.464065739284706e-08, 0.00035356366424821317, 0.9993677735328674, 0.0002776293840724975, 0.0], [9.436550429953172e-10, 1.382057315812979e-11, 5.017835036369434e-10, 2.965183876213473e-09, 0.9999971389770508, 2.8644042231462663e-06]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9948632121086121, 0.005136783700436354, 0.0, 0.0, 0.0, 0.0], [0.9274215698242188, 0.01832387037575245, 0.05425456911325455, 0.0, 0.0, 0.0], [0.9678993225097656, 0.004143435508012772, 0.004314453341066837, 0.023642776533961296, 0.0, 0.0], [0.8999068737030029, 0.001467161695472896, 0.00029133574571460485, 0.002585014794021845, 0.09574954956769943, 0.0], [0.9386115670204163, 0.00022248300956562161, 0.0006146665546111763, 0.0015495637198910117, 0.030689461156725883, 0.028312424197793007]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999959468841553, 4.042720775032649e-06, 0.0, 0.0, 0.0, 0.0], [0.9982761144638062, 3.2613831990602193e-06, 0.001720669330097735, 0.0, 0.0, 0.0], [0.9998809099197388, 5.328835683826583e-08, 6.376215537784446e-07, 0.00011847059795400128, 0.0, 0.0], [0.9996154308319092, 3.473169556400535e-07, 3.8920820344401363e-08, 4.468433303372876e-07, 0.00038369710091501474, 0.0], [0.9994840621948242, 1.655020476221125e-08, 2.8715557931491276e-08, 1.0638284493325045e-06, 0.0002126671897713095, 0.00030212008277885616]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514135718345642, 0.048586405813694, 0.0, 0.0, 0.0, 0.0], [0.5749948024749756, 0.39028096199035645, 0.03472418338060379, 0.0, 0.0, 0.0], [0.7442318201065063, 0.1752411425113678, 0.0756477490067482, 0.004879283253103495, 0.0, 0.0], [0.5232070684432983, 0.09429339319467545, 0.1138191670179367, 0.19979268312454224, 0.06888769567012787, 0.0], [0.47472575306892395, 0.05636607110500336, 0.04530389606952667, 0.06967321783304214, 0.3098014295101166, 0.0441296212375164]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8734648823738098, 0.12653514742851257, 0.0, 0.0, 0.0, 0.0], [0.6097912788391113, 0.3541727066040039, 0.036036062985658646, 0.0, 0.0, 0.0], [0.45984190702438354, 0.38697871565818787, 0.0996011346578598, 0.05357823893427849, 0.0, 0.0], [0.572220504283905, 0.23636263608932495, 0.08344558626413345, 0.06921917200088501, 0.03875211998820305, 0.0], [0.5143564343452454, 0.16723087430000305, 0.09019406139850616, 0.0765448659658432, 0.10578085482120514, 0.04589281603693962]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.981228768825531, 0.018771231174468994, 0.0, 0.0, 0.0, 0.0], [0.6142941117286682, 0.3503977954387665, 0.0353081189095974, 0.0, 0.0, 0.0], [0.5770686268806458, 0.32858458161354065, 0.05508256331086159, 0.03926428034901619, 0.0, 0.0], [0.17188192903995514, 0.011042501777410507, 0.054578714072704315, 0.7326585650444031, 0.029838265851140022, 0.0], [0.3783015012741089, 0.017070062458515167, 0.021754134446382523, 0.4409688115119934, 0.06093813106417656, 0.08096737414598465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9923112392425537, 0.007688735146075487, 0.0, 0.0, 0.0, 0.0], [0.9498787522315979, 0.016709784045815468, 0.03341152146458626, 0.0, 0.0, 0.0], [0.9961295127868652, 0.0008787295082584023, 0.0006868162308819592, 0.0023048371076583862, 0.0, 0.0], [0.9935757517814636, 0.0032634998206049204, 0.0009993825806304812, 0.00027932299417443573, 0.0018820574041455984, 0.0], [0.9907532930374146, 0.00021344318520277739, 0.0004595233185682446, 0.0007905619568191469, 0.004424723796546459, 0.003358350833877921]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647740125656128, 0.03522596135735512, 0.0, 0.0, 0.0, 0.0], [0.8194130063056946, 0.1365436613559723, 0.04404333233833313, 0.0, 0.0, 0.0], [0.7584245800971985, 0.006878929678350687, 0.20653395354747772, 0.028162529692053795, 0.0, 0.0], [0.5298128128051758, 0.002678812015801668, 0.07857988774776459, 0.3598373234272003, 0.02909109927713871, 0.0], [0.7544413208961487, 0.00036782227107323706, 0.0019713479559868574, 0.00324004958383739, 0.1942344754934311, 0.04574500769376755]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9749131202697754, 0.02508680149912834, 0.0, 0.0, 0.0, 0.0], [0.9306471943855286, 0.05705660209059715, 0.012296222150325775, 0.0, 0.0, 0.0], [0.9305251836776733, 0.052770983427762985, 0.01111945416778326, 0.005584415514022112, 0.0, 0.0], [0.8863320350646973, 0.01292418036609888, 0.017724711447954178, 0.06150198355317116, 0.021517015993595123, 0.0], [0.791684627532959, 0.015036096796393394, 0.0317479707300663, 0.03392200171947479, 0.03707978501915932, 0.09052948653697968]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608501195907593, 0.039149850606918335, 0.0, 0.0, 0.0, 0.0], [0.9121272563934326, 0.02257651649415493, 0.06529619544744492, 0.0, 0.0, 0.0], [0.9364108443260193, 0.015584447421133518, 0.024544963613152504, 0.02345985174179077, 0.0, 0.0], [0.9454620480537415, 0.006762288510799408, 0.022026237100362778, 0.009137796238064766, 0.016611700877547264, 0.0], [0.8346164226531982, 0.001881699077785015, 0.00560904573649168, 0.01887359470129013, 0.12449200451374054, 0.014527074061334133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964227080345154, 0.0035772807896137238, 0.0, 0.0, 0.0, 0.0], [0.9713928699493408, 0.024453025311231613, 0.004154058638960123, 0.0, 0.0, 0.0], [0.9735792279243469, 0.019003381952643394, 0.003664410673081875, 0.0037529165856540203, 0.0, 0.0], [0.9586312174797058, 0.007116180844604969, 0.009218388237059116, 0.022725583985447884, 0.0023084774147719145, 0.0], [0.973607063293457, 0.008490582928061485, 0.0032512471079826355, 0.003606445388868451, 0.004877461586147547, 0.006167212035506964]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.97598797082901, 0.024011990055441856, 0.0, 0.0, 0.0, 0.0], [0.9460638165473938, 0.04211375489830971, 0.011822436936199665, 0.0, 0.0, 0.0], [0.8446813225746155, 0.04293116182088852, 0.05218198522925377, 0.06020559370517731, 0.0, 0.0], [0.9378372430801392, 0.03354858607053757, 0.008826455101370811, 0.0028792242519557476, 0.016908427700400352, 0.0], [0.8124931454658508, 0.02696753479540348, 0.05999218672513962, 0.03445731848478317, 0.011011860333383083, 0.05507794767618179]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9001203775405884, 0.09987961500883102, 0.0, 0.0, 0.0, 0.0], [0.627193033695221, 0.07988718152046204, 0.29291975498199463, 0.0, 0.0, 0.0], [0.7624077796936035, 0.02734432928264141, 0.038679543882608414, 0.17156831920146942, 0.0, 0.0], [0.7995968461036682, 0.014336260966956615, 0.01437566988170147, 0.025438452139496803, 0.14625284075737, 0.0], [0.7851970791816711, 0.04204057529568672, 0.025253651663661003, 0.02908395044505596, 0.029306314885616302, 0.08911846578121185]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954467415809631, 0.0045532057993113995, 0.0, 0.0, 0.0, 0.0], [0.9356001615524292, 0.04476744681596756, 0.019632352516055107, 0.0, 0.0, 0.0], [0.5605552792549133, 0.09861977398395538, 0.29983264207839966, 0.040992289781570435, 0.0, 0.0], [0.5893709659576416, 0.11000988632440567, 0.08033622056245804, 0.16754034161567688, 0.05274256691336632, 0.0], [0.22305884957313538, 0.05680817365646362, 0.05467984080314636, 0.24733951687812805, 0.3111244738101959, 0.1069890558719635]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9301451444625854, 0.06985488533973694, 0.0, 0.0, 0.0, 0.0], [0.8936478495597839, 0.08535721153020859, 0.020994966849684715, 0.0, 0.0, 0.0], [0.8404538035392761, 0.10619214922189713, 0.02363673783838749, 0.029717326164245605, 0.0, 0.0], [0.8927386403083801, 0.024784674867987633, 0.008319000713527203, 0.05165454372763634, 0.022503145039081573, 0.0], [0.8646610975265503, 0.009503193199634552, 0.0024329854641109705, 0.04796753078699112, 0.04273205250501633, 0.03270319849252701]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859625697135925, 0.014037408865988255, 0.0, 0.0, 0.0, 0.0], [0.9702037572860718, 0.0168070700019598, 0.012989125214517117, 0.0, 0.0, 0.0], [0.9524770379066467, 0.016064459457993507, 0.013456220738589764, 0.018002323806285858, 0.0, 0.0], [0.9332928657531738, 0.01897200010716915, 0.02014683373272419, 0.017023753374814987, 0.010564540512859821, 0.0], [0.9113592505455017, 0.012528638355433941, 0.02209620550274849, 0.01751861348748207, 0.018517911434173584, 0.01797938533127308]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9681769013404846, 0.03182310611009598, 0.0, 0.0, 0.0, 0.0], [0.9096417427062988, 0.07916690409183502, 0.011191264726221561, 0.0, 0.0, 0.0], [0.8379932045936584, 0.13078266382217407, 0.012140989303588867, 0.019083037972450256, 0.0, 0.0], [0.9116525053977966, 0.05451957508921623, 0.009499342180788517, 0.00746585289016366, 0.01686275750398636, 0.0], [0.8510289192199707, 0.07338211685419083, 0.008022507652640343, 0.009083161130547523, 0.04261006414890289, 0.015873271971940994]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799023866653442, 0.020097682252526283, 0.0, 0.0, 0.0, 0.0], [0.9558742642402649, 0.029063312336802483, 0.015062497928738594, 0.0, 0.0, 0.0], [0.7943133115768433, 0.06074100360274315, 0.06907659024000168, 0.07586916536092758, 0.0, 0.0], [0.5494324564933777, 0.03154711425304413, 0.05482015758752823, 0.05788077041506767, 0.3063195049762726, 0.0], [0.6453980803489685, 0.010770943015813828, 0.017528092488646507, 0.02157985046505928, 0.24958276748657227, 0.05514020845293999]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9506809115409851, 0.0493190623819828, 0.0, 0.0, 0.0, 0.0], [0.8553215265274048, 0.09256264567375183, 0.05211575701832771, 0.0, 0.0, 0.0], [0.850852370262146, 0.04734604433178902, 0.044177331030368805, 0.057624250650405884, 0.0, 0.0], [0.7697131633758545, 0.02788589708507061, 0.031017286702990532, 0.06842502951622009, 0.1029587835073471, 0.0], [0.7931903004646301, 0.04052198305726051, 0.029242033138871193, 0.04478124529123306, 0.04894689470529556, 0.04331749677658081]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9770310521125793, 0.02296893112361431, 0.0, 0.0, 0.0, 0.0], [0.9429817199707031, 0.017321482300758362, 0.03969680890440941, 0.0, 0.0, 0.0], [0.9144344925880432, 0.008583576418459415, 0.013035810552537441, 0.06394599378108978, 0.0, 0.0], [0.9222429990768433, 0.0036440351977944374, 0.003740275977179408, 0.010410364717245102, 0.05996239185333252, 0.0], [0.9198879599571228, 0.0030822583939880133, 0.0034827394410967827, 0.004206796642392874, 0.02125428058207035, 0.048085976392030716]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.977458119392395, 0.022541873157024384, 0.0, 0.0, 0.0, 0.0], [0.8929325342178345, 0.07475466281175613, 0.032312843948602676, 0.0, 0.0, 0.0], [0.8423511385917664, 0.05980278551578522, 0.03740081936120987, 0.06044524535536766, 0.0, 0.0], [0.7674624919891357, 0.03536349534988403, 0.042155250906944275, 0.06658654659986496, 0.08843226730823517, 0.0], [0.6182611584663391, 0.01611059531569481, 0.020167622715234756, 0.03868892416357994, 0.23147016763687134, 0.07530155777931213]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9634856581687927, 0.036514393985271454, 0.0, 0.0, 0.0, 0.0], [0.4363938570022583, 0.522637128829956, 0.04096902906894684, 0.0, 0.0, 0.0], [0.3608614206314087, 0.35129693150520325, 0.2655103802680969, 0.022331148386001587, 0.0, 0.0], [0.3942921757698059, 0.021704670041799545, 0.07794328778982162, 0.37168896198272705, 0.1343708038330078, 0.0], [0.6310713887214661, 0.01698400266468525, 0.025942081585526466, 0.08615949749946594, 0.2183200567960739, 0.021522950381040573]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9988250136375427, 0.0011750265257433057, 0.0, 0.0, 0.0, 0.0], [0.9944871068000793, 0.0004826401418540627, 0.0050302306190133095, 0.0, 0.0, 0.0], [0.9981209635734558, 2.705173392314464e-05, 0.0001130745149566792, 0.0017389442073181272, 0.0, 0.0], [0.9982239603996277, 6.83655816828832e-05, 0.00010199935059063137, 6.028370262356475e-05, 0.0015453165397047997, 0.0], [0.9982888102531433, 1.055222810464329e-06, 3.2781026675365865e-05, 0.00013038977340329438, 0.0006605894886888564, 0.0008863684488460422]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936710596084595, 0.006328921765089035, 0.0, 0.0, 0.0, 0.0], [0.9727688431739807, 0.0018561368342489004, 0.025375060737133026, 0.0, 0.0, 0.0], [0.9724299907684326, 0.0019586149137467146, 0.011192461475729942, 0.014418890699744225, 0.0, 0.0], [0.9782041311264038, 0.0009589138207957149, 0.0018706483533605933, 0.006326568778604269, 0.012639678083360195, 0.0], [0.9592596888542175, 0.0024555064737796783, 0.00161241355817765, 0.005019655916839838, 0.006687097251415253, 0.024965662509202957]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629000425338745, 0.03709998354315758, 0.0, 0.0, 0.0, 0.0], [0.36801934242248535, 0.6152258515357971, 0.016754813492298126, 0.0, 0.0, 0.0], [0.3173511326313019, 0.6140013337135315, 0.05375149846076965, 0.014896026812493801, 0.0, 0.0], [0.48987284302711487, 0.21071474254131317, 0.04693019017577171, 0.20700432360172272, 0.04547784850001335, 0.0], [0.48774227499961853, 0.1769528090953827, 0.06915216147899628, 0.09849268198013306, 0.12091436982154846, 0.046745721250772476]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9794419407844543, 0.020558049902319908, 0.0, 0.0, 0.0, 0.0], [0.6677903532981873, 0.31032365560531616, 0.021886007860302925, 0.0, 0.0, 0.0], [0.7118757367134094, 0.11108540743589401, 0.14187385141849518, 0.03516504913568497, 0.0, 0.0], [0.4501457214355469, 0.04036055505275726, 0.040458209812641144, 0.388570100069046, 0.08046531677246094, 0.0], [0.49346262216567993, 0.013696977868676186, 0.008126799948513508, 0.13074499368667603, 0.3086138069629669, 0.04535480588674545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846054315567017, 0.015394587069749832, 0.0, 0.0, 0.0, 0.0], [0.9806739091873169, 0.007713791914284229, 0.011612347327172756, 0.0, 0.0, 0.0], [0.932663083076477, 0.01957838423550129, 0.02410353161394596, 0.023654978722333908, 0.0, 0.0], [0.9422016739845276, 0.0009538981830701232, 0.0010898025939241052, 0.00319337984547019, 0.05256118252873421, 0.0], [0.9352930784225464, 0.0010279357666149735, 0.004444425459951162, 0.001637140172533691, 0.010590963996946812, 0.04700646549463272]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985783100128174, 0.0014216724084690213, 0.0, 0.0, 0.0, 0.0], [0.9893348813056946, 0.0011178902350366116, 0.00954714696854353, 0.0, 0.0, 0.0], [0.9979978203773499, 7.997050124686211e-05, 0.00013218850654084235, 0.0017900333041325212, 0.0, 0.0], [0.9986976385116577, 4.1044117097044364e-05, 3.8683547245454974e-06, 2.3676282580709085e-05, 0.0012337174266576767, 0.0], [0.9971563816070557, 1.852225250331685e-05, 1.8826559653462027e-06, 2.7900125132873654e-05, 0.0006533482228405774, 0.0021419788245111704]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9768233299255371, 0.023176640272140503, 0.0, 0.0, 0.0, 0.0], [0.9194678068161011, 0.05088186264038086, 0.029650341719388962, 0.0, 0.0, 0.0], [0.8474554419517517, 0.06100169196724892, 0.04372376948595047, 0.04781914874911308, 0.0, 0.0], [0.8011623620986938, 0.041866958141326904, 0.04375807195901871, 0.041894737631082535, 0.07131782174110413, 0.0], [0.8031871914863586, 0.02450493723154068, 0.017323585227131844, 0.04744395986199379, 0.06109930947422981, 0.046441152691841125]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829428195953369, 0.01705716922879219, 0.0, 0.0, 0.0, 0.0], [0.8863736987113953, 0.09492647647857666, 0.018699750304222107, 0.0, 0.0, 0.0], [0.9231085777282715, 0.03696346655488014, 0.032198335975408554, 0.007729663979262114, 0.0, 0.0], [0.9068527221679688, 0.016046639531850815, 0.014310522936284542, 0.04543786868453026, 0.017352323979139328, 0.0], [0.6555973887443542, 0.05091019719839096, 0.028384855017066002, 0.1256549060344696, 0.10546853393316269, 0.03398407623171806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502318501472473, 0.049768079072237015, 0.0, 0.0, 0.0, 0.0], [0.8829865455627441, 0.1000962108373642, 0.01691717840731144, 0.0, 0.0, 0.0], [0.8057457804679871, 0.14463546872138977, 0.03018922731280327, 0.019429458305239677, 0.0, 0.0], [0.8706230521202087, 0.032440632581710815, 0.026951627805829048, 0.04410304129123688, 0.025881657376885414, 0.0], [0.688364565372467, 0.009681451134383678, 0.016449343413114548, 0.0987110361456871, 0.08971209079027176, 0.09708156436681747]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9792683124542236, 0.02073168195784092, 0.0, 0.0, 0.0, 0.0], [0.9523284435272217, 0.025933818891644478, 0.021737735718488693, 0.0, 0.0, 0.0], [0.9144353270530701, 0.017671240493655205, 0.022358495742082596, 0.04553484544157982, 0.0, 0.0], [0.9448292851448059, 0.006467597559094429, 0.006386063527315855, 0.03263096138834953, 0.00968620739877224, 0.0], [0.9347906112670898, 0.007862505502998829, 0.007788175716996193, 0.021432818844914436, 0.008491144515573978, 0.01963483914732933]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983370304107666, 0.016629677265882492, 0.0, 0.0, 0.0, 0.0], [0.963111400604248, 0.009229931980371475, 0.027658598497509956, 0.0, 0.0, 0.0], [0.9706628322601318, 0.0041494048200547695, 0.0068131014704704285, 0.018374638631939888, 0.0, 0.0], [0.987951934337616, 0.002165885642170906, 0.00034901127219200134, 0.001583816367201507, 0.00794942770153284, 0.0], [0.9457950592041016, 0.014583553187549114, 0.0003652951563708484, 0.0009569536778144538, 0.013621564954519272, 0.02467755414545536]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878059029579163, 0.01219407469034195, 0.0, 0.0, 0.0, 0.0], [0.87103670835495, 0.09448163211345673, 0.03448161482810974, 0.0, 0.0, 0.0], [0.6309783458709717, 0.11090382188558578, 0.1923021823167801, 0.06581564992666245, 0.0, 0.0], [0.5360490083694458, 0.04618944972753525, 0.13605308532714844, 0.26455509662628174, 0.017153292894363403, 0.0], [0.8287520408630371, 0.023732755333185196, 0.02008037269115448, 0.07245264202356339, 0.030431220307946205, 0.024550989270210266]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995685577392578, 0.10043150931596756, 0.0, 0.0, 0.0, 0.0], [0.270343542098999, 0.6504329442977905, 0.07922357320785522, 0.0, 0.0, 0.0], [0.20541730523109436, 0.5892508625984192, 0.18085837364196777, 0.024473490193486214, 0.0, 0.0], [0.5573861002922058, 0.1774134784936905, 0.08806808292865753, 0.09881848096847534, 0.07831384986639023, 0.0], [0.5922912359237671, 0.08700639009475708, 0.05643285810947418, 0.05685883015394211, 0.12181518226861954, 0.08559554070234299]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9316380620002747, 0.06836195290088654, 0.0, 0.0, 0.0, 0.0], [0.9572945833206177, 0.026243582367897034, 0.0164618119597435, 0.0, 0.0, 0.0], [0.9880544543266296, 0.00427332753315568, 0.002954584313556552, 0.004717645235359669, 0.0, 0.0], [0.99403977394104, 0.0009413420339114964, 0.0004739820142276585, 0.00011646930943243206, 0.004428447224199772, 0.0], [0.9806035161018372, 2.5468933017691597e-05, 0.00016239412070717663, 0.0001476418401580304, 0.0013442443450912833, 0.017716845497488976]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993178129196167, 0.006821857299655676, 0.0, 0.0, 0.0, 0.0], [0.9756524562835693, 0.01318411435931921, 0.011163423769176006, 0.0, 0.0, 0.0], [0.9418966770172119, 0.004721744451671839, 0.0023818055633455515, 0.050999753177165985, 0.0, 0.0], [0.9905040860176086, 0.0022848136723041534, 6.198462506290525e-05, 0.0005984465242363513, 0.006550676189363003, 0.0], [0.9697660207748413, 0.0008878845837898552, 0.00023466735729016364, 0.0017040816601365805, 0.004128355998545885, 0.02327893301844597]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716231822967529, 0.02837684564292431, 0.0, 0.0, 0.0, 0.0], [0.9223619699478149, 0.028907248750329018, 0.048730745911598206, 0.0, 0.0, 0.0], [0.8426317572593689, 0.023872116580605507, 0.04748132824897766, 0.08601479232311249, 0.0, 0.0], [0.8521121740341187, 0.020744236186146736, 0.04494619369506836, 0.05765002593398094, 0.02454746514558792, 0.0], [0.8800725936889648, 0.022448532283306122, 0.018235722556710243, 0.01925482600927353, 0.015854258090257645, 0.044134121388196945]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9412723779678345, 0.058727629482746124, 0.0, 0.0, 0.0, 0.0], [0.916313886642456, 0.05759201943874359, 0.02609400637447834, 0.0, 0.0, 0.0], [0.8392423391342163, 0.057690516114234924, 0.01382902916520834, 0.08923812955617905, 0.0, 0.0], [0.8987162113189697, 0.0134778693318367, 0.0003456450067460537, 0.003298751311376691, 0.08416149020195007, 0.0], [0.8701692223548889, 0.002700856188312173, 0.00143499206751585, 0.0056661744602024555, 0.08874300867319107, 0.031285665929317474]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9656725525856018, 0.03432750701904297, 0.0, 0.0, 0.0, 0.0], [0.9178615808486938, 0.062257930636405945, 0.019880469888448715, 0.0, 0.0, 0.0], [0.823314905166626, 0.06282395124435425, 0.03670429438352585, 0.07715693861246109, 0.0, 0.0], [0.8501748442649841, 0.03816927224397659, 0.03196492791175842, 0.0516013503074646, 0.02808968350291252, 0.0], [0.6572404503822327, 0.05877397954463959, 0.04336007311940193, 0.09013211727142334, 0.08146599680185318, 0.06902744621038437]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9162061810493469, 0.0837937667965889, 0.0, 0.0, 0.0, 0.0], [0.9451773762702942, 0.04099284112453461, 0.013829832896590233, 0.0, 0.0, 0.0], [0.8928355574607849, 0.05368670076131821, 0.017596954479813576, 0.03588071092963219, 0.0, 0.0], [0.8337052464485168, 0.04799601063132286, 0.033513229340314865, 0.04680858924984932, 0.03797686845064163, 0.0], [0.8167192339897156, 0.06337132304906845, 0.013286277651786804, 0.020469767972826958, 0.025292355567216873, 0.06086111441254616]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525133371353149, 0.04748663306236267, 0.0, 0.0, 0.0, 0.0], [0.3019869327545166, 0.6520938873291016, 0.04591925069689751, 0.0, 0.0, 0.0], [0.285582959651947, 0.556952178478241, 0.1444743126630783, 0.012990524061024189, 0.0, 0.0], [0.843804121017456, 0.032251205295324326, 0.03954290598630905, 0.06848159432411194, 0.015920041128993034, 0.0], [0.6664940714836121, 0.06095913052558899, 0.04064354673027992, 0.06804485619068146, 0.09186329692602158, 0.07199501991271973]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682655334472656, 0.031734466552734375, 0.0, 0.0, 0.0, 0.0], [0.738521933555603, 0.22856839001178741, 0.032909639179706573, 0.0, 0.0, 0.0], [0.5946676135063171, 0.2303314357995987, 0.14867636561393738, 0.02632458508014679, 0.0, 0.0], [0.6339254975318909, 0.05813034623861313, 0.09654320776462555, 0.14291946589946747, 0.06848153471946716, 0.0], [0.40375572443008423, 0.08945391327142715, 0.07635112851858139, 0.25587135553359985, 0.1433039754629135, 0.03126389905810356]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9869793653488159, 0.013020593672990799, 0.0, 0.0, 0.0, 0.0], [0.8631385564804077, 0.1105666309595108, 0.02629482001066208, 0.0, 0.0, 0.0], [0.9488080143928528, 0.028614996001124382, 0.006535546388477087, 0.016041526570916176, 0.0, 0.0], [0.9672170877456665, 0.006604980677366257, 0.00045171406236477196, 0.004844417329877615, 0.020881708711385727, 0.0], [0.9354621171951294, 0.02047806605696678, 0.0011700231116265059, 0.007056943140923977, 0.0163181871175766, 0.019514625892043114]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846673011779785, 0.015332723967730999, 0.0, 0.0, 0.0, 0.0], [0.9052747488021851, 0.08373606950044632, 0.010989243164658546, 0.0, 0.0, 0.0], [0.8145939111709595, 0.04283742979168892, 0.10568301379680634, 0.03688570484519005, 0.0, 0.0], [0.23519809544086456, 0.012018457986414433, 0.05280117318034172, 0.6516180038452148, 0.04836418479681015, 0.0], [0.31818512082099915, 0.018632443621754646, 0.03948190063238144, 0.3755541741847992, 0.20787373185157776, 0.04027257487177849]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9811733365058899, 0.018826685845851898, 0.0, 0.0, 0.0, 0.0], [0.8618939518928528, 0.06479164958000183, 0.07331438362598419, 0.0, 0.0, 0.0], [0.7664540410041809, 0.07330425828695297, 0.10353513062000275, 0.056706514209508896, 0.0, 0.0], [0.8128499984741211, 0.03215480223298073, 0.059005625545978546, 0.05416511744260788, 0.04182446748018265, 0.0], [0.8687856197357178, 0.026987861841917038, 0.02047000452876091, 0.01629738137125969, 0.03218390792608261, 0.03527523949742317]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9264583587646484, 0.07354167848825455, 0.0, 0.0, 0.0, 0.0], [0.8403540849685669, 0.06373751163482666, 0.09590838104486465, 0.0, 0.0, 0.0], [0.7330995798110962, 0.06451118737459183, 0.10380073636770248, 0.09858842939138412, 0.0, 0.0], [0.9143612384796143, 0.008257776498794556, 0.007320381235331297, 0.017966248095035553, 0.05209439620375633, 0.0], [0.8971915245056152, 0.008555498905479908, 0.007019453682005405, 0.014860544353723526, 0.03399762138724327, 0.03837529569864273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180347919464111, 0.08196526020765305, 0.0, 0.0, 0.0, 0.0], [0.8328666687011719, 0.1219901517033577, 0.04514322429895401, 0.0, 0.0, 0.0], [0.7994157075881958, 0.0874413549900055, 0.03605784848332405, 0.07708510011434555, 0.0, 0.0], [0.880984902381897, 0.020749641582369804, 0.020554615184664726, 0.017120830714702606, 0.06058995798230171, 0.0], [0.745303213596344, 0.044334057718515396, 0.022549288347363472, 0.0331527441740036, 0.03357058763504028, 0.12109009176492691]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867060780525208, 0.013293893076479435, 0.0, 0.0, 0.0, 0.0], [0.982177734375, 0.012414131313562393, 0.005408108700066805, 0.0, 0.0, 0.0], [0.9630486369132996, 0.015290752984583378, 0.010345698334276676, 0.0113149369135499, 0.0, 0.0], [0.9213568568229675, 0.014132463373243809, 0.017639216035604477, 0.016567690297961235, 0.030303770676255226, 0.0], [0.9373326301574707, 0.009064299054443836, 0.007548365276306868, 0.006576443091034889, 0.011827622540295124, 0.027650514617562294]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9951004385948181, 0.00489962799474597, 0.0, 0.0, 0.0, 0.0], [0.9476007223129272, 0.041407931596040726, 0.010991275310516357, 0.0, 0.0, 0.0], [0.9142175316810608, 0.023523783311247826, 0.039145033806562424, 0.023113621398806572, 0.0, 0.0], [0.9534738659858704, 0.008932933211326599, 0.015272765420377254, 0.007908251136541367, 0.014412266202270985, 0.0], [0.9427101016044617, 0.00823307130485773, 0.004650997929275036, 0.004178107250481844, 0.005463531706482172, 0.03476419299840927]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543376564979553, 0.045662373304367065, 0.0, 0.0, 0.0, 0.0], [0.9696040749549866, 0.01954760029911995, 0.01084828469902277, 0.0, 0.0, 0.0], [0.9710449576377869, 0.012425386346876621, 0.008068876340985298, 0.008460716344416142, 0.0, 0.0], [0.9726192951202393, 0.002697656163945794, 0.00044831327977590263, 0.0013814778067171574, 0.022853154689073563, 0.0], [0.9675466418266296, 0.009613442234694958, 0.003203035332262516, 0.00424883933737874, 0.007442260626703501, 0.00794589426368475]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887008666992188, 0.011299116536974907, 0.0, 0.0, 0.0, 0.0], [0.9382632374763489, 0.04204244911670685, 0.019694412127137184, 0.0, 0.0, 0.0], [0.8351995944976807, 0.03487853705883026, 0.05134471505880356, 0.07857715338468552, 0.0, 0.0], [0.9042676687240601, 0.010541575029492378, 0.016426723450422287, 0.025921987369656563, 0.04284200444817543, 0.0], [0.8913140892982483, 0.00891267228871584, 0.005010711494833231, 0.008175632916390896, 0.013514749705791473, 0.07307209819555283]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8693912029266357, 0.13060881197452545, 0.0, 0.0, 0.0, 0.0], [0.3507988452911377, 0.606351912021637, 0.04284917935729027, 0.0, 0.0, 0.0], [0.35475659370422363, 0.3502019941806793, 0.24722407758235931, 0.04781729355454445, 0.0, 0.0], [0.35370609164237976, 0.03527737781405449, 0.09567111730575562, 0.449796199798584, 0.06554921716451645, 0.0], [0.4132595360279083, 0.09055527299642563, 0.05286579951643944, 0.174679696559906, 0.173848956823349, 0.09479076415300369]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629756212234497, 0.037024300545454025, 0.0, 0.0, 0.0, 0.0], [0.9756426811218262, 0.01965854875743389, 0.004698706325143576, 0.0, 0.0, 0.0], [0.9775736927986145, 0.013286248780786991, 0.0025590297300368547, 0.006581062916666269, 0.0, 0.0], [0.9870142936706543, 0.007388236932456493, 0.0009579154429957271, 0.0018318220973014832, 0.0028077505994588137, 0.0], [0.9409245848655701, 0.016633737832307816, 0.0022979143541306257, 0.0058906711637973785, 0.0055129327811300755, 0.02874022163450718]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.962827205657959, 0.037172831594944, 0.0, 0.0, 0.0, 0.0], [0.9582237601280212, 0.024641817435622215, 0.017134377732872963, 0.0, 0.0, 0.0], [0.9351300001144409, 0.015331573784351349, 0.014810982160270214, 0.034727465361356735, 0.0, 0.0], [0.9225171208381653, 0.010528750717639923, 0.011010154150426388, 0.01944003626704216, 0.036503832787275314, 0.0], [0.8420165777206421, 0.04357199743390083, 0.007488282397389412, 0.01496153138577938, 0.02385285682976246, 0.06810864061117172]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9926387071609497, 0.00736132962629199, 0.0, 0.0, 0.0, 0.0], [0.9957393407821655, 0.0033469819463789463, 0.000913690309971571, 0.0, 0.0, 0.0], [0.9869900345802307, 0.001974786864593625, 0.001524551771581173, 0.009510699659585953, 0.0, 0.0], [0.9933527708053589, 0.001020324882119894, 0.00034337223041802645, 0.0010291127255186439, 0.004254369530826807, 0.0], [0.9749016761779785, 0.00043480272870510817, 0.0004306558985263109, 0.0012364407302811742, 0.0015347707085311413, 0.021461669355630875]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9897475242614746, 0.010252462700009346, 0.0, 0.0, 0.0, 0.0], [0.9790639281272888, 0.01650906540453434, 0.0044270907528698444, 0.0, 0.0, 0.0], [0.9521436095237732, 0.029432358220219612, 0.008943161927163601, 0.009480923414230347, 0.0, 0.0], [0.939594030380249, 0.021510960534214973, 0.010278552770614624, 0.004555229097604752, 0.024061163887381554, 0.0], [0.9205074906349182, 0.016153652220964432, 0.010818594135344028, 0.01664440892636776, 0.014566398225724697, 0.021309375762939453]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898501634597778, 0.010149780660867691, 0.0, 0.0, 0.0, 0.0], [0.9820910096168518, 0.006907520350068808, 0.011001535691320896, 0.0, 0.0, 0.0], [0.9684997200965881, 0.008987602777779102, 0.015342563390731812, 0.007170087192207575, 0.0, 0.0], [0.9274120330810547, 0.009485266171395779, 0.022066107019782066, 0.03222890570759773, 0.008807653561234474, 0.0], [0.900665819644928, 0.021623756736516953, 0.013808279298245907, 0.009843860752880573, 0.008521373383700848, 0.04553695768117905]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954444169998169, 0.004555588588118553, 0.0, 0.0, 0.0, 0.0], [0.995254397392273, 0.002460238989442587, 0.002285485854372382, 0.0, 0.0, 0.0], [0.9862446188926697, 0.0015168144600465894, 0.004072288051247597, 0.008166354149580002, 0.0, 0.0], [0.9889963865280151, 0.001226040069013834, 0.0007996349013410509, 0.0006774227367714047, 0.008300574496388435, 0.0], [0.9865202903747559, 0.00039427157025784254, 0.0009571771952323616, 0.0004954367759637535, 0.0009604979422874749, 0.010672281496226788]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9821295142173767, 0.017870500683784485, 0.0, 0.0, 0.0, 0.0], [0.7489436268806458, 0.22002726793289185, 0.031029189005494118, 0.0, 0.0, 0.0], [0.28547799587249756, 0.21125678718090057, 0.47871601581573486, 0.024549242109060287, 0.0, 0.0], [0.8056644201278687, 0.026974644511938095, 0.04302806034684181, 0.06993705034255981, 0.05439583212137222, 0.0], [0.3307209014892578, 0.022326624020934105, 0.016627125442028046, 0.08019453287124634, 0.41574832797050476, 0.13438253104686737]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9697746634483337, 0.030225319787859917, 0.0, 0.0, 0.0, 0.0], [0.9800565838813782, 0.015018894337117672, 0.004924521781504154, 0.0, 0.0, 0.0], [0.9237861037254333, 0.052764780819416046, 0.00630240747705102, 0.017146753147244453, 0.0, 0.0], [0.9451844096183777, 0.03618047758936882, 0.001989208161830902, 0.003958724904805422, 0.012687299400568008, 0.0], [0.9633325934410095, 0.018662991002202034, 0.0030418417882174253, 0.007070912979543209, 0.0050094155594706535, 0.002882065251469612]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873244762420654, 0.012675459496676922, 0.0, 0.0, 0.0, 0.0], [0.9904569983482361, 0.0055419523268938065, 0.004001122899353504, 0.0, 0.0, 0.0], [0.9814971685409546, 0.004653455223888159, 0.003725277027115226, 0.010124054737389088, 0.0, 0.0], [0.9744365811347961, 0.004632251337170601, 0.002379992976784706, 0.006518087349832058, 0.012033028528094292, 0.0], [0.9624497294425964, 0.0033743639942258596, 0.0013198587112128735, 0.0017275003483518958, 0.002944675739854574, 0.028183799237012863]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807674288749695, 0.01923258602619171, 0.0, 0.0, 0.0, 0.0], [0.9664245843887329, 0.015413926914334297, 0.018161438405513763, 0.0, 0.0, 0.0], [0.9632682204246521, 0.004538117907941341, 0.002925391308963299, 0.029268190264701843, 0.0, 0.0], [0.9562349319458008, 0.0012223608791828156, 0.0005304080550558865, 0.00867149606347084, 0.03334089741110802, 0.0], [0.9657101035118103, 0.0009808284230530262, 0.0016686266753822565, 0.002634831238538027, 0.005866361316293478, 0.023139292374253273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639716148376465, 0.036028459668159485, 0.0, 0.0, 0.0, 0.0], [0.9562800526618958, 0.03373315557837486, 0.009986846707761288, 0.0, 0.0, 0.0], [0.8539998531341553, 0.08073022216558456, 0.03334445133805275, 0.031925540417432785, 0.0, 0.0], [0.9547491073608398, 0.009605025872588158, 0.004146162886172533, 0.0020133228972554207, 0.029486361891031265, 0.0], [0.9331137537956238, 0.028699662536382675, 0.005477475933730602, 0.006368075497448444, 0.012613046914339066, 0.013728085905313492]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392993450164795, 0.06070063263177872, 0.0, 0.0, 0.0, 0.0], [0.9298391342163086, 0.061895377933979034, 0.008265496231615543, 0.0, 0.0, 0.0], [0.8471823334693909, 0.09035038203001022, 0.01763608679175377, 0.044831156730651855, 0.0, 0.0], [0.8857703804969788, 0.03918175399303436, 0.007867704145610332, 0.02276589721441269, 0.04441439360380173, 0.0], [0.8563280701637268, 0.10088995099067688, 0.006531452294439077, 0.008485927246510983, 0.007368441205471754, 0.020396249368786812]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8353264331817627, 0.1646735519170761, 0.0, 0.0, 0.0, 0.0], [0.6160858869552612, 0.3137648403644562, 0.07014927268028259, 0.0, 0.0, 0.0], [0.34316325187683105, 0.2758493721485138, 0.1196604073047638, 0.26132699847221375, 0.0, 0.0], [0.5908172130584717, 0.050290752202272415, 0.041665926575660706, 0.2199493646621704, 0.0972767099738121, 0.0], [0.8481413125991821, 0.06318090111017227, 0.014733693562448025, 0.055267371237277985, 0.00901501253247261, 0.009661628864705563]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9627319574356079, 0.03726799786090851, 0.0, 0.0, 0.0, 0.0], [0.7757522463798523, 0.1799626499414444, 0.044285036623477936, 0.0, 0.0, 0.0], [0.6317060589790344, 0.24380716681480408, 0.10925652086734772, 0.015230235643684864, 0.0, 0.0], [0.9539909958839417, 0.018182311207056046, 0.011601822450757027, 0.012299076654016972, 0.003925766795873642, 0.0], [0.40356943011283875, 0.14237558841705322, 0.05661217123270035, 0.1975736767053604, 0.0929921343922615, 0.10687707364559174]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802619218826294, 0.019738124683499336, 0.0, 0.0, 0.0, 0.0], [0.9873908162117004, 0.007800452411174774, 0.004808681085705757, 0.0, 0.0, 0.0], [0.9283918738365173, 0.008301235735416412, 0.01330565195530653, 0.05000120773911476, 0.0, 0.0], [0.8981055021286011, 0.015591299161314964, 0.010177576914429665, 0.039987027645111084, 0.0361386202275753, 0.0], [0.9753499031066895, 0.00035433052107691765, 0.0005866039427928627, 0.0011877501383423805, 0.0010750899091362953, 0.021446440368890762]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9295330047607422, 0.07046692818403244, 0.0, 0.0, 0.0, 0.0], [0.9361506104469299, 0.04116682708263397, 0.022682538256049156, 0.0, 0.0, 0.0], [0.8486821055412292, 0.05802798643708229, 0.024856165051460266, 0.0684337466955185, 0.0, 0.0], [0.8661180734634399, 0.02232467755675316, 0.010369130410254002, 0.02600197121500969, 0.07518619298934937, 0.0], [0.8074421882629395, 0.044382549822330475, 0.01849711686372757, 0.03357789292931557, 0.018561245873570442, 0.07753907144069672]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9680535197257996, 0.03194643557071686, 0.0, 0.0, 0.0, 0.0], [0.9693689942359924, 0.02568492479622364, 0.004946070723235607, 0.0, 0.0, 0.0], [0.9620568156242371, 0.022552406415343285, 0.005471326876431704, 0.009919456206262112, 0.0, 0.0], [0.9727528095245361, 0.010137127712368965, 0.000757327419705689, 0.0028828983195126057, 0.013469807803630829, 0.0], [0.9624635577201843, 0.0031109037809073925, 0.0010007602395489812, 0.0019475930603221059, 0.008266227319836617, 0.02321087196469307]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8542501330375671, 0.14574992656707764, 0.0, 0.0, 0.0, 0.0], [0.9725967645645142, 0.014116315171122551, 0.01328685600310564, 0.0, 0.0, 0.0], [0.9257621765136719, 0.03257262706756592, 0.01461210660636425, 0.027053095400333405, 0.0, 0.0], [0.7923423051834106, 0.027305101975798607, 0.01880674995481968, 0.13854165375232697, 0.023004096001386642, 0.0], [0.6152060627937317, 0.02665526419878006, 0.029352931305766106, 0.05590886250138283, 0.11611279845237732, 0.15676409006118774]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9804654121398926, 0.019534552469849586, 0.0, 0.0, 0.0, 0.0], [0.9882452487945557, 0.007509466726332903, 0.004245325922966003, 0.0, 0.0, 0.0], [0.9584206938743591, 0.0109635591506958, 0.010456060990691185, 0.020159708335995674, 0.0, 0.0], [0.9604811668395996, 0.007182627450674772, 0.003072339342907071, 0.006898913532495499, 0.02236509881913662, 0.0], [0.966888964176178, 0.0032812939025461674, 0.00550054432824254, 0.004234083462506533, 0.005038043484091759, 0.015057181939482689]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498194456100464, 0.05018055811524391, 0.0, 0.0, 0.0, 0.0], [0.9781363606452942, 0.016430046409368515, 0.0054335566237568855, 0.0, 0.0, 0.0], [0.8618696331977844, 0.036093585193157196, 0.07555554062128067, 0.026481209322810173, 0.0, 0.0], [0.5449837446212769, 0.015411133877933025, 0.023516526445746422, 0.25743600726127625, 0.15865260362625122, 0.0], [0.9571874737739563, 0.0030803855042904615, 0.0014446862041950226, 0.006861559115350246, 0.014818714000284672, 0.01660723052918911]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6156560778617859, 0.3843439519405365, 0.0, 0.0, 0.0, 0.0], [0.36760634183883667, 0.42816370725631714, 0.20423001050949097, 0.0, 0.0, 0.0], [0.16471554338932037, 0.4136792719364166, 0.2509237229824066, 0.17068152129650116, 0.0, 0.0], [0.4184456169605255, 0.1524762362241745, 0.10305401682853699, 0.11071498692035675, 0.21530911326408386, 0.0], [0.19686934351921082, 0.2014620453119278, 0.12827259302139282, 0.09203246980905533, 0.09167550504207611, 0.2896881103515625]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9027364253997803, 0.09726352989673615, 0.0, 0.0, 0.0, 0.0], [0.9736634492874146, 0.014004302211105824, 0.01233230996876955, 0.0, 0.0, 0.0], [0.8504456281661987, 0.05690572410821915, 0.032060906291007996, 0.06058764085173607, 0.0, 0.0], [0.7661210298538208, 0.03530392050743103, 0.03433045372366905, 0.09675204753875732, 0.06749245524406433, 0.0], [0.8650374412536621, 0.020085260272026062, 0.01149806659668684, 0.01855834573507309, 0.018430285155773163, 0.06639053672552109]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9653082489967346, 0.03469168767333031, 0.0, 0.0, 0.0, 0.0], [0.9816323518753052, 0.014176066033542156, 0.004191514104604721, 0.0, 0.0, 0.0], [0.9275256395339966, 0.04737218841910362, 0.01152826938778162, 0.013573966920375824, 0.0, 0.0], [0.9293117523193359, 0.025833239778876305, 0.007227106485515833, 0.014300585724413395, 0.02332727052271366, 0.0], [0.8895062804222107, 0.04689619690179825, 0.0047171092592179775, 0.006286581978201866, 0.00609014043584466, 0.04650374501943588]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8938026428222656, 0.10619727522134781, 0.0, 0.0, 0.0, 0.0], [0.8221707940101624, 0.06304481625556946, 0.11478441953659058, 0.0, 0.0, 0.0], [0.5047380924224854, 0.15375731885433197, 0.2277037501335144, 0.11380083113908768, 0.0, 0.0], [0.4082071781158447, 0.09066355973482132, 0.11696872115135193, 0.24553199112415314, 0.13862857222557068, 0.0], [0.7291035652160645, 0.06638889014720917, 0.023112818598747253, 0.031103096902370453, 0.057143256068229675, 0.09314827620983124]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9247531890869141, 0.07524678111076355, 0.0, 0.0, 0.0, 0.0], [0.8957376480102539, 0.06989553570747375, 0.03436679765582085, 0.0, 0.0, 0.0], [0.7924937605857849, 0.0960114598274231, 0.05509118735790253, 0.056403566151857376, 0.0, 0.0], [0.7891505360603333, 0.07880303263664246, 0.03840155899524689, 0.05396979674696922, 0.03967496380209923, 0.0], [0.7807856798171997, 0.0799354612827301, 0.042531758546829224, 0.03234211727976799, 0.0178169384598732, 0.046588052064180374]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9480886459350586, 0.05191127583384514, 0.0, 0.0, 0.0, 0.0], [0.863694965839386, 0.04756204038858414, 0.08874296396970749, 0.0, 0.0, 0.0], [0.9341371059417725, 0.022224076092243195, 0.022624483332037926, 0.021014342084527016, 0.0, 0.0], [0.9588143229484558, 0.008020909503102303, 0.004490078426897526, 0.005862293299287558, 0.022812429815530777, 0.0], [0.9385918378829956, 0.021227721124887466, 0.0048724692314863205, 0.010940189473330975, 0.009524582885205746, 0.014843451790511608]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9763734340667725, 0.023626558482646942, 0.0, 0.0, 0.0, 0.0], [0.9884802103042603, 0.005189393647015095, 0.0063303736969828606, 0.0, 0.0, 0.0], [0.9477092027664185, 0.0179851483553648, 0.010156610049307346, 0.024149026721715927, 0.0, 0.0], [0.967192530632019, 0.006552813574671745, 0.0033227826934307814, 0.00556332478299737, 0.017368387430906296, 0.0], [0.9584562182426453, 0.007502961438149214, 0.0051363310776650906, 0.008071648888289928, 0.005997124593704939, 0.014835843816399574]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.884070873260498, 0.11592914164066315, 0.0, 0.0, 0.0, 0.0], [0.9931254386901855, 0.005070806015282869, 0.0018038019770756364, 0.0, 0.0, 0.0], [0.9534159302711487, 0.02382904477417469, 0.007748977281153202, 0.015006075613200665, 0.0, 0.0], [0.9151289463043213, 0.010873105376958847, 0.013190957717597485, 0.011050421744585037, 0.04975655674934387, 0.0], [0.8769673109054565, 0.03385210782289505, 0.00848648976534605, 0.009969149716198444, 0.03468578681349754, 0.036039214581251144]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003709519514814019, 0.999629020690918, 0.0, 0.0, 0.0, 0.0], [6.525027856696397e-05, 0.3737829029560089, 0.6261518597602844, 0.0, 0.0, 0.0], [4.606018774211407e-05, 0.210508793592453, 0.4115968942642212, 0.3778482675552368, 0.0, 0.0], [4.753069515572861e-05, 0.11616954207420349, 0.23264272511005402, 0.3985331058502197, 0.2526070475578308, 0.0], [1.247641534973809e-06, 0.14819711446762085, 0.15813173353672028, 0.30074331164360046, 0.11939018964767456, 0.27353641390800476]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971555769443512, 0.028444187715649605, 0.0, 0.0, 0.0, 0.0], [0.9529065489768982, 0.03233075141906738, 0.014762768521904945, 0.0, 0.0, 0.0], [0.9343128204345703, 0.02351292595267296, 0.02049802988767624, 0.021676240488886833, 0.0, 0.0], [0.9529678225517273, 0.00855141133069992, 0.004359325394034386, 0.008064556866884232, 0.026056913658976555, 0.0], [0.9653593897819519, 0.008487647399306297, 0.003499280195683241, 0.002721576252952218, 0.0032828773837536573, 0.016649367287755013]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8630780577659607, 0.13692188262939453, 0.0, 0.0, 0.0, 0.0], [0.7696157097816467, 0.0851333811879158, 0.14525099098682404, 0.0, 0.0, 0.0], [0.7133337259292603, 0.10170899331569672, 0.11931268870830536, 0.06564456224441528, 0.0, 0.0], [0.7186222076416016, 0.05444284901022911, 0.01386815495789051, 0.07808027416467667, 0.13498654961585999, 0.0], [0.7990148663520813, 0.05805593729019165, 0.009447019547224045, 0.017770467326045036, 0.02113853208720684, 0.09457314014434814]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9518988728523254, 0.048101115971803665, 0.0, 0.0, 0.0, 0.0], [0.8580653071403503, 0.02944577857851982, 0.11248888075351715, 0.0, 0.0, 0.0], [0.6577738523483276, 0.08513449877500534, 0.1261308640241623, 0.1309608370065689, 0.0, 0.0], [0.8087368607521057, 0.0323016420006752, 0.01841817982494831, 0.06856140494346619, 0.07198194414377213, 0.0], [0.6683295965194702, 0.13281384110450745, 0.021880635991692543, 0.02787741646170616, 0.04923408478498459, 0.0998644009232521]]]], \"left_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"], \"right_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-df8d67b4ace847b4a67aa800cdd9cb83\", \"include_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"total_heads\": 12}; // HACK: {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.961219847202301, 0.038780149072408676, 0.0, 0.0, 0.0, 0.0], [0.7466979026794434, 0.11987314373254776, 0.1334289014339447, 0.0, 0.0, 0.0], [0.5885030031204224, 0.13792067766189575, 0.212137371301651, 0.06143897399306297, 0.0, 0.0], [0.6570857763290405, 0.08996301889419556, 0.12751281261444092, 0.08361563086509705, 0.041822850704193115, 0.0], [0.2728874385356903, 0.11203353852033615, 0.1663985401391983, 0.08467111736536026, 0.16952736675739288, 0.19448210299015045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010616563260555267, 0.9893833994865417, 0.0, 0.0, 0.0, 0.0], [0.0024677535984665155, 0.008448007516562939, 0.9890841841697693, 0.0, 0.0, 0.0], [0.0001232847134815529, 0.0018733182223513722, 0.013126976788043976, 0.9848763942718506, 0.0, 0.0], [0.0010669564362615347, 0.001136627048254013, 0.003034998197108507, 0.0015735096530988812, 0.9931879043579102, 0.0], [0.00019791982776951045, 0.0010528112761676311, 0.0015437351539731026, 0.0009642760851420462, 3.4924432839034125e-05, 0.9962062835693359]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47578439116477966, 0.524215579032898, 0.0, 0.0, 0.0, 0.0], [0.5906045436859131, 0.2486611008644104, 0.16073434054851532, 0.0, 0.0, 0.0], [0.5529289841651917, 0.18856702744960785, 0.14457571506500244, 0.11392831057310104, 0.0, 0.0], [0.45094072818756104, 0.16486799716949463, 0.17318038642406464, 0.11748014390468597, 0.09353074431419373, 0.0], [0.4257245659828186, 0.1732865273952484, 0.15651953220367432, 0.07022649794816971, 0.0808701142668724, 0.09337282180786133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6133623123168945, 0.38663768768310547, 0.0, 0.0, 0.0, 0.0], [0.06098509579896927, 0.03253461793065071, 0.9064802527427673, 0.0, 0.0, 0.0], [0.006717085838317871, 0.0004012881254311651, 0.7572958469390869, 0.23558568954467773, 0.0, 0.0], [0.03722766041755676, 0.002948855282738805, 0.10081092268228531, 0.04142269119620323, 0.8175898790359497, 0.0], [0.04989781975746155, 0.00030758307548239827, 0.0024198265746235847, 0.0034334994852542877, 0.0006823898293077946, 0.9432588815689087]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9489555954933167, 0.051044441759586334, 0.0, 0.0, 0.0, 0.0], [0.6821408867835999, 0.1395241767168045, 0.17833495140075684, 0.0, 0.0, 0.0], [0.20366324484348297, 0.05641487240791321, 0.06399301439523697, 0.6759288311004639, 0.0, 0.0], [0.3419547975063324, 0.06725440919399261, 0.07926183938980103, 0.1783619523048401, 0.3331669867038727, 0.0], [0.09464015811681747, 0.0074282134883105755, 0.006983973551541567, 0.0071843694895505905, 0.018724264577031136, 0.865039050579071]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33834606409072876, 0.6616539359092712, 0.0, 0.0, 0.0, 0.0], [0.07855993509292603, 0.006165449041873217, 0.9152746200561523, 0.0, 0.0, 0.0], [0.01677597686648369, 0.0004037705948576331, 0.003340460592880845, 0.9794798493385315, 0.0, 0.0], [0.027600426226854324, 0.00044415233423933387, 0.0006541680195368826, 0.0002266185765620321, 0.971074640750885, 0.0], [0.010248198173940182, 3.701553578139283e-05, 0.00016064041119534522, 2.7341819077264518e-05, 1.0187304724240676e-05, 0.98951655626297]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.982503354549408, 0.017496665939688683, 0.0, 0.0, 0.0, 0.0], [0.8874197006225586, 0.05467939004302025, 0.05790085718035698, 0.0, 0.0, 0.0], [0.6849910616874695, 0.1228068619966507, 0.04972026124596596, 0.14248186349868774, 0.0, 0.0], [0.6015856862068176, 0.09881888329982758, 0.07070108503103256, 0.16652540862560272, 0.06236903741955757, 0.0], [0.3232504427433014, 0.12567411363124847, 0.04432179778814316, 0.07076980918645859, 0.06606649607419968, 0.36991727352142334]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9191647171974182, 0.0808352455496788, 0.0, 0.0, 0.0, 0.0], [0.45986413955688477, 0.39703112840652466, 0.14310479164123535, 0.0, 0.0, 0.0], [0.3003872334957123, 0.22181738913059235, 0.38161516189575195, 0.09618020057678223, 0.0, 0.0], [0.18963925540447235, 0.1376371532678604, 0.20173484086990356, 0.23632164299488068, 0.23466713726520538, 0.0], [0.15410441160202026, 0.09489496797323227, 0.11902562528848648, 0.10277965664863586, 0.4317220449447632, 0.09747327119112015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.364999920129776, 0.6350001096725464, 0.0, 0.0, 0.0, 0.0], [0.24595215916633606, 0.5519201755523682, 0.20212766528129578, 0.0, 0.0, 0.0], [0.2721358835697174, 0.40738627314567566, 0.25186213850975037, 0.06861574947834015, 0.0, 0.0], [0.10242555290460587, 0.16683615744113922, 0.524804949760437, 0.05445462837815285, 0.15147870779037476, 0.0], [0.25029507279396057, 0.22198128700256348, 0.18899968266487122, 0.10677118599414825, 0.1303267478942871, 0.10162602365016937]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6990506649017334, 0.3009493350982666, 0.0, 0.0, 0.0, 0.0], [0.5107942819595337, 0.2948642075061798, 0.1943415403366089, 0.0, 0.0, 0.0], [0.4604707360267639, 0.2805190980434418, 0.19174803793430328, 0.0672621801495552, 0.0, 0.0], [0.37648412585258484, 0.21120662987232208, 0.20214538276195526, 0.10207021236419678, 0.10809355974197388, 0.0], [0.30138441920280457, 0.20456179976463318, 0.18250338733196259, 0.11019382625818253, 0.1629127413034439, 0.03844383731484413]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7131582498550415, 0.2868417799472809, 0.0, 0.0, 0.0, 0.0], [0.4058799147605896, 0.18063297867774963, 0.41348710656166077, 0.0, 0.0, 0.0], [0.265546053647995, 0.1698586493730545, 0.3358593285083771, 0.228736013174057, 0.0, 0.0], [0.31385406851768494, 0.1831669807434082, 0.14928358793258667, 0.05377671495079994, 0.29991865158081055, 0.0], [0.20466560125350952, 0.18731118738651276, 0.15959151089191437, 0.06381776183843613, 0.03642302006483078, 0.34819093346595764]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6586242914199829, 0.3413757383823395, 0.0, 0.0, 0.0, 0.0], [0.5917776226997375, 0.3160035014152527, 0.0922188088297844, 0.0, 0.0, 0.0], [0.5477152466773987, 0.23586955666542053, 0.061456020921468735, 0.1549593061208725, 0.0, 0.0], [0.4587061107158661, 0.22439992427825928, 0.07887422293424606, 0.0992034301161766, 0.13881628215312958, 0.0], [0.32743722200393677, 0.19600819051265717, 0.068057119846344, 0.0892510637640953, 0.11618079245090485, 0.20306548476219177]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9961552023887634, 0.0038448425475507975, 0.0, 0.0, 0.0, 0.0], [0.8594854474067688, 0.06906110048294067, 0.07145342975854874, 0.0, 0.0, 0.0], [0.3800053000450134, 0.04127567633986473, 0.5496612787246704, 0.029057776555418968, 0.0, 0.0], [0.21445226669311523, 0.05088742449879646, 0.4317440092563629, 0.25869303941726685, 0.044223275035619736, 0.0], [0.11175256222486496, 0.017593080177903175, 0.027507441118359566, 0.04086771607398987, 0.7754669785499573, 0.026812179014086723]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9285967946052551, 0.07140326499938965, 0.0, 0.0, 0.0, 0.0], [0.6077286005020142, 0.3121427297592163, 0.08012867718935013, 0.0, 0.0, 0.0], [0.4942909777164459, 0.28503698110580444, 0.11849315464496613, 0.10217894613742828, 0.0, 0.0], [0.4183879494667053, 0.23117904365062714, 0.0834062322974205, 0.11365949362516403, 0.1533672958612442, 0.0], [0.42215850949287415, 0.12917140126228333, 0.08740927278995514, 0.1016375944018364, 0.21230268478393555, 0.04732053726911545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9786475896835327, 0.02135237120091915, 0.0, 0.0, 0.0, 0.0], [0.7749121785163879, 0.06510371714830399, 0.15998409688472748, 0.0, 0.0, 0.0], [0.6484923362731934, 0.07483134418725967, 0.14751605689525604, 0.12916021049022675, 0.0, 0.0], [0.5224639773368835, 0.06921815127134323, 0.13823404908180237, 0.1110658198595047, 0.15901805460453033, 0.0], [0.3964517116546631, 0.07325823605060577, 0.12938153743743896, 0.1064242571592331, 0.14864002168178558, 0.1458442211151123]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5525906085968018, 0.44740936160087585, 0.0, 0.0, 0.0, 0.0], [0.5585009455680847, 0.2176259458065033, 0.22387312352657318, 0.0, 0.0, 0.0], [0.5143128633499146, 0.15964674949645996, 0.15491968393325806, 0.1711207628250122, 0.0, 0.0], [0.5039961338043213, 0.11401888728141785, 0.11974027007818222, 0.12552587687969208, 0.13671889901161194, 0.0], [0.5061842799186707, 0.08567393571138382, 0.08903021365404129, 0.09759818762540817, 0.1027572825551033, 0.11875619739294052]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9242545366287231, 0.07574543356895447, 0.0, 0.0, 0.0, 0.0], [0.8257425427436829, 0.07932533323764801, 0.09493216127157211, 0.0, 0.0, 0.0], [0.7306380271911621, 0.0857183039188385, 0.08043931424617767, 0.10320431739091873, 0.0, 0.0], [0.6383238434791565, 0.07886394113302231, 0.07815027981996536, 0.08758097141981125, 0.1170809343457222, 0.0], [0.5552157163619995, 0.07409121096134186, 0.06834889203310013, 0.07778600603342056, 0.09999319165945053, 0.12456497550010681]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8578913807868958, 0.14210854470729828, 0.0, 0.0, 0.0, 0.0], [0.6423038244247437, 0.166290283203125, 0.19140593707561493, 0.0, 0.0, 0.0], [0.5530979633331299, 0.10609274357557297, 0.07821257412433624, 0.26259663701057434, 0.0, 0.0], [0.40121692419052124, 0.12223611027002335, 0.1934729963541031, 0.14164622128009796, 0.14142780005931854, 0.0], [0.40212565660476685, 0.18450751900672913, 0.07516805827617645, 0.05849048122763634, 0.1444634348154068, 0.13524490594863892]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791558980941772, 0.020844051614403725, 0.0, 0.0, 0.0, 0.0], [0.8829841613769531, 0.06233249977231026, 0.05468335747718811, 0.0, 0.0, 0.0], [0.8105455040931702, 0.08617085963487625, 0.07321777194738388, 0.03006584383547306, 0.0, 0.0], [0.6819812059402466, 0.04990820586681366, 0.08296552300453186, 0.08369525521993637, 0.10144983977079391, 0.0], [0.4056689441204071, 0.07337666302919388, 0.08601408451795578, 0.061709366738796234, 0.13226434588432312, 0.2409665435552597]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9670190811157227, 0.03298088163137436, 0.0, 0.0, 0.0, 0.0], [0.8449064493179321, 0.0851450264453888, 0.06994850933551788, 0.0, 0.0, 0.0], [0.7123572826385498, 0.07896047830581665, 0.055410757660865784, 0.15327158570289612, 0.0, 0.0], [0.6402613520622253, 0.0739755630493164, 0.044393062591552734, 0.14322125911712646, 0.09814881533384323, 0.0], [0.5073903799057007, 0.07523059099912643, 0.07754647731781006, 0.11362491548061371, 0.13947951793670654, 0.08672808855772018]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8487569093704224, 0.1512431502342224, 0.0, 0.0, 0.0, 0.0], [0.8415648937225342, 0.12107233703136444, 0.03736274689435959, 0.0, 0.0, 0.0], [0.7505517601966858, 0.11348944902420044, 0.06179959326982498, 0.07415912300348282, 0.0, 0.0], [0.6614719033241272, 0.10242646187543869, 0.052934251725673676, 0.07529708743095398, 0.10787025839090347, 0.0], [0.6014202237129211, 0.11340376734733582, 0.05631929263472557, 0.07096721231937408, 0.10906282067298889, 0.04882663115859032]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9445484280586243, 0.05545158311724663, 0.0, 0.0, 0.0, 0.0], [0.8874568939208984, 0.05474215745925903, 0.0578010231256485, 0.0, 0.0, 0.0], [0.8281888961791992, 0.06895001977682114, 0.059034693986177444, 0.0438263975083828, 0.0, 0.0], [0.6429892778396606, 0.0674755647778511, 0.11629703640937805, 0.05417950078845024, 0.11905858665704727, 0.0], [0.7367823719978333, 0.056119054555892944, 0.06857288628816605, 0.034219540655612946, 0.0787537544965744, 0.02555238828063011]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002913394710049033, 0.9997085928916931, 0.0, 0.0, 0.0, 0.0], [0.0007981209782883525, 0.5288336873054504, 0.4703682065010071, 0.0, 0.0, 0.0], [0.0007648481405340135, 0.34519824385643005, 0.3085267245769501, 0.34551018476486206, 0.0, 0.0], [0.0010283143492415547, 0.241359144449234, 0.23320138454437256, 0.2555713355541229, 0.2688397467136383, 0.0], [0.0009746829164214432, 0.17789699137210846, 0.16743157804012299, 0.1858760118484497, 0.18734444677829742, 0.28047630190849304]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.824492871761322, 0.17550717294216156, 0.0, 0.0, 0.0, 0.0], [0.12386877834796906, 0.044499922543764114, 0.8316312432289124, 0.0, 0.0, 0.0], [0.07924355566501617, 0.01296587660908699, 0.0015277155907824636, 0.9062628149986267, 0.0, 0.0], [0.08806384354829788, 0.0213409923017025, 0.0028886159416288137, 0.002845379989594221, 0.884861171245575, 0.0], [0.09983218461275101, 0.03363388776779175, 0.0054999832063913345, 0.002433052286505699, 0.0015082412865012884, 0.8570926189422607]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9646892547607422, 0.03531072288751602, 0.0, 0.0, 0.0, 0.0], [0.7529157400131226, 0.08733473718166351, 0.15974950790405273, 0.0, 0.0, 0.0], [0.4202282726764679, 0.09195102006196976, 0.23549850285053253, 0.25232216715812683, 0.0, 0.0], [0.30848920345306396, 0.05908140912652016, 0.38391315937042236, 0.15659146010875702, 0.09192468225955963, 0.0], [0.44790443778038025, 0.04329312965273857, 0.0796918049454689, 0.11081931740045547, 0.22124572098255157, 0.09704558551311493]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.991096019744873, 0.008904009126126766, 0.0, 0.0, 0.0, 0.0], [0.9697675704956055, 0.026084503158926964, 0.004147922620177269, 0.0, 0.0, 0.0], [0.9082901477813721, 0.033206019550561905, 0.00942116230726242, 0.049082688987255096, 0.0, 0.0], [0.8949133157730103, 0.05544555187225342, 0.005577624775469303, 0.03150692582130432, 0.012556522153317928, 0.0], [0.8497740030288696, 0.028890123590826988, 0.0036647915840148926, 0.03751987963914871, 0.038427725434303284, 0.04172350466251373]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9984525442123413, 0.0015474462416023016, 0.0, 0.0, 0.0, 0.0], [0.48947831988334656, 0.4812193810939789, 0.029302269220352173, 0.0, 0.0, 0.0], [0.11772153526544571, 0.13121186196804047, 0.6702314615249634, 0.08083520829677582, 0.0, 0.0], [0.13043689727783203, 0.04068669304251671, 0.2652038037776947, 0.4114362895488739, 0.15223638713359833, 0.0], [0.12661904096603394, 0.03275119513273239, 0.03567872568964958, 0.06039190664887428, 0.6021825075149536, 0.1423766165971756]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9805176854133606, 0.019482342526316643, 0.0, 0.0, 0.0, 0.0], [0.7948849201202393, 0.12061909586191177, 0.08449601382017136, 0.0, 0.0, 0.0], [0.5612356066703796, 0.15743127465248108, 0.20339730381965637, 0.0779358446598053, 0.0, 0.0], [0.42583736777305603, 0.10742014646530151, 0.15123659372329712, 0.08755031228065491, 0.22795552015304565, 0.0], [0.24752654135227203, 0.024188270792365074, 0.03039524517953396, 0.08586956560611725, 0.5714336633682251, 0.040586672723293304]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887767434120178, 0.011223225854337215, 0.0, 0.0, 0.0, 0.0], [0.7572693228721619, 0.22317346930503845, 0.019557112827897072, 0.0, 0.0, 0.0], [0.5341880321502686, 0.22107566893100739, 0.1762184202671051, 0.06851787120103836, 0.0, 0.0], [0.17095312476158142, 0.0822940468788147, 0.576022207736969, 0.11097585409879684, 0.059754710644483566, 0.0], [0.2487109899520874, 0.08880793303251266, 0.08980197459459305, 0.09729334712028503, 0.4413093626499176, 0.03407646715641022]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8422133326530457, 0.15778663754463196, 0.0, 0.0, 0.0, 0.0], [0.468412846326828, 0.46105360984802246, 0.07053359597921371, 0.0, 0.0, 0.0], [0.2588140666484833, 0.4635888636112213, 0.18503506481647491, 0.09256205707788467, 0.0, 0.0], [0.18399578332901, 0.29154160618782043, 0.17031098902225494, 0.27173006534576416, 0.08242159336805344, 0.0], [0.1646990180015564, 0.2472696155309677, 0.08770562708377838, 0.22575001418590546, 0.1774536371231079, 0.09712201356887817]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919946193695068, 0.008005390875041485, 0.0, 0.0, 0.0, 0.0], [0.9068724513053894, 0.044065121561288834, 0.04906242713332176, 0.0, 0.0, 0.0], [0.8582221865653992, 0.055348269641399384, 0.040419407188892365, 0.046010036021471024, 0.0, 0.0], [0.7855252623558044, 0.041242364794015884, 0.08369296044111252, 0.04887620359659195, 0.040663279592990875, 0.0], [0.7856317162513733, 0.05014643445611, 0.04751267284154892, 0.027365952730178833, 0.05614755302667618, 0.03319567069411278]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9041035175323486, 0.09589648246765137, 0.0, 0.0, 0.0, 0.0], [0.5862312912940979, 0.07199832051992416, 0.34177035093307495, 0.0, 0.0, 0.0], [0.3878960907459259, 0.04660807177424431, 0.20278996229171753, 0.36270591616630554, 0.0, 0.0], [0.2665242552757263, 0.024533024057745934, 0.12211935967206955, 0.20041218400001526, 0.386411190032959, 0.0], [0.23357485234737396, 0.02053728699684143, 0.09610321372747421, 0.13062246143817902, 0.22990450263023376, 0.289257675409317]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639912247657776, 0.036008793860673904, 0.0, 0.0, 0.0, 0.0], [0.7075552344322205, 0.2542775869369507, 0.038167137652635574, 0.0, 0.0, 0.0], [0.2566526234149933, 0.20589298009872437, 0.01665665954351425, 0.5207977294921875, 0.0, 0.0], [0.1037939190864563, 0.04639088362455368, 0.008698614314198494, 0.7866851687431335, 0.05443140119314194, 0.0], [0.2214341163635254, 0.03379744663834572, 0.029023902490735054, 0.541292130947113, 0.15286092460155487, 0.021591555327177048]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891703724861145, 0.010829661041498184, 0.0, 0.0, 0.0, 0.0], [0.7913155555725098, 0.12309625744819641, 0.08558809012174606, 0.0, 0.0, 0.0], [0.2954600155353546, 0.15808308124542236, 0.4217240810394287, 0.1247328370809555, 0.0, 0.0], [0.23440983891487122, 0.09886523336172104, 0.33160170912742615, 0.1971396654844284, 0.1379835456609726, 0.0], [0.19728390872478485, 0.05741839483380318, 0.06909029185771942, 0.16469819843769073, 0.2797277867794037, 0.23178131878376007]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9359127879142761, 0.0640871673822403, 0.0, 0.0, 0.0, 0.0], [0.7888627648353577, 0.08673475682735443, 0.12440246343612671, 0.0, 0.0, 0.0], [0.6535118818283081, 0.07573551684617996, 0.09732568264007568, 0.17342689633369446, 0.0, 0.0], [0.522276759147644, 0.058278825134038925, 0.09920477122068405, 0.17020836472511292, 0.15003129839897156, 0.0], [0.4108840823173523, 0.047306034713983536, 0.07265672832727432, 0.10560744255781174, 0.10550004243850708, 0.25804558396339417]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9683833122253418, 0.03161672502756119, 0.0, 0.0, 0.0, 0.0], [0.8965396881103516, 0.038870569318532944, 0.06458976864814758, 0.0, 0.0, 0.0], [0.8264952898025513, 0.03213464096188545, 0.05196719989180565, 0.0894029513001442, 0.0, 0.0], [0.7718173265457153, 0.030402837321162224, 0.045827414840459824, 0.07118473201990128, 0.08076759427785873, 0.0], [0.7292331457138062, 0.021699821576476097, 0.033074747771024704, 0.04720093309879303, 0.06474557518959045, 0.10404567420482635]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9979567527770996, 0.0020432830788195133, 0.0, 0.0, 0.0, 0.0], [0.955294132232666, 0.00802531372755766, 0.03668047487735748, 0.0, 0.0, 0.0], [0.9254710078239441, 0.002755576279014349, 0.0020629852078855038, 0.06971040368080139, 0.0, 0.0], [0.8660576939582825, 0.0038883681409060955, 0.0006785982404835522, 0.0006981453043408692, 0.1286771297454834, 0.0], [0.8455929160118103, 0.0037804055027663708, 0.000253423087997362, 6.0270751419011503e-05, 0.00011820747749879956, 0.15019479393959045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9262455105781555, 0.07375453412532806, 0.0, 0.0, 0.0, 0.0], [0.7717157006263733, 0.16241952776908875, 0.06586471945047379, 0.0, 0.0, 0.0], [0.8167637586593628, 0.07807160913944244, 0.06324034929275513, 0.041924238204956055, 0.0, 0.0], [0.6867184638977051, 0.07755157351493835, 0.10056912153959274, 0.05955080687999725, 0.07561002671718597, 0.0], [0.6421161890029907, 0.11014898866415024, 0.07688194513320923, 0.054033469408750534, 0.10333634912967682, 0.013483096845448017]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9395954608917236, 0.060404520481824875, 0.0, 0.0, 0.0, 0.0], [0.23004619777202606, 0.6617380380630493, 0.1082158014178276, 0.0, 0.0, 0.0], [0.2670227289199829, 0.3607950508594513, 0.3249626159667969, 0.047219593077898026, 0.0, 0.0], [0.595201313495636, 0.12269274890422821, 0.06302059441804886, 0.08916817605495453, 0.12991715967655182, 0.0], [0.10284596681594849, 0.02938011661171913, 0.013739082030951977, 0.045860596001148224, 0.7698501348495483, 0.03832406550645828]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9040980935096741, 0.09590194374322891, 0.0, 0.0, 0.0, 0.0], [0.357237845659256, 0.6274612545967102, 0.015300876460969448, 0.0, 0.0, 0.0], [0.5917996764183044, 0.2764042019844055, 0.10476048290729523, 0.027035649865865707, 0.0, 0.0], [0.7254403829574585, 0.04983152449131012, 0.014982940629124641, 0.1778142899274826, 0.031930916011333466, 0.0], [0.7612743973731995, 0.06158972904086113, 0.005942251533269882, 0.01642685756087303, 0.1267806589603424, 0.0279861893504858]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947587847709656, 0.005241230130195618, 0.0, 0.0, 0.0, 0.0], [0.9632415771484375, 0.017816413193941116, 0.018942030146718025, 0.0, 0.0, 0.0], [0.9671078324317932, 0.008509586565196514, 0.00856222677975893, 0.015820473432540894, 0.0, 0.0], [0.9340996146202087, 0.011952387169003487, 0.02018021047115326, 0.02675083465874195, 0.0070168930105865, 0.0], [0.9587237238883972, 0.004657115787267685, 0.003326789475977421, 0.006545313633978367, 0.010182461701333523, 0.016564540565013885]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769991040229797, 0.023000910878181458, 0.0, 0.0, 0.0, 0.0], [0.7917609214782715, 0.1753319948911667, 0.032907065004110336, 0.0, 0.0, 0.0], [0.7949192523956299, 0.10531841963529587, 0.040218502283096313, 0.05954383686184883, 0.0, 0.0], [0.7097718715667725, 0.10552527755498886, 0.06597573310136795, 0.05765606462955475, 0.061070989817380905, 0.0], [0.7506601214408875, 0.026514461264014244, 0.021576043218374252, 0.034296683967113495, 0.08494450151920319, 0.08200812339782715]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983751654624939, 0.016248304396867752, 0.0, 0.0, 0.0, 0.0], [0.5615494847297668, 0.08956841379404068, 0.3488820493221283, 0.0, 0.0, 0.0], [0.32929039001464844, 0.024114903062582016, 0.5428059697151184, 0.10378880053758621, 0.0, 0.0], [0.34330207109451294, 0.01308644749224186, 0.5121983289718628, 0.11146228760480881, 0.019950881600379944, 0.0], [0.4792812764644623, 0.01733359508216381, 0.1180536150932312, 0.06130281835794449, 0.20071913301944733, 0.12330964207649231]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908847212791443, 0.009115329943597317, 0.0, 0.0, 0.0, 0.0], [0.5282707214355469, 0.3292262554168701, 0.1425030380487442, 0.0, 0.0, 0.0], [0.48788541555404663, 0.23368670046329498, 0.17578084766864777, 0.10264702141284943, 0.0, 0.0], [0.31444698572158813, 0.18065163493156433, 0.168714240193367, 0.09506598114967346, 0.24112118780612946, 0.0], [0.5168765187263489, 0.035897161811590195, 0.026188155636191368, 0.04039734974503517, 0.18791745603084564, 0.1927233189344406]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8750308156013489, 0.12496919929981232, 0.0, 0.0, 0.0, 0.0], [0.4550614655017853, 0.4900427758693695, 0.05489582195878029, 0.0, 0.0, 0.0], [0.2933720052242279, 0.5449907183647156, 0.09444297850131989, 0.06719419360160828, 0.0, 0.0], [0.489708811044693, 0.2720997631549835, 0.06861965358257294, 0.14694802463054657, 0.022623788565397263, 0.0], [0.4729066491127014, 0.08103099465370178, 0.016052134335041046, 0.30672287940979004, 0.10120721161365509, 0.022080255672335625]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9630220532417297, 0.03697792813181877, 0.0, 0.0, 0.0, 0.0], [0.7557195425033569, 0.16436372697353363, 0.07991670072078705, 0.0, 0.0, 0.0], [0.6947705745697021, 0.08409853279590607, 0.0638260766863823, 0.15730486810207367, 0.0, 0.0], [0.5821147561073303, 0.03297805413603783, 0.07936596870422363, 0.19441406428813934, 0.11112712323665619, 0.0], [0.5974540710449219, 0.04261096194386482, 0.06919723749160767, 0.14563441276550293, 0.12481734901666641, 0.020285936072468758]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957822561264038, 0.004217816516757011, 0.0, 0.0, 0.0, 0.0], [0.9312832951545715, 0.010560247115790844, 0.05815650522708893, 0.0, 0.0, 0.0], [0.8435326814651489, 0.015695005655288696, 0.045751139521598816, 0.09502115100622177, 0.0, 0.0], [0.772409975528717, 0.011981245130300522, 0.03504609689116478, 0.03876771405339241, 0.14179500937461853, 0.0], [0.7642908692359924, 0.009868789464235306, 0.00812275055795908, 0.013314393348991871, 0.04824395477771759, 0.15615922212600708]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9701177477836609, 0.02988232672214508, 0.0, 0.0, 0.0, 0.0], [0.6564007997512817, 0.22506150603294373, 0.11853761970996857, 0.0, 0.0, 0.0], [0.6958062648773193, 0.14701850712299347, 0.07145983725786209, 0.08571550250053406, 0.0, 0.0], [0.6353274583816528, 0.1346064656972885, 0.030994214117527008, 0.056916315108537674, 0.1421555131673813, 0.0], [0.6779401898384094, 0.053654152899980545, 0.01800631172955036, 0.06284520775079727, 0.1103820651769638, 0.07717210054397583]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822334051132202, 0.017766647040843964, 0.0, 0.0, 0.0, 0.0], [0.9037663340568542, 0.06541544198989868, 0.03081829659640789, 0.0, 0.0, 0.0], [0.8119193911552429, 0.03679030388593674, 0.060560714453458786, 0.09072960168123245, 0.0, 0.0], [0.40546438097953796, 0.10383912175893784, 0.10211236774921417, 0.35434210300445557, 0.03424208238720894, 0.0], [0.22824221849441528, 0.017278727144002914, 0.05055465176701546, 0.6015752553939819, 0.09411764144897461, 0.008231506682932377]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873148202896118, 0.012685136869549751, 0.0, 0.0, 0.0, 0.0], [0.35445743799209595, 0.5317603349685669, 0.11378221958875656, 0.0, 0.0, 0.0], [0.07823363691568375, 0.7221359014511108, 0.10936623811721802, 0.090264230966568, 0.0, 0.0], [0.21967869997024536, 0.4048435091972351, 0.12358088046312332, 0.20018866658210754, 0.051708199083805084, 0.0], [0.36089760065078735, 0.10459021478891373, 0.06983799487352371, 0.2976483404636383, 0.13869903981685638, 0.02832675166428089]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732162356376648, 0.0267837755382061, 0.0, 0.0, 0.0, 0.0], [0.9167553782463074, 0.061452705413103104, 0.02179192565381527, 0.0, 0.0, 0.0], [0.8543081283569336, 0.08049600571393967, 0.030334919691085815, 0.03486092761158943, 0.0, 0.0], [0.8919214606285095, 0.04280779883265495, 0.022045055404305458, 0.023470671847462654, 0.01975487545132637, 0.0], [0.8116763234138489, 0.03413533419370651, 0.03567665070295334, 0.04748587682843208, 0.0253971628844738, 0.04562860727310181]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502761960029602, 0.04972382262349129, 0.0, 0.0, 0.0, 0.0], [0.7637454271316528, 0.2007361352443695, 0.03551840782165527, 0.0, 0.0, 0.0], [0.6279097199440002, 0.03768139332532883, 0.1994536966085434, 0.13495522737503052, 0.0, 0.0], [0.6397060751914978, 0.027007432654500008, 0.09082036465406418, 0.20653828978538513, 0.03592785820364952, 0.0], [0.4559425115585327, 0.021641194820404053, 0.12939567863941193, 0.21800927817821503, 0.10379841923713684, 0.07121295481920242]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498406648635864, 0.050159383565187454, 0.0, 0.0, 0.0, 0.0], [0.8688724637031555, 0.0872218981385231, 0.043905653059482574, 0.0, 0.0, 0.0], [0.6937950253486633, 0.06359200924634933, 0.091790571808815, 0.15082231163978577, 0.0, 0.0], [0.7266597151756287, 0.04389883577823639, 0.04683985933661461, 0.09851823002099991, 0.08408336341381073, 0.0], [0.7848998308181763, 0.037147827446460724, 0.012907838448882103, 0.01053939200937748, 0.12079165875911713, 0.03371351957321167]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891054034233093, 0.01089458167552948, 0.0, 0.0, 0.0, 0.0], [0.8929519653320312, 0.08700055629014969, 0.02004752680659294, 0.0, 0.0, 0.0], [0.7891124486923218, 0.09797251224517822, 0.08633202314376831, 0.026582980528473854, 0.0, 0.0], [0.8850635886192322, 0.03645012155175209, 0.05395457148551941, 0.01237727515399456, 0.012154522351920605, 0.0], [0.6861329674720764, 0.05720378831028938, 0.011636304669082165, 0.021660611033439636, 0.1748800277709961, 0.048486363142728806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9396191835403442, 0.06038080155849457, 0.0, 0.0, 0.0, 0.0], [0.7851794958114624, 0.19751444458961487, 0.017306052148342133, 0.0, 0.0, 0.0], [0.7660509943962097, 0.15444670617580414, 0.03188290074467659, 0.04761936888098717, 0.0, 0.0], [0.703522801399231, 0.05171430483460426, 0.07760990411043167, 0.1533905267715454, 0.013762423768639565, 0.0], [0.7121888399124146, 0.04994234815239906, 0.03772548958659172, 0.08649132400751114, 0.06541401147842407, 0.04823806509375572]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.974072277545929, 0.025927715003490448, 0.0, 0.0, 0.0, 0.0], [0.792539656162262, 0.01171559002250433, 0.19574476778507233, 0.0, 0.0, 0.0], [0.5106770992279053, 0.007296787109225988, 0.039619915187358856, 0.4424062669277191, 0.0, 0.0], [0.5862472057342529, 0.012099712155759335, 0.024585209786891937, 0.06737840175628662, 0.30968940258026123, 0.0], [0.30196306109428406, 0.007724012713879347, 0.011518122628331184, 0.046947259455919266, 0.22146707773208618, 0.41038045287132263]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9744554162025452, 0.02554464340209961, 0.0, 0.0, 0.0, 0.0], [0.9769195318222046, 0.015048524364829063, 0.008031901903450489, 0.0, 0.0, 0.0], [0.9060619473457336, 0.025875424966216087, 0.025954782962799072, 0.04210779070854187, 0.0, 0.0], [0.9400081038475037, 0.00555665697902441, 0.005828304681926966, 0.031757812947034836, 0.016849134117364883, 0.0], [0.9105738401412964, 0.0019752182997763157, 0.008646721951663494, 0.013360846787691116, 0.03543964773416519, 0.030003678053617477]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791666865348816, 0.020833350718021393, 0.0, 0.0, 0.0, 0.0], [0.8444858193397522, 0.13507869839668274, 0.020435383543372154, 0.0, 0.0, 0.0], [0.7903086543083191, 0.14559169113636017, 0.037529975175857544, 0.026569725945591927, 0.0, 0.0], [0.7298924326896667, 0.056496407836675644, 0.032735615968704224, 0.10400459170341492, 0.07687094807624817, 0.0], [0.5684185028076172, 0.04388832300901413, 0.026293467730283737, 0.0811714455485344, 0.24314835667610168, 0.037079911679029465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9499868154525757, 0.05001320689916611, 0.0, 0.0, 0.0, 0.0], [0.9336170554161072, 0.05848868936300278, 0.007894262671470642, 0.0, 0.0, 0.0], [0.7897834181785583, 0.11071821302175522, 0.05360178276896477, 0.04589657858014107, 0.0, 0.0], [0.885930061340332, 0.05752986669540405, 0.01374326553195715, 0.0033877466339617968, 0.03940902277827263, 0.0], [0.9337607622146606, 0.02647063508629799, 0.004523396957665682, 0.0061904797330498695, 0.014132906682789326, 0.014921708963811398]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.8521224554035598e-09, 0.0, 0.0, 0.0, 0.0], [6.6758907451003324e-06, 0.9999804496765137, 1.2841281204600818e-05, 0.0, 0.0, 0.0], [2.2194194926328237e-08, 2.6684581211355862e-09, 0.9999971389770508, 2.8136880700913025e-06, 0.0, 0.0], [1.0145409987671883e-06, 4.464065739284706e-08, 0.00035356366424821317, 0.9993677735328674, 0.0002776293840724975, 0.0], [9.436550429953172e-10, 1.382057315812979e-11, 5.017835036369434e-10, 2.965183876213473e-09, 0.9999971389770508, 2.8644042231462663e-06]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9948632121086121, 0.005136783700436354, 0.0, 0.0, 0.0, 0.0], [0.9274215698242188, 0.01832387037575245, 0.05425456911325455, 0.0, 0.0, 0.0], [0.9678993225097656, 0.004143435508012772, 0.004314453341066837, 0.023642776533961296, 0.0, 0.0], [0.8999068737030029, 0.001467161695472896, 0.00029133574571460485, 0.002585014794021845, 0.09574954956769943, 0.0], [0.9386115670204163, 0.00022248300956562161, 0.0006146665546111763, 0.0015495637198910117, 0.030689461156725883, 0.028312424197793007]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999959468841553, 4.042720775032649e-06, 0.0, 0.0, 0.0, 0.0], [0.9982761144638062, 3.2613831990602193e-06, 0.001720669330097735, 0.0, 0.0, 0.0], [0.9998809099197388, 5.328835683826583e-08, 6.376215537784446e-07, 0.00011847059795400128, 0.0, 0.0], [0.9996154308319092, 3.473169556400535e-07, 3.8920820344401363e-08, 4.468433303372876e-07, 0.00038369710091501474, 0.0], [0.9994840621948242, 1.655020476221125e-08, 2.8715557931491276e-08, 1.0638284493325045e-06, 0.0002126671897713095, 0.00030212008277885616]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514135718345642, 0.048586405813694, 0.0, 0.0, 0.0, 0.0], [0.5749948024749756, 0.39028096199035645, 0.03472418338060379, 0.0, 0.0, 0.0], [0.7442318201065063, 0.1752411425113678, 0.0756477490067482, 0.004879283253103495, 0.0, 0.0], [0.5232070684432983, 0.09429339319467545, 0.1138191670179367, 0.19979268312454224, 0.06888769567012787, 0.0], [0.47472575306892395, 0.05636607110500336, 0.04530389606952667, 0.06967321783304214, 0.3098014295101166, 0.0441296212375164]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8734648823738098, 0.12653514742851257, 0.0, 0.0, 0.0, 0.0], [0.6097912788391113, 0.3541727066040039, 0.036036062985658646, 0.0, 0.0, 0.0], [0.45984190702438354, 0.38697871565818787, 0.0996011346578598, 0.05357823893427849, 0.0, 0.0], [0.572220504283905, 0.23636263608932495, 0.08344558626413345, 0.06921917200088501, 0.03875211998820305, 0.0], [0.5143564343452454, 0.16723087430000305, 0.09019406139850616, 0.0765448659658432, 0.10578085482120514, 0.04589281603693962]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.981228768825531, 0.018771231174468994, 0.0, 0.0, 0.0, 0.0], [0.6142941117286682, 0.3503977954387665, 0.0353081189095974, 0.0, 0.0, 0.0], [0.5770686268806458, 0.32858458161354065, 0.05508256331086159, 0.03926428034901619, 0.0, 0.0], [0.17188192903995514, 0.011042501777410507, 0.054578714072704315, 0.7326585650444031, 0.029838265851140022, 0.0], [0.3783015012741089, 0.017070062458515167, 0.021754134446382523, 0.4409688115119934, 0.06093813106417656, 0.08096737414598465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9923112392425537, 0.007688735146075487, 0.0, 0.0, 0.0, 0.0], [0.9498787522315979, 0.016709784045815468, 0.03341152146458626, 0.0, 0.0, 0.0], [0.9961295127868652, 0.0008787295082584023, 0.0006868162308819592, 0.0023048371076583862, 0.0, 0.0], [0.9935757517814636, 0.0032634998206049204, 0.0009993825806304812, 0.00027932299417443573, 0.0018820574041455984, 0.0], [0.9907532930374146, 0.00021344318520277739, 0.0004595233185682446, 0.0007905619568191469, 0.004424723796546459, 0.003358350833877921]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647740125656128, 0.03522596135735512, 0.0, 0.0, 0.0, 0.0], [0.8194130063056946, 0.1365436613559723, 0.04404333233833313, 0.0, 0.0, 0.0], [0.7584245800971985, 0.006878929678350687, 0.20653395354747772, 0.028162529692053795, 0.0, 0.0], [0.5298128128051758, 0.002678812015801668, 0.07857988774776459, 0.3598373234272003, 0.02909109927713871, 0.0], [0.7544413208961487, 0.00036782227107323706, 0.0019713479559868574, 0.00324004958383739, 0.1942344754934311, 0.04574500769376755]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9749131202697754, 0.02508680149912834, 0.0, 0.0, 0.0, 0.0], [0.9306471943855286, 0.05705660209059715, 0.012296222150325775, 0.0, 0.0, 0.0], [0.9305251836776733, 0.052770983427762985, 0.01111945416778326, 0.005584415514022112, 0.0, 0.0], [0.8863320350646973, 0.01292418036609888, 0.017724711447954178, 0.06150198355317116, 0.021517015993595123, 0.0], [0.791684627532959, 0.015036096796393394, 0.0317479707300663, 0.03392200171947479, 0.03707978501915932, 0.09052948653697968]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608501195907593, 0.039149850606918335, 0.0, 0.0, 0.0, 0.0], [0.9121272563934326, 0.02257651649415493, 0.06529619544744492, 0.0, 0.0, 0.0], [0.9364108443260193, 0.015584447421133518, 0.024544963613152504, 0.02345985174179077, 0.0, 0.0], [0.9454620480537415, 0.006762288510799408, 0.022026237100362778, 0.009137796238064766, 0.016611700877547264, 0.0], [0.8346164226531982, 0.001881699077785015, 0.00560904573649168, 0.01887359470129013, 0.12449200451374054, 0.014527074061334133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964227080345154, 0.0035772807896137238, 0.0, 0.0, 0.0, 0.0], [0.9713928699493408, 0.024453025311231613, 0.004154058638960123, 0.0, 0.0, 0.0], [0.9735792279243469, 0.019003381952643394, 0.003664410673081875, 0.0037529165856540203, 0.0, 0.0], [0.9586312174797058, 0.007116180844604969, 0.009218388237059116, 0.022725583985447884, 0.0023084774147719145, 0.0], [0.973607063293457, 0.008490582928061485, 0.0032512471079826355, 0.003606445388868451, 0.004877461586147547, 0.006167212035506964]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.97598797082901, 0.024011990055441856, 0.0, 0.0, 0.0, 0.0], [0.9460638165473938, 0.04211375489830971, 0.011822436936199665, 0.0, 0.0, 0.0], [0.8446813225746155, 0.04293116182088852, 0.05218198522925377, 0.06020559370517731, 0.0, 0.0], [0.9378372430801392, 0.03354858607053757, 0.008826455101370811, 0.0028792242519557476, 0.016908427700400352, 0.0], [0.8124931454658508, 0.02696753479540348, 0.05999218672513962, 0.03445731848478317, 0.011011860333383083, 0.05507794767618179]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9001203775405884, 0.09987961500883102, 0.0, 0.0, 0.0, 0.0], [0.627193033695221, 0.07988718152046204, 0.29291975498199463, 0.0, 0.0, 0.0], [0.7624077796936035, 0.02734432928264141, 0.038679543882608414, 0.17156831920146942, 0.0, 0.0], [0.7995968461036682, 0.014336260966956615, 0.01437566988170147, 0.025438452139496803, 0.14625284075737, 0.0], [0.7851970791816711, 0.04204057529568672, 0.025253651663661003, 0.02908395044505596, 0.029306314885616302, 0.08911846578121185]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954467415809631, 0.0045532057993113995, 0.0, 0.0, 0.0, 0.0], [0.9356001615524292, 0.04476744681596756, 0.019632352516055107, 0.0, 0.0, 0.0], [0.5605552792549133, 0.09861977398395538, 0.29983264207839966, 0.040992289781570435, 0.0, 0.0], [0.5893709659576416, 0.11000988632440567, 0.08033622056245804, 0.16754034161567688, 0.05274256691336632, 0.0], [0.22305884957313538, 0.05680817365646362, 0.05467984080314636, 0.24733951687812805, 0.3111244738101959, 0.1069890558719635]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9301451444625854, 0.06985488533973694, 0.0, 0.0, 0.0, 0.0], [0.8936478495597839, 0.08535721153020859, 0.020994966849684715, 0.0, 0.0, 0.0], [0.8404538035392761, 0.10619214922189713, 0.02363673783838749, 0.029717326164245605, 0.0, 0.0], [0.8927386403083801, 0.024784674867987633, 0.008319000713527203, 0.05165454372763634, 0.022503145039081573, 0.0], [0.8646610975265503, 0.009503193199634552, 0.0024329854641109705, 0.04796753078699112, 0.04273205250501633, 0.03270319849252701]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859625697135925, 0.014037408865988255, 0.0, 0.0, 0.0, 0.0], [0.9702037572860718, 0.0168070700019598, 0.012989125214517117, 0.0, 0.0, 0.0], [0.9524770379066467, 0.016064459457993507, 0.013456220738589764, 0.018002323806285858, 0.0, 0.0], [0.9332928657531738, 0.01897200010716915, 0.02014683373272419, 0.017023753374814987, 0.010564540512859821, 0.0], [0.9113592505455017, 0.012528638355433941, 0.02209620550274849, 0.01751861348748207, 0.018517911434173584, 0.01797938533127308]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9681769013404846, 0.03182310611009598, 0.0, 0.0, 0.0, 0.0], [0.9096417427062988, 0.07916690409183502, 0.011191264726221561, 0.0, 0.0, 0.0], [0.8379932045936584, 0.13078266382217407, 0.012140989303588867, 0.019083037972450256, 0.0, 0.0], [0.9116525053977966, 0.05451957508921623, 0.009499342180788517, 0.00746585289016366, 0.01686275750398636, 0.0], [0.8510289192199707, 0.07338211685419083, 0.008022507652640343, 0.009083161130547523, 0.04261006414890289, 0.015873271971940994]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799023866653442, 0.020097682252526283, 0.0, 0.0, 0.0, 0.0], [0.9558742642402649, 0.029063312336802483, 0.015062497928738594, 0.0, 0.0, 0.0], [0.7943133115768433, 0.06074100360274315, 0.06907659024000168, 0.07586916536092758, 0.0, 0.0], [0.5494324564933777, 0.03154711425304413, 0.05482015758752823, 0.05788077041506767, 0.3063195049762726, 0.0], [0.6453980803489685, 0.010770943015813828, 0.017528092488646507, 0.02157985046505928, 0.24958276748657227, 0.05514020845293999]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9506809115409851, 0.0493190623819828, 0.0, 0.0, 0.0, 0.0], [0.8553215265274048, 0.09256264567375183, 0.05211575701832771, 0.0, 0.0, 0.0], [0.850852370262146, 0.04734604433178902, 0.044177331030368805, 0.057624250650405884, 0.0, 0.0], [0.7697131633758545, 0.02788589708507061, 0.031017286702990532, 0.06842502951622009, 0.1029587835073471, 0.0], [0.7931903004646301, 0.04052198305726051, 0.029242033138871193, 0.04478124529123306, 0.04894689470529556, 0.04331749677658081]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9770310521125793, 0.02296893112361431, 0.0, 0.0, 0.0, 0.0], [0.9429817199707031, 0.017321482300758362, 0.03969680890440941, 0.0, 0.0, 0.0], [0.9144344925880432, 0.008583576418459415, 0.013035810552537441, 0.06394599378108978, 0.0, 0.0], [0.9222429990768433, 0.0036440351977944374, 0.003740275977179408, 0.010410364717245102, 0.05996239185333252, 0.0], [0.9198879599571228, 0.0030822583939880133, 0.0034827394410967827, 0.004206796642392874, 0.02125428058207035, 0.048085976392030716]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.977458119392395, 0.022541873157024384, 0.0, 0.0, 0.0, 0.0], [0.8929325342178345, 0.07475466281175613, 0.032312843948602676, 0.0, 0.0, 0.0], [0.8423511385917664, 0.05980278551578522, 0.03740081936120987, 0.06044524535536766, 0.0, 0.0], [0.7674624919891357, 0.03536349534988403, 0.042155250906944275, 0.06658654659986496, 0.08843226730823517, 0.0], [0.6182611584663391, 0.01611059531569481, 0.020167622715234756, 0.03868892416357994, 0.23147016763687134, 0.07530155777931213]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9634856581687927, 0.036514393985271454, 0.0, 0.0, 0.0, 0.0], [0.4363938570022583, 0.522637128829956, 0.04096902906894684, 0.0, 0.0, 0.0], [0.3608614206314087, 0.35129693150520325, 0.2655103802680969, 0.022331148386001587, 0.0, 0.0], [0.3942921757698059, 0.021704670041799545, 0.07794328778982162, 0.37168896198272705, 0.1343708038330078, 0.0], [0.6310713887214661, 0.01698400266468525, 0.025942081585526466, 0.08615949749946594, 0.2183200567960739, 0.021522950381040573]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9988250136375427, 0.0011750265257433057, 0.0, 0.0, 0.0, 0.0], [0.9944871068000793, 0.0004826401418540627, 0.0050302306190133095, 0.0, 0.0, 0.0], [0.9981209635734558, 2.705173392314464e-05, 0.0001130745149566792, 0.0017389442073181272, 0.0, 0.0], [0.9982239603996277, 6.83655816828832e-05, 0.00010199935059063137, 6.028370262356475e-05, 0.0015453165397047997, 0.0], [0.9982888102531433, 1.055222810464329e-06, 3.2781026675365865e-05, 0.00013038977340329438, 0.0006605894886888564, 0.0008863684488460422]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936710596084595, 0.006328921765089035, 0.0, 0.0, 0.0, 0.0], [0.9727688431739807, 0.0018561368342489004, 0.025375060737133026, 0.0, 0.0, 0.0], [0.9724299907684326, 0.0019586149137467146, 0.011192461475729942, 0.014418890699744225, 0.0, 0.0], [0.9782041311264038, 0.0009589138207957149, 0.0018706483533605933, 0.006326568778604269, 0.012639678083360195, 0.0], [0.9592596888542175, 0.0024555064737796783, 0.00161241355817765, 0.005019655916839838, 0.006687097251415253, 0.024965662509202957]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629000425338745, 0.03709998354315758, 0.0, 0.0, 0.0, 0.0], [0.36801934242248535, 0.6152258515357971, 0.016754813492298126, 0.0, 0.0, 0.0], [0.3173511326313019, 0.6140013337135315, 0.05375149846076965, 0.014896026812493801, 0.0, 0.0], [0.48987284302711487, 0.21071474254131317, 0.04693019017577171, 0.20700432360172272, 0.04547784850001335, 0.0], [0.48774227499961853, 0.1769528090953827, 0.06915216147899628, 0.09849268198013306, 0.12091436982154846, 0.046745721250772476]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9794419407844543, 0.020558049902319908, 0.0, 0.0, 0.0, 0.0], [0.6677903532981873, 0.31032365560531616, 0.021886007860302925, 0.0, 0.0, 0.0], [0.7118757367134094, 0.11108540743589401, 0.14187385141849518, 0.03516504913568497, 0.0, 0.0], [0.4501457214355469, 0.04036055505275726, 0.040458209812641144, 0.388570100069046, 0.08046531677246094, 0.0], [0.49346262216567993, 0.013696977868676186, 0.008126799948513508, 0.13074499368667603, 0.3086138069629669, 0.04535480588674545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846054315567017, 0.015394587069749832, 0.0, 0.0, 0.0, 0.0], [0.9806739091873169, 0.007713791914284229, 0.011612347327172756, 0.0, 0.0, 0.0], [0.932663083076477, 0.01957838423550129, 0.02410353161394596, 0.023654978722333908, 0.0, 0.0], [0.9422016739845276, 0.0009538981830701232, 0.0010898025939241052, 0.00319337984547019, 0.05256118252873421, 0.0], [0.9352930784225464, 0.0010279357666149735, 0.004444425459951162, 0.001637140172533691, 0.010590963996946812, 0.04700646549463272]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985783100128174, 0.0014216724084690213, 0.0, 0.0, 0.0, 0.0], [0.9893348813056946, 0.0011178902350366116, 0.00954714696854353, 0.0, 0.0, 0.0], [0.9979978203773499, 7.997050124686211e-05, 0.00013218850654084235, 0.0017900333041325212, 0.0, 0.0], [0.9986976385116577, 4.1044117097044364e-05, 3.8683547245454974e-06, 2.3676282580709085e-05, 0.0012337174266576767, 0.0], [0.9971563816070557, 1.852225250331685e-05, 1.8826559653462027e-06, 2.7900125132873654e-05, 0.0006533482228405774, 0.0021419788245111704]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9768233299255371, 0.023176640272140503, 0.0, 0.0, 0.0, 0.0], [0.9194678068161011, 0.05088186264038086, 0.029650341719388962, 0.0, 0.0, 0.0], [0.8474554419517517, 0.06100169196724892, 0.04372376948595047, 0.04781914874911308, 0.0, 0.0], [0.8011623620986938, 0.041866958141326904, 0.04375807195901871, 0.041894737631082535, 0.07131782174110413, 0.0], [0.8031871914863586, 0.02450493723154068, 0.017323585227131844, 0.04744395986199379, 0.06109930947422981, 0.046441152691841125]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829428195953369, 0.01705716922879219, 0.0, 0.0, 0.0, 0.0], [0.8863736987113953, 0.09492647647857666, 0.018699750304222107, 0.0, 0.0, 0.0], [0.9231085777282715, 0.03696346655488014, 0.032198335975408554, 0.007729663979262114, 0.0, 0.0], [0.9068527221679688, 0.016046639531850815, 0.014310522936284542, 0.04543786868453026, 0.017352323979139328, 0.0], [0.6555973887443542, 0.05091019719839096, 0.028384855017066002, 0.1256549060344696, 0.10546853393316269, 0.03398407623171806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502318501472473, 0.049768079072237015, 0.0, 0.0, 0.0, 0.0], [0.8829865455627441, 0.1000962108373642, 0.01691717840731144, 0.0, 0.0, 0.0], [0.8057457804679871, 0.14463546872138977, 0.03018922731280327, 0.019429458305239677, 0.0, 0.0], [0.8706230521202087, 0.032440632581710815, 0.026951627805829048, 0.04410304129123688, 0.025881657376885414, 0.0], [0.688364565372467, 0.009681451134383678, 0.016449343413114548, 0.0987110361456871, 0.08971209079027176, 0.09708156436681747]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9792683124542236, 0.02073168195784092, 0.0, 0.0, 0.0, 0.0], [0.9523284435272217, 0.025933818891644478, 0.021737735718488693, 0.0, 0.0, 0.0], [0.9144353270530701, 0.017671240493655205, 0.022358495742082596, 0.04553484544157982, 0.0, 0.0], [0.9448292851448059, 0.006467597559094429, 0.006386063527315855, 0.03263096138834953, 0.00968620739877224, 0.0], [0.9347906112670898, 0.007862505502998829, 0.007788175716996193, 0.021432818844914436, 0.008491144515573978, 0.01963483914732933]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983370304107666, 0.016629677265882492, 0.0, 0.0, 0.0, 0.0], [0.963111400604248, 0.009229931980371475, 0.027658598497509956, 0.0, 0.0, 0.0], [0.9706628322601318, 0.0041494048200547695, 0.0068131014704704285, 0.018374638631939888, 0.0, 0.0], [0.987951934337616, 0.002165885642170906, 0.00034901127219200134, 0.001583816367201507, 0.00794942770153284, 0.0], [0.9457950592041016, 0.014583553187549114, 0.0003652951563708484, 0.0009569536778144538, 0.013621564954519272, 0.02467755414545536]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878059029579163, 0.01219407469034195, 0.0, 0.0, 0.0, 0.0], [0.87103670835495, 0.09448163211345673, 0.03448161482810974, 0.0, 0.0, 0.0], [0.6309783458709717, 0.11090382188558578, 0.1923021823167801, 0.06581564992666245, 0.0, 0.0], [0.5360490083694458, 0.04618944972753525, 0.13605308532714844, 0.26455509662628174, 0.017153292894363403, 0.0], [0.8287520408630371, 0.023732755333185196, 0.02008037269115448, 0.07245264202356339, 0.030431220307946205, 0.024550989270210266]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995685577392578, 0.10043150931596756, 0.0, 0.0, 0.0, 0.0], [0.270343542098999, 0.6504329442977905, 0.07922357320785522, 0.0, 0.0, 0.0], [0.20541730523109436, 0.5892508625984192, 0.18085837364196777, 0.024473490193486214, 0.0, 0.0], [0.5573861002922058, 0.1774134784936905, 0.08806808292865753, 0.09881848096847534, 0.07831384986639023, 0.0], [0.5922912359237671, 0.08700639009475708, 0.05643285810947418, 0.05685883015394211, 0.12181518226861954, 0.08559554070234299]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9316380620002747, 0.06836195290088654, 0.0, 0.0, 0.0, 0.0], [0.9572945833206177, 0.026243582367897034, 0.0164618119597435, 0.0, 0.0, 0.0], [0.9880544543266296, 0.00427332753315568, 0.002954584313556552, 0.004717645235359669, 0.0, 0.0], [0.99403977394104, 0.0009413420339114964, 0.0004739820142276585, 0.00011646930943243206, 0.004428447224199772, 0.0], [0.9806035161018372, 2.5468933017691597e-05, 0.00016239412070717663, 0.0001476418401580304, 0.0013442443450912833, 0.017716845497488976]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993178129196167, 0.006821857299655676, 0.0, 0.0, 0.0, 0.0], [0.9756524562835693, 0.01318411435931921, 0.011163423769176006, 0.0, 0.0, 0.0], [0.9418966770172119, 0.004721744451671839, 0.0023818055633455515, 0.050999753177165985, 0.0, 0.0], [0.9905040860176086, 0.0022848136723041534, 6.198462506290525e-05, 0.0005984465242363513, 0.006550676189363003, 0.0], [0.9697660207748413, 0.0008878845837898552, 0.00023466735729016364, 0.0017040816601365805, 0.004128355998545885, 0.02327893301844597]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716231822967529, 0.02837684564292431, 0.0, 0.0, 0.0, 0.0], [0.9223619699478149, 0.028907248750329018, 0.048730745911598206, 0.0, 0.0, 0.0], [0.8426317572593689, 0.023872116580605507, 0.04748132824897766, 0.08601479232311249, 0.0, 0.0], [0.8521121740341187, 0.020744236186146736, 0.04494619369506836, 0.05765002593398094, 0.02454746514558792, 0.0], [0.8800725936889648, 0.022448532283306122, 0.018235722556710243, 0.01925482600927353, 0.015854258090257645, 0.044134121388196945]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9412723779678345, 0.058727629482746124, 0.0, 0.0, 0.0, 0.0], [0.916313886642456, 0.05759201943874359, 0.02609400637447834, 0.0, 0.0, 0.0], [0.8392423391342163, 0.057690516114234924, 0.01382902916520834, 0.08923812955617905, 0.0, 0.0], [0.8987162113189697, 0.0134778693318367, 0.0003456450067460537, 0.003298751311376691, 0.08416149020195007, 0.0], [0.8701692223548889, 0.002700856188312173, 0.00143499206751585, 0.0056661744602024555, 0.08874300867319107, 0.031285665929317474]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9656725525856018, 0.03432750701904297, 0.0, 0.0, 0.0, 0.0], [0.9178615808486938, 0.062257930636405945, 0.019880469888448715, 0.0, 0.0, 0.0], [0.823314905166626, 0.06282395124435425, 0.03670429438352585, 0.07715693861246109, 0.0, 0.0], [0.8501748442649841, 0.03816927224397659, 0.03196492791175842, 0.0516013503074646, 0.02808968350291252, 0.0], [0.6572404503822327, 0.05877397954463959, 0.04336007311940193, 0.09013211727142334, 0.08146599680185318, 0.06902744621038437]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9162061810493469, 0.0837937667965889, 0.0, 0.0, 0.0, 0.0], [0.9451773762702942, 0.04099284112453461, 0.013829832896590233, 0.0, 0.0, 0.0], [0.8928355574607849, 0.05368670076131821, 0.017596954479813576, 0.03588071092963219, 0.0, 0.0], [0.8337052464485168, 0.04799601063132286, 0.033513229340314865, 0.04680858924984932, 0.03797686845064163, 0.0], [0.8167192339897156, 0.06337132304906845, 0.013286277651786804, 0.020469767972826958, 0.025292355567216873, 0.06086111441254616]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525133371353149, 0.04748663306236267, 0.0, 0.0, 0.0, 0.0], [0.3019869327545166, 0.6520938873291016, 0.04591925069689751, 0.0, 0.0, 0.0], [0.285582959651947, 0.556952178478241, 0.1444743126630783, 0.012990524061024189, 0.0, 0.0], [0.843804121017456, 0.032251205295324326, 0.03954290598630905, 0.06848159432411194, 0.015920041128993034, 0.0], [0.6664940714836121, 0.06095913052558899, 0.04064354673027992, 0.06804485619068146, 0.09186329692602158, 0.07199501991271973]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682655334472656, 0.031734466552734375, 0.0, 0.0, 0.0, 0.0], [0.738521933555603, 0.22856839001178741, 0.032909639179706573, 0.0, 0.0, 0.0], [0.5946676135063171, 0.2303314357995987, 0.14867636561393738, 0.02632458508014679, 0.0, 0.0], [0.6339254975318909, 0.05813034623861313, 0.09654320776462555, 0.14291946589946747, 0.06848153471946716, 0.0], [0.40375572443008423, 0.08945391327142715, 0.07635112851858139, 0.25587135553359985, 0.1433039754629135, 0.03126389905810356]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9869793653488159, 0.013020593672990799, 0.0, 0.0, 0.0, 0.0], [0.8631385564804077, 0.1105666309595108, 0.02629482001066208, 0.0, 0.0, 0.0], [0.9488080143928528, 0.028614996001124382, 0.006535546388477087, 0.016041526570916176, 0.0, 0.0], [0.9672170877456665, 0.006604980677366257, 0.00045171406236477196, 0.004844417329877615, 0.020881708711385727, 0.0], [0.9354621171951294, 0.02047806605696678, 0.0011700231116265059, 0.007056943140923977, 0.0163181871175766, 0.019514625892043114]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846673011779785, 0.015332723967730999, 0.0, 0.0, 0.0, 0.0], [0.9052747488021851, 0.08373606950044632, 0.010989243164658546, 0.0, 0.0, 0.0], [0.8145939111709595, 0.04283742979168892, 0.10568301379680634, 0.03688570484519005, 0.0, 0.0], [0.23519809544086456, 0.012018457986414433, 0.05280117318034172, 0.6516180038452148, 0.04836418479681015, 0.0], [0.31818512082099915, 0.018632443621754646, 0.03948190063238144, 0.3755541741847992, 0.20787373185157776, 0.04027257487177849]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9811733365058899, 0.018826685845851898, 0.0, 0.0, 0.0, 0.0], [0.8618939518928528, 0.06479164958000183, 0.07331438362598419, 0.0, 0.0, 0.0], [0.7664540410041809, 0.07330425828695297, 0.10353513062000275, 0.056706514209508896, 0.0, 0.0], [0.8128499984741211, 0.03215480223298073, 0.059005625545978546, 0.05416511744260788, 0.04182446748018265, 0.0], [0.8687856197357178, 0.026987861841917038, 0.02047000452876091, 0.01629738137125969, 0.03218390792608261, 0.03527523949742317]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9264583587646484, 0.07354167848825455, 0.0, 0.0, 0.0, 0.0], [0.8403540849685669, 0.06373751163482666, 0.09590838104486465, 0.0, 0.0, 0.0], [0.7330995798110962, 0.06451118737459183, 0.10380073636770248, 0.09858842939138412, 0.0, 0.0], [0.9143612384796143, 0.008257776498794556, 0.007320381235331297, 0.017966248095035553, 0.05209439620375633, 0.0], [0.8971915245056152, 0.008555498905479908, 0.007019453682005405, 0.014860544353723526, 0.03399762138724327, 0.03837529569864273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180347919464111, 0.08196526020765305, 0.0, 0.0, 0.0, 0.0], [0.8328666687011719, 0.1219901517033577, 0.04514322429895401, 0.0, 0.0, 0.0], [0.7994157075881958, 0.0874413549900055, 0.03605784848332405, 0.07708510011434555, 0.0, 0.0], [0.880984902381897, 0.020749641582369804, 0.020554615184664726, 0.017120830714702606, 0.06058995798230171, 0.0], [0.745303213596344, 0.044334057718515396, 0.022549288347363472, 0.0331527441740036, 0.03357058763504028, 0.12109009176492691]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867060780525208, 0.013293893076479435, 0.0, 0.0, 0.0, 0.0], [0.982177734375, 0.012414131313562393, 0.005408108700066805, 0.0, 0.0, 0.0], [0.9630486369132996, 0.015290752984583378, 0.010345698334276676, 0.0113149369135499, 0.0, 0.0], [0.9213568568229675, 0.014132463373243809, 0.017639216035604477, 0.016567690297961235, 0.030303770676255226, 0.0], [0.9373326301574707, 0.009064299054443836, 0.007548365276306868, 0.006576443091034889, 0.011827622540295124, 0.027650514617562294]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9951004385948181, 0.00489962799474597, 0.0, 0.0, 0.0, 0.0], [0.9476007223129272, 0.041407931596040726, 0.010991275310516357, 0.0, 0.0, 0.0], [0.9142175316810608, 0.023523783311247826, 0.039145033806562424, 0.023113621398806572, 0.0, 0.0], [0.9534738659858704, 0.008932933211326599, 0.015272765420377254, 0.007908251136541367, 0.014412266202270985, 0.0], [0.9427101016044617, 0.00823307130485773, 0.004650997929275036, 0.004178107250481844, 0.005463531706482172, 0.03476419299840927]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543376564979553, 0.045662373304367065, 0.0, 0.0, 0.0, 0.0], [0.9696040749549866, 0.01954760029911995, 0.01084828469902277, 0.0, 0.0, 0.0], [0.9710449576377869, 0.012425386346876621, 0.008068876340985298, 0.008460716344416142, 0.0, 0.0], [0.9726192951202393, 0.002697656163945794, 0.00044831327977590263, 0.0013814778067171574, 0.022853154689073563, 0.0], [0.9675466418266296, 0.009613442234694958, 0.003203035332262516, 0.00424883933737874, 0.007442260626703501, 0.00794589426368475]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887008666992188, 0.011299116536974907, 0.0, 0.0, 0.0, 0.0], [0.9382632374763489, 0.04204244911670685, 0.019694412127137184, 0.0, 0.0, 0.0], [0.8351995944976807, 0.03487853705883026, 0.05134471505880356, 0.07857715338468552, 0.0, 0.0], [0.9042676687240601, 0.010541575029492378, 0.016426723450422287, 0.025921987369656563, 0.04284200444817543, 0.0], [0.8913140892982483, 0.00891267228871584, 0.005010711494833231, 0.008175632916390896, 0.013514749705791473, 0.07307209819555283]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8693912029266357, 0.13060881197452545, 0.0, 0.0, 0.0, 0.0], [0.3507988452911377, 0.606351912021637, 0.04284917935729027, 0.0, 0.0, 0.0], [0.35475659370422363, 0.3502019941806793, 0.24722407758235931, 0.04781729355454445, 0.0, 0.0], [0.35370609164237976, 0.03527737781405449, 0.09567111730575562, 0.449796199798584, 0.06554921716451645, 0.0], [0.4132595360279083, 0.09055527299642563, 0.05286579951643944, 0.174679696559906, 0.173848956823349, 0.09479076415300369]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629756212234497, 0.037024300545454025, 0.0, 0.0, 0.0, 0.0], [0.9756426811218262, 0.01965854875743389, 0.004698706325143576, 0.0, 0.0, 0.0], [0.9775736927986145, 0.013286248780786991, 0.0025590297300368547, 0.006581062916666269, 0.0, 0.0], [0.9870142936706543, 0.007388236932456493, 0.0009579154429957271, 0.0018318220973014832, 0.0028077505994588137, 0.0], [0.9409245848655701, 0.016633737832307816, 0.0022979143541306257, 0.0058906711637973785, 0.0055129327811300755, 0.02874022163450718]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.962827205657959, 0.037172831594944, 0.0, 0.0, 0.0, 0.0], [0.9582237601280212, 0.024641817435622215, 0.017134377732872963, 0.0, 0.0, 0.0], [0.9351300001144409, 0.015331573784351349, 0.014810982160270214, 0.034727465361356735, 0.0, 0.0], [0.9225171208381653, 0.010528750717639923, 0.011010154150426388, 0.01944003626704216, 0.036503832787275314, 0.0], [0.8420165777206421, 0.04357199743390083, 0.007488282397389412, 0.01496153138577938, 0.02385285682976246, 0.06810864061117172]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9926387071609497, 0.00736132962629199, 0.0, 0.0, 0.0, 0.0], [0.9957393407821655, 0.0033469819463789463, 0.000913690309971571, 0.0, 0.0, 0.0], [0.9869900345802307, 0.001974786864593625, 0.001524551771581173, 0.009510699659585953, 0.0, 0.0], [0.9933527708053589, 0.001020324882119894, 0.00034337223041802645, 0.0010291127255186439, 0.004254369530826807, 0.0], [0.9749016761779785, 0.00043480272870510817, 0.0004306558985263109, 0.0012364407302811742, 0.0015347707085311413, 0.021461669355630875]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9897475242614746, 0.010252462700009346, 0.0, 0.0, 0.0, 0.0], [0.9790639281272888, 0.01650906540453434, 0.0044270907528698444, 0.0, 0.0, 0.0], [0.9521436095237732, 0.029432358220219612, 0.008943161927163601, 0.009480923414230347, 0.0, 0.0], [0.939594030380249, 0.021510960534214973, 0.010278552770614624, 0.004555229097604752, 0.024061163887381554, 0.0], [0.9205074906349182, 0.016153652220964432, 0.010818594135344028, 0.01664440892636776, 0.014566398225724697, 0.021309375762939453]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898501634597778, 0.010149780660867691, 0.0, 0.0, 0.0, 0.0], [0.9820910096168518, 0.006907520350068808, 0.011001535691320896, 0.0, 0.0, 0.0], [0.9684997200965881, 0.008987602777779102, 0.015342563390731812, 0.007170087192207575, 0.0, 0.0], [0.9274120330810547, 0.009485266171395779, 0.022066107019782066, 0.03222890570759773, 0.008807653561234474, 0.0], [0.900665819644928, 0.021623756736516953, 0.013808279298245907, 0.009843860752880573, 0.008521373383700848, 0.04553695768117905]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954444169998169, 0.004555588588118553, 0.0, 0.0, 0.0, 0.0], [0.995254397392273, 0.002460238989442587, 0.002285485854372382, 0.0, 0.0, 0.0], [0.9862446188926697, 0.0015168144600465894, 0.004072288051247597, 0.008166354149580002, 0.0, 0.0], [0.9889963865280151, 0.001226040069013834, 0.0007996349013410509, 0.0006774227367714047, 0.008300574496388435, 0.0], [0.9865202903747559, 0.00039427157025784254, 0.0009571771952323616, 0.0004954367759637535, 0.0009604979422874749, 0.010672281496226788]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9821295142173767, 0.017870500683784485, 0.0, 0.0, 0.0, 0.0], [0.7489436268806458, 0.22002726793289185, 0.031029189005494118, 0.0, 0.0, 0.0], [0.28547799587249756, 0.21125678718090057, 0.47871601581573486, 0.024549242109060287, 0.0, 0.0], [0.8056644201278687, 0.026974644511938095, 0.04302806034684181, 0.06993705034255981, 0.05439583212137222, 0.0], [0.3307209014892578, 0.022326624020934105, 0.016627125442028046, 0.08019453287124634, 0.41574832797050476, 0.13438253104686737]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9697746634483337, 0.030225319787859917, 0.0, 0.0, 0.0, 0.0], [0.9800565838813782, 0.015018894337117672, 0.004924521781504154, 0.0, 0.0, 0.0], [0.9237861037254333, 0.052764780819416046, 0.00630240747705102, 0.017146753147244453, 0.0, 0.0], [0.9451844096183777, 0.03618047758936882, 0.001989208161830902, 0.003958724904805422, 0.012687299400568008, 0.0], [0.9633325934410095, 0.018662991002202034, 0.0030418417882174253, 0.007070912979543209, 0.0050094155594706535, 0.002882065251469612]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873244762420654, 0.012675459496676922, 0.0, 0.0, 0.0, 0.0], [0.9904569983482361, 0.0055419523268938065, 0.004001122899353504, 0.0, 0.0, 0.0], [0.9814971685409546, 0.004653455223888159, 0.003725277027115226, 0.010124054737389088, 0.0, 0.0], [0.9744365811347961, 0.004632251337170601, 0.002379992976784706, 0.006518087349832058, 0.012033028528094292, 0.0], [0.9624497294425964, 0.0033743639942258596, 0.0013198587112128735, 0.0017275003483518958, 0.002944675739854574, 0.028183799237012863]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807674288749695, 0.01923258602619171, 0.0, 0.0, 0.0, 0.0], [0.9664245843887329, 0.015413926914334297, 0.018161438405513763, 0.0, 0.0, 0.0], [0.9632682204246521, 0.004538117907941341, 0.002925391308963299, 0.029268190264701843, 0.0, 0.0], [0.9562349319458008, 0.0012223608791828156, 0.0005304080550558865, 0.00867149606347084, 0.03334089741110802, 0.0], [0.9657101035118103, 0.0009808284230530262, 0.0016686266753822565, 0.002634831238538027, 0.005866361316293478, 0.023139292374253273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639716148376465, 0.036028459668159485, 0.0, 0.0, 0.0, 0.0], [0.9562800526618958, 0.03373315557837486, 0.009986846707761288, 0.0, 0.0, 0.0], [0.8539998531341553, 0.08073022216558456, 0.03334445133805275, 0.031925540417432785, 0.0, 0.0], [0.9547491073608398, 0.009605025872588158, 0.004146162886172533, 0.0020133228972554207, 0.029486361891031265, 0.0], [0.9331137537956238, 0.028699662536382675, 0.005477475933730602, 0.006368075497448444, 0.012613046914339066, 0.013728085905313492]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392993450164795, 0.06070063263177872, 0.0, 0.0, 0.0, 0.0], [0.9298391342163086, 0.061895377933979034, 0.008265496231615543, 0.0, 0.0, 0.0], [0.8471823334693909, 0.09035038203001022, 0.01763608679175377, 0.044831156730651855, 0.0, 0.0], [0.8857703804969788, 0.03918175399303436, 0.007867704145610332, 0.02276589721441269, 0.04441439360380173, 0.0], [0.8563280701637268, 0.10088995099067688, 0.006531452294439077, 0.008485927246510983, 0.007368441205471754, 0.020396249368786812]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8353264331817627, 0.1646735519170761, 0.0, 0.0, 0.0, 0.0], [0.6160858869552612, 0.3137648403644562, 0.07014927268028259, 0.0, 0.0, 0.0], [0.34316325187683105, 0.2758493721485138, 0.1196604073047638, 0.26132699847221375, 0.0, 0.0], [0.5908172130584717, 0.050290752202272415, 0.041665926575660706, 0.2199493646621704, 0.0972767099738121, 0.0], [0.8481413125991821, 0.06318090111017227, 0.014733693562448025, 0.055267371237277985, 0.00901501253247261, 0.009661628864705563]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9627319574356079, 0.03726799786090851, 0.0, 0.0, 0.0, 0.0], [0.7757522463798523, 0.1799626499414444, 0.044285036623477936, 0.0, 0.0, 0.0], [0.6317060589790344, 0.24380716681480408, 0.10925652086734772, 0.015230235643684864, 0.0, 0.0], [0.9539909958839417, 0.018182311207056046, 0.011601822450757027, 0.012299076654016972, 0.003925766795873642, 0.0], [0.40356943011283875, 0.14237558841705322, 0.05661217123270035, 0.1975736767053604, 0.0929921343922615, 0.10687707364559174]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802619218826294, 0.019738124683499336, 0.0, 0.0, 0.0, 0.0], [0.9873908162117004, 0.007800452411174774, 0.004808681085705757, 0.0, 0.0, 0.0], [0.9283918738365173, 0.008301235735416412, 0.01330565195530653, 0.05000120773911476, 0.0, 0.0], [0.8981055021286011, 0.015591299161314964, 0.010177576914429665, 0.039987027645111084, 0.0361386202275753, 0.0], [0.9753499031066895, 0.00035433052107691765, 0.0005866039427928627, 0.0011877501383423805, 0.0010750899091362953, 0.021446440368890762]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9295330047607422, 0.07046692818403244, 0.0, 0.0, 0.0, 0.0], [0.9361506104469299, 0.04116682708263397, 0.022682538256049156, 0.0, 0.0, 0.0], [0.8486821055412292, 0.05802798643708229, 0.024856165051460266, 0.0684337466955185, 0.0, 0.0], [0.8661180734634399, 0.02232467755675316, 0.010369130410254002, 0.02600197121500969, 0.07518619298934937, 0.0], [0.8074421882629395, 0.044382549822330475, 0.01849711686372757, 0.03357789292931557, 0.018561245873570442, 0.07753907144069672]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9680535197257996, 0.03194643557071686, 0.0, 0.0, 0.0, 0.0], [0.9693689942359924, 0.02568492479622364, 0.004946070723235607, 0.0, 0.0, 0.0], [0.9620568156242371, 0.022552406415343285, 0.005471326876431704, 0.009919456206262112, 0.0, 0.0], [0.9727528095245361, 0.010137127712368965, 0.000757327419705689, 0.0028828983195126057, 0.013469807803630829, 0.0], [0.9624635577201843, 0.0031109037809073925, 0.0010007602395489812, 0.0019475930603221059, 0.008266227319836617, 0.02321087196469307]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8542501330375671, 0.14574992656707764, 0.0, 0.0, 0.0, 0.0], [0.9725967645645142, 0.014116315171122551, 0.01328685600310564, 0.0, 0.0, 0.0], [0.9257621765136719, 0.03257262706756592, 0.01461210660636425, 0.027053095400333405, 0.0, 0.0], [0.7923423051834106, 0.027305101975798607, 0.01880674995481968, 0.13854165375232697, 0.023004096001386642, 0.0], [0.6152060627937317, 0.02665526419878006, 0.029352931305766106, 0.05590886250138283, 0.11611279845237732, 0.15676409006118774]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9804654121398926, 0.019534552469849586, 0.0, 0.0, 0.0, 0.0], [0.9882452487945557, 0.007509466726332903, 0.004245325922966003, 0.0, 0.0, 0.0], [0.9584206938743591, 0.0109635591506958, 0.010456060990691185, 0.020159708335995674, 0.0, 0.0], [0.9604811668395996, 0.007182627450674772, 0.003072339342907071, 0.006898913532495499, 0.02236509881913662, 0.0], [0.966888964176178, 0.0032812939025461674, 0.00550054432824254, 0.004234083462506533, 0.005038043484091759, 0.015057181939482689]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498194456100464, 0.05018055811524391, 0.0, 0.0, 0.0, 0.0], [0.9781363606452942, 0.016430046409368515, 0.0054335566237568855, 0.0, 0.0, 0.0], [0.8618696331977844, 0.036093585193157196, 0.07555554062128067, 0.026481209322810173, 0.0, 0.0], [0.5449837446212769, 0.015411133877933025, 0.023516526445746422, 0.25743600726127625, 0.15865260362625122, 0.0], [0.9571874737739563, 0.0030803855042904615, 0.0014446862041950226, 0.006861559115350246, 0.014818714000284672, 0.01660723052918911]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6156560778617859, 0.3843439519405365, 0.0, 0.0, 0.0, 0.0], [0.36760634183883667, 0.42816370725631714, 0.20423001050949097, 0.0, 0.0, 0.0], [0.16471554338932037, 0.4136792719364166, 0.2509237229824066, 0.17068152129650116, 0.0, 0.0], [0.4184456169605255, 0.1524762362241745, 0.10305401682853699, 0.11071498692035675, 0.21530911326408386, 0.0], [0.19686934351921082, 0.2014620453119278, 0.12827259302139282, 0.09203246980905533, 0.09167550504207611, 0.2896881103515625]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9027364253997803, 0.09726352989673615, 0.0, 0.0, 0.0, 0.0], [0.9736634492874146, 0.014004302211105824, 0.01233230996876955, 0.0, 0.0, 0.0], [0.8504456281661987, 0.05690572410821915, 0.032060906291007996, 0.06058764085173607, 0.0, 0.0], [0.7661210298538208, 0.03530392050743103, 0.03433045372366905, 0.09675204753875732, 0.06749245524406433, 0.0], [0.8650374412536621, 0.020085260272026062, 0.01149806659668684, 0.01855834573507309, 0.018430285155773163, 0.06639053672552109]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9653082489967346, 0.03469168767333031, 0.0, 0.0, 0.0, 0.0], [0.9816323518753052, 0.014176066033542156, 0.004191514104604721, 0.0, 0.0, 0.0], [0.9275256395339966, 0.04737218841910362, 0.01152826938778162, 0.013573966920375824, 0.0, 0.0], [0.9293117523193359, 0.025833239778876305, 0.007227106485515833, 0.014300585724413395, 0.02332727052271366, 0.0], [0.8895062804222107, 0.04689619690179825, 0.0047171092592179775, 0.006286581978201866, 0.00609014043584466, 0.04650374501943588]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8938026428222656, 0.10619727522134781, 0.0, 0.0, 0.0, 0.0], [0.8221707940101624, 0.06304481625556946, 0.11478441953659058, 0.0, 0.0, 0.0], [0.5047380924224854, 0.15375731885433197, 0.2277037501335144, 0.11380083113908768, 0.0, 0.0], [0.4082071781158447, 0.09066355973482132, 0.11696872115135193, 0.24553199112415314, 0.13862857222557068, 0.0], [0.7291035652160645, 0.06638889014720917, 0.023112818598747253, 0.031103096902370453, 0.057143256068229675, 0.09314827620983124]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9247531890869141, 0.07524678111076355, 0.0, 0.0, 0.0, 0.0], [0.8957376480102539, 0.06989553570747375, 0.03436679765582085, 0.0, 0.0, 0.0], [0.7924937605857849, 0.0960114598274231, 0.05509118735790253, 0.056403566151857376, 0.0, 0.0], [0.7891505360603333, 0.07880303263664246, 0.03840155899524689, 0.05396979674696922, 0.03967496380209923, 0.0], [0.7807856798171997, 0.0799354612827301, 0.042531758546829224, 0.03234211727976799, 0.0178169384598732, 0.046588052064180374]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9480886459350586, 0.05191127583384514, 0.0, 0.0, 0.0, 0.0], [0.863694965839386, 0.04756204038858414, 0.08874296396970749, 0.0, 0.0, 0.0], [0.9341371059417725, 0.022224076092243195, 0.022624483332037926, 0.021014342084527016, 0.0, 0.0], [0.9588143229484558, 0.008020909503102303, 0.004490078426897526, 0.005862293299287558, 0.022812429815530777, 0.0], [0.9385918378829956, 0.021227721124887466, 0.0048724692314863205, 0.010940189473330975, 0.009524582885205746, 0.014843451790511608]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9763734340667725, 0.023626558482646942, 0.0, 0.0, 0.0, 0.0], [0.9884802103042603, 0.005189393647015095, 0.0063303736969828606, 0.0, 0.0, 0.0], [0.9477092027664185, 0.0179851483553648, 0.010156610049307346, 0.024149026721715927, 0.0, 0.0], [0.967192530632019, 0.006552813574671745, 0.0033227826934307814, 0.00556332478299737, 0.017368387430906296, 0.0], [0.9584562182426453, 0.007502961438149214, 0.0051363310776650906, 0.008071648888289928, 0.005997124593704939, 0.014835843816399574]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.884070873260498, 0.11592914164066315, 0.0, 0.0, 0.0, 0.0], [0.9931254386901855, 0.005070806015282869, 0.0018038019770756364, 0.0, 0.0, 0.0], [0.9534159302711487, 0.02382904477417469, 0.007748977281153202, 0.015006075613200665, 0.0, 0.0], [0.9151289463043213, 0.010873105376958847, 0.013190957717597485, 0.011050421744585037, 0.04975655674934387, 0.0], [0.8769673109054565, 0.03385210782289505, 0.00848648976534605, 0.009969149716198444, 0.03468578681349754, 0.036039214581251144]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003709519514814019, 0.999629020690918, 0.0, 0.0, 0.0, 0.0], [6.525027856696397e-05, 0.3737829029560089, 0.6261518597602844, 0.0, 0.0, 0.0], [4.606018774211407e-05, 0.210508793592453, 0.4115968942642212, 0.3778482675552368, 0.0, 0.0], [4.753069515572861e-05, 0.11616954207420349, 0.23264272511005402, 0.3985331058502197, 0.2526070475578308, 0.0], [1.247641534973809e-06, 0.14819711446762085, 0.15813173353672028, 0.30074331164360046, 0.11939018964767456, 0.27353641390800476]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971555769443512, 0.028444187715649605, 0.0, 0.0, 0.0, 0.0], [0.9529065489768982, 0.03233075141906738, 0.014762768521904945, 0.0, 0.0, 0.0], [0.9343128204345703, 0.02351292595267296, 0.02049802988767624, 0.021676240488886833, 0.0, 0.0], [0.9529678225517273, 0.00855141133069992, 0.004359325394034386, 0.008064556866884232, 0.026056913658976555, 0.0], [0.9653593897819519, 0.008487647399306297, 0.003499280195683241, 0.002721576252952218, 0.0032828773837536573, 0.016649367287755013]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8630780577659607, 0.13692188262939453, 0.0, 0.0, 0.0, 0.0], [0.7696157097816467, 0.0851333811879158, 0.14525099098682404, 0.0, 0.0, 0.0], [0.7133337259292603, 0.10170899331569672, 0.11931268870830536, 0.06564456224441528, 0.0, 0.0], [0.7186222076416016, 0.05444284901022911, 0.01386815495789051, 0.07808027416467667, 0.13498654961585999, 0.0], [0.7990148663520813, 0.05805593729019165, 0.009447019547224045, 0.017770467326045036, 0.02113853208720684, 0.09457314014434814]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9518988728523254, 0.048101115971803665, 0.0, 0.0, 0.0, 0.0], [0.8580653071403503, 0.02944577857851982, 0.11248888075351715, 0.0, 0.0, 0.0], [0.6577738523483276, 0.08513449877500534, 0.1261308640241623, 0.1309608370065689, 0.0, 0.0], [0.8087368607521057, 0.0323016420006752, 0.01841817982494831, 0.06856140494346619, 0.07198194414377213, 0.0], [0.6683295965194702, 0.13281384110450745, 0.021880635991692543, 0.02787741646170616, 0.04923408478498459, 0.0998644009232521]]]], \"left_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"], \"right_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-df8d67b4ace847b4a67aa800cdd9cb83\", \"include_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"total_heads\": 12} is a template marker that is replaced by actual params.\n",
       "        const config = {};\n",
       "\n",
       "        const MIN_X = 0;\n",
       "        const MIN_Y = 0;\n",
       "        const DIV_WIDTH = 970;\n",
       "        const THUMBNAIL_PADDING = 5;\n",
       "        const DETAIL_WIDTH = 300;\n",
       "        const DETAIL_ATTENTION_WIDTH = 140;\n",
       "        const DETAIL_BOX_WIDTH = 80;\n",
       "        const DETAIL_BOX_HEIGHT = 18;\n",
       "        const DETAIL_PADDING = 15;\n",
       "        const ATTN_PADDING = 0;\n",
       "        const DETAIL_HEADING_HEIGHT = 25;\n",
       "        const HEADING_TEXT_SIZE = 15;\n",
       "        const HEADING_PADDING = 5;\n",
       "        const TEXT_SIZE = 13;\n",
       "        const TEXT_PADDING = 5;\n",
       "        const LAYER_COLORS = d3.schemeCategory10;\n",
       "        const PALETTE = {\n",
       "            'light': {\n",
       "                'text': 'black',\n",
       "                'background': 'white',\n",
       "                'highlight': '#F5F5F5'\n",
       "            },\n",
       "            'dark': {\n",
       "                'text': '#ccc',\n",
       "                'background': 'black',\n",
       "                'highlight': '#222'\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function render() {\n",
       "\n",
       "            // Set global state variables\n",
       "\n",
       "            var attData = config.attention[config.filter];\n",
       "            config.leftText = attData.left_text;\n",
       "            config.rightText = attData.right_text;\n",
       "            config.attn = attData.attn;\n",
       "            config.numLayers = config.attn.length;\n",
       "            config.numHeads = config.attn[0].length;\n",
       "            config.thumbnailBoxHeight = 7 * (12 / config.totalHeads);\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            config.thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n",
       "            config.thumbnailWidth = (DIV_WIDTH - axisSize) / config.totalHeads;\n",
       "            config.detailHeight = Math.max(config.leftText.length, config.rightText.length) * DETAIL_BOX_HEIGHT + 2 * DETAIL_PADDING + DETAIL_HEADING_HEIGHT;\n",
       "            config.divHeight = Math.max(config.numLayers * config.thumbnailHeight + axisSize, config.detailHeight);\n",
       "\n",
       "            const vis = $(`#${config.rootDivId} #vis`)\n",
       "            vis.empty();\n",
       "            vis.attr(\"height\", config.divHeight);\n",
       "            config.svg = d3.select(`#${config.rootDivId} #vis`)\n",
       "                .append('svg')\n",
       "                .attr(\"width\", DIV_WIDTH)\n",
       "                .attr(\"height\", config.divHeight)\n",
       "                .attr(\"fill\", getBackgroundColor());\n",
       "\n",
       "            renderAxisLabels();\n",
       "\n",
       "            var i;\n",
       "            var j;\n",
       "            for (i = 0; i < config.numLayers; i++) {\n",
       "                for (j = 0; j < config.numHeads; j++) {\n",
       "                    renderThumbnail(i, j);\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function renderAxisLabels() {\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            const tableWidth = config.thumbnailWidth * config.heads.length;\n",
       "            config.svg.append(\"text\")\n",
       "                .text(\"Heads\")\n",
       "                .attr(\"fill\", \"black\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n",
       "                .attr(\"x\", axisSize + tableWidth / 2)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", 0)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "            for (let i = 0; i < config.numHeads; i++) {\n",
       "                config.svg.append(\"text\")\n",
       "                    .text(config.heads[i])\n",
       "                    .attr(\"fill\", \"black\")\n",
       "                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                    .attr(\"x\", axisSize + (i + .5) * config.thumbnailWidth)\n",
       "                    .attr(\"text-anchor\", \"middle\")\n",
       "                    .attr(\"y\", HEADING_TEXT_SIZE + HEADING_PADDING)\n",
       "                    .attr(\"dy\", TEXT_SIZE);\n",
       "            }\n",
       "            let x = 0;\n",
       "            let y = axisSize + config.thumbnailHeight * config.layers.length / 2;\n",
       "            console.log(\"x\", x, y)\n",
       "            config.svg.append(\"text\")\n",
       "                .text(\"Layers\")\n",
       "                .attr(\"fill\", \"black\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .attr(\"transform\", \"rotate(270, \" + x  + \", \" + y + \")\")\n",
       "                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "            for (let i = 0; i < config.numLayers; i++) {\n",
       "                x = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE; // HACK\n",
       "                y = axisSize + (i + .5) * config.thumbnailHeight;\n",
       "                config.svg.append(\"text\")\n",
       "                    .text(config.layers[i])\n",
       "                    .attr(\"fill\", \"black\")\n",
       "                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                    .attr(\"x\", x)\n",
       "                    .attr(\"text-anchor\", \"end\")\n",
       "                    .attr(\"y\", y)\n",
       "                    .attr(\"dy\", TEXT_SIZE / 2);\n",
       "            }\n",
       "        }\n",
       "\n",
       "\n",
       "        function renderThumbnail(layerIndex, headIndex) {\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING\n",
       "            const x = headIndex * config.thumbnailWidth + axisSize;\n",
       "            const y = layerIndex * config.thumbnailHeight + axisSize;\n",
       "            renderThumbnailAttn(x, y, config.attn[layerIndex][headIndex], layerIndex, headIndex);\n",
       "        }\n",
       "\n",
       "        function renderDetail(att, layerIndex, headIndex) {\n",
       "            const axisSize = TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            var xOffset = .8 * config.thumbnailWidth;\n",
       "            var maxX = DIV_WIDTH;\n",
       "            var maxY = config.divHeight - 3;\n",
       "            var leftPos = axisSize + headIndex * config.thumbnailWidth;\n",
       "            var x = leftPos + THUMBNAIL_PADDING + xOffset;\n",
       "            if (x < MIN_X) {\n",
       "                x = MIN_X;\n",
       "            } else if (x + DETAIL_WIDTH > maxX) {\n",
       "                x = leftPos + THUMBNAIL_PADDING - DETAIL_WIDTH + 8;\n",
       "            }\n",
       "            var posLeftText = x;\n",
       "            var posAttention = posLeftText + DETAIL_BOX_WIDTH;\n",
       "            var posRightText = posAttention + DETAIL_ATTENTION_WIDTH;\n",
       "            var thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n",
       "            var yOffset = 20;\n",
       "            var y = layerIndex * thumbnailHeight + THUMBNAIL_PADDING + yOffset;\n",
       "            if (y < MIN_Y) {\n",
       "                y = MIN_Y;\n",
       "            } else if (y + config.detailHeight > maxY) {\n",
       "                y = maxY - config.detailHeight;\n",
       "            }\n",
       "            renderDetailFrame(x, y, layerIndex);\n",
       "            y = y + DETAIL_PADDING;\n",
       "            renderDetailHeading(x, y, layerIndex, headIndex);\n",
       "            y = y + DETAIL_HEADING_HEIGHT;\n",
       "            renderDetailText(config.leftText, \"leftText\", posLeftText, y , layerIndex);\n",
       "            renderDetailAttn(posAttention, y, att, layerIndex, headIndex);\n",
       "            renderDetailText(config.rightText, \"rightText\", posRightText, y, layerIndex);\n",
       "        }\n",
       "\n",
       "        function renderDetailHeading(x, y, layerIndex, headIndex) {\n",
       "            var fillColor = getTextColor();\n",
       "            config.svg.append(\"text\")\n",
       "                .classed(\"detail\", true)\n",
       "                .text('Layer ' + config.layers[layerIndex] + \", Head \" + config.heads[headIndex])\n",
       "                .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .style(\"cursor\", \"default\")\n",
       "                .style(\"-webkit-user-select\", \"none\")\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .attr(\"x\", x + DETAIL_WIDTH / 2)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", DETAIL_HEADING_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_WIDTH)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "        }\n",
       "\n",
       "        function renderDetailText(text, id, x, y, layerIndex) {\n",
       "            var tokenContainer = config.svg.append(\"svg:g\")\n",
       "                .classed(\"detail\", true)\n",
       "                .selectAll(\"g\")\n",
       "                .data(text)\n",
       "                .enter()\n",
       "                .append(\"g\");\n",
       "\n",
       "            var fillColor = getTextColor();\n",
       "\n",
       "            tokenContainer.append(\"rect\")\n",
       "                .classed(\"highlight\", true)\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .style(\"opacity\", 0.0)\n",
       "                .attr(\"height\", DETAIL_BOX_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_BOX_WIDTH)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", function (d, i) {\n",
       "                    return y + i * DETAIL_BOX_HEIGHT;\n",
       "                });\n",
       "\n",
       "            var textContainer = tokenContainer.append(\"text\")\n",
       "                .classed(\"token\", true)\n",
       "                .text(function (d) {\n",
       "                    return d;\n",
       "                })\n",
       "                .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                .style(\"cursor\", \"default\")\n",
       "                .style(\"-webkit-user-select\", \"none\")\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", function (d, i) {\n",
       "                    return i * DETAIL_BOX_HEIGHT + y;\n",
       "                })\n",
       "                .attr(\"height\", DETAIL_BOX_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_BOX_WIDTH)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "\n",
       "            if (id == \"leftText\") {\n",
       "                textContainer.style(\"text-anchor\", \"end\")\n",
       "                    .attr(\"dx\", DETAIL_BOX_WIDTH - 2);\n",
       "                tokenContainer.on(\"mouseover\", function (d, index) {\n",
       "                    highlightSelection(index);\n",
       "                });\n",
       "                tokenContainer.on(\"mouseleave\", function () {\n",
       "                    unhighlightSelection();\n",
       "                });\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function highlightSelection(index) {\n",
       "            config.svg.select(\"#leftText\")\n",
       "                .selectAll(\".highlight\")\n",
       "                .style(\"opacity\", function (d, i) {\n",
       "                    return i == index ? 1.0 : 0.0;\n",
       "                });\n",
       "            config.svg.selectAll(\".attn-line-group\")\n",
       "                .style(\"opacity\", function (d, i) {\n",
       "                    return i == index ? 1.0 : 0.0;\n",
       "                });\n",
       "        }\n",
       "\n",
       "        function unhighlightSelection() {\n",
       "            config.svg.select(\"#leftText\")\n",
       "                .selectAll(\".highlight\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "            config.svg.selectAll(\".attn-line-group\")\n",
       "                .style(\"opacity\", 1);\n",
       "        }\n",
       "\n",
       "        function renderThumbnailAttn(x, y, att, layerIndex, headIndex) {\n",
       "\n",
       "            var attnContainer = config.svg.append(\"svg:g\");\n",
       "\n",
       "            var attnBackground = attnContainer.append(\"rect\")\n",
       "                .attr(\"id\", 'attn_background_' + layerIndex + \"_\" + headIndex)\n",
       "                .classed(\"attn_background\", true)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.thumbnailHeight)\n",
       "                .attr(\"width\", config.thumbnailWidth)\n",
       "                .attr(\"stroke-width\", 2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", 0)\n",
       "                .attr(\"fill\", getBackgroundColor());\n",
       "            var x1 = x + THUMBNAIL_PADDING;\n",
       "            var x2 = x1 + config.thumbnailWidth - 14;\n",
       "            var y1 = y + THUMBNAIL_PADDING;\n",
       "\n",
       "            attnContainer.selectAll(\"g\")\n",
       "                .data(att)\n",
       "                .enter()\n",
       "                .append(\"g\") // Add group for each source token\n",
       "                .attr(\"source-index\", function (d, i) { // Save index of source token\n",
       "                    return i;\n",
       "                })\n",
       "                .selectAll(\"line\")\n",
       "                .data(function (d) { // Loop over all target tokens\n",
       "                    return d;\n",
       "                })\n",
       "                .enter() // When entering\n",
       "                .append(\"line\")\n",
       "                .attr(\"x1\", x1)\n",
       "                .attr(\"y1\", function (d) {\n",
       "                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n",
       "                    return y1 + (sourceIndex + .5) * config.thumbnailBoxHeight;\n",
       "                })\n",
       "                .attr(\"x2\", x2)\n",
       "                .attr(\"y2\", function (d, targetIndex) {\n",
       "                    return y1 + (targetIndex + .5) * config.thumbnailBoxHeight;\n",
       "                })\n",
       "                .attr(\"stroke-width\", 2.2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", function (d) {\n",
       "                    return d;\n",
       "                });\n",
       "\n",
       "            var clickRegion = attnContainer.append(\"rect\")\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.thumbnailHeight)\n",
       "                .attr(\"width\", config.thumbnailWidth)\n",
       "                .style(\"opacity\", 0);\n",
       "\n",
       "            clickRegion.on(\"click\", function (d, index) {\n",
       "                var attnBackgroundOther = config.svg.selectAll(\".attn_background\");\n",
       "                attnBackgroundOther.attr(\"fill\", getBackgroundColor());\n",
       "                attnBackgroundOther.attr(\"stroke-opacity\", 0);\n",
       "\n",
       "                config.svg.selectAll(\".detail\").remove();\n",
       "                if (config.detail_layer != layerIndex || config.detail_head != headIndex) {\n",
       "                    renderDetail(att, layerIndex, headIndex);\n",
       "                    config.detail_layer = layerIndex;\n",
       "                    config.detail_head = headIndex;\n",
       "                    attnBackground.attr(\"fill\", getHighlightColor());\n",
       "                    attnBackground.attr(\"stroke-opacity\", .8);\n",
       "                } else {\n",
       "                    config.detail_layer = null;\n",
       "                    config.detail_head = null;\n",
       "                    attnBackground.attr(\"fill\", getBackgroundColor());\n",
       "                    attnBackground.attr(\"stroke-opacity\", 0);\n",
       "                }\n",
       "            });\n",
       "\n",
       "            clickRegion.on(\"mouseover\", function (d) {\n",
       "                d3.select(this).style(\"cursor\", \"pointer\");\n",
       "            });\n",
       "        }\n",
       "\n",
       "        function renderDetailFrame(x, y, layerIndex) {\n",
       "            var detailFrame = config.svg.append(\"rect\")\n",
       "                .classed(\"detail\", true)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.detailHeight)\n",
       "                .attr(\"width\", DETAIL_WIDTH)\n",
       "                .style(\"opacity\", 1)\n",
       "                .attr(\"stroke-width\", 1.5)\n",
       "                .attr(\"stroke-opacity\", 0.7)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex));\n",
       "        }\n",
       "\n",
       "        function renderDetailAttn(x, y, att, layerIndex) {\n",
       "            var attnContainer = config.svg.append(\"svg:g\")\n",
       "                .classed(\"detail\", true)\n",
       "                .attr(\"pointer-events\", \"none\");\n",
       "            attnContainer.selectAll(\"g\")\n",
       "                .data(att)\n",
       "                .enter()\n",
       "                .append(\"g\") // Add group for each source token\n",
       "                .classed('attn-line-group', true)\n",
       "                .attr(\"source-index\", function (d, i) { // Save index of source token\n",
       "                    return i;\n",
       "                })\n",
       "                .selectAll(\"line\")\n",
       "                .data(function (d) { // Loop over all target tokens\n",
       "                    return d;\n",
       "                })\n",
       "                .enter()\n",
       "                .append(\"line\")\n",
       "                .attr(\"x1\", x + ATTN_PADDING)\n",
       "                .attr(\"y1\", function (d) {\n",
       "                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n",
       "                    return y + (sourceIndex + .5) * DETAIL_BOX_HEIGHT;\n",
       "                })\n",
       "                .attr(\"x2\", x + DETAIL_ATTENTION_WIDTH - ATTN_PADDING)\n",
       "                .attr(\"y2\", function (d, targetIndex) {\n",
       "                    return y + (targetIndex + .5) * DETAIL_BOX_HEIGHT;\n",
       "                })\n",
       "                .attr(\"stroke-width\", 2.2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", function (d) {\n",
       "                    return d;\n",
       "                });\n",
       "        }\n",
       "\n",
       "        function getLayerColor(layer) {\n",
       "          return LAYER_COLORS[config.layers[layer] % 10];\n",
       "        }\n",
       "\n",
       "        function getTextColor() {\n",
       "            return PALETTE[config.mode]['text']\n",
       "        }\n",
       "\n",
       "        function getBackgroundColor() {\n",
       "           return PALETTE[config.mode]['background']\n",
       "        }\n",
       "\n",
       "        function getHighlightColor() {\n",
       "           return PALETTE[config.mode]['highlight']\n",
       "        }\n",
       "\n",
       "        function initialize() {\n",
       "            config.attention = params['attention'];\n",
       "            config.filter = params['default_filter'];\n",
       "            config.mode = params['display_mode'];\n",
       "            config.layers = params['include_layers']\n",
       "            config.heads = params['include_heads']\n",
       "            config.totalHeads = params['total_heads']\n",
       "            config.rootDivId = params['root_div_id'];\n",
       "            $(`#${config.rootDivId} #filter`).on('change', function (e) {\n",
       "                config.filter = e.currentTarget.value;\n",
       "                render();\n",
       "            });\n",
       "        }\n",
       "\n",
       "        initialize();\n",
       "        render();\n",
       "\n",
       "    });"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\n",
    "\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "model_name = 'openai-community/gpt2'\n",
    "input_text = \"No, I am your father\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9396011c-d0f6-4a4c-93d5-4fcf4049b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 65\n",
    "n_embd = 64\n",
    "\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "position_embedding_table = nn.Embedding(block_size, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd904f91-6746-4e31-b8e3-e77da1ba5bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7835e-01,  7.8658e-01, -2.9929e-01, -2.3873e-02, -5.0914e-01,\n",
      "         2.4441e-01, -8.2169e-02,  3.6466e-01, -1.8270e+00, -1.2369e+00,\n",
      "        -7.3462e-01,  4.6940e-01, -3.5877e-01,  2.5710e-01,  2.0993e+00,\n",
      "        -6.5342e-01,  1.2830e+00, -3.9340e-01, -2.4689e-01, -2.3424e-01,\n",
      "        -9.2546e-01,  4.4579e-01,  6.5937e-01,  3.4739e-01, -2.0431e+00,\n",
      "         5.1571e-01,  7.1762e-01, -7.0401e-01,  3.1290e-02, -8.9843e-01,\n",
      "        -1.2214e+00,  1.2941e-01, -1.3824e+00,  4.5792e-03, -1.8496e+00,\n",
      "         3.1221e-01, -5.8248e-02,  1.1976e+00,  3.8808e-01,  7.5234e-04,\n",
      "         5.7398e-02,  1.1069e+00, -1.1624e-01,  1.1645e-01,  3.2158e-01,\n",
      "         3.8281e-01,  6.7035e-01,  1.3830e-01, -6.9637e-01, -2.0735e-01,\n",
      "         1.3373e+00, -1.5333e+00, -2.8898e-01,  1.1616e+00, -1.0458e+00,\n",
      "        -1.3420e+00,  1.2078e+00, -8.5998e-02,  1.0968e+00,  6.9780e-01,\n",
      "         5.5507e-01,  4.4563e-01, -8.4776e-01, -1.5894e+00],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,3,15,4,7,1,4,9])\n",
    "x = token_embedding_table(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8918bee0-ba4d-4d89-b4dc-f2b27b25c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5348,  1.0764, -1.0312,  1.7759, -2.5882,  0.0404, -1.5108, -1.1320,\n",
      "        -3.0570, -1.8606, -0.7382, -1.2091, -1.0702,  1.1321,  0.9480, -0.2523,\n",
      "         2.6011, -2.4835, -0.1853, -0.5392, -0.6317,  1.8903,  0.4332, -0.5731,\n",
      "        -0.0286,  1.3354, -0.6846,  1.2807, -0.2313, -0.5876, -0.3106, -0.3120,\n",
      "        -1.4616, -0.4213, -2.6455, -0.1974, -1.1531,  2.6413,  1.4639, -0.4758,\n",
      "         1.0479,  1.2803,  0.5265,  0.6791,  0.4458, -1.3022,  0.1740, -0.2600,\n",
      "        -1.1940, -0.8797,  2.3601, -2.3947, -0.8889, -1.2826, -1.9416, -0.9030,\n",
      "         0.4287,  0.0597,  2.7288,  0.8362, -0.0783, -0.4978, -1.6223, -1.4962],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,3,15,4,7,1,4,9])\n",
    "x= position_embedding_table(x) + token_embedding_table(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc3ad03-ed60-40a0-aa51-9e39ac933b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9119)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "logits = torch.tensor([0.5, 0.1, 0.3])\n",
    "targets = torch.tensor([1.0, 0.0, 0.0])\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a6ee2d-df70-47c3-9b6a-401a51ca0a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4891)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c71853-a887-4b0f-95c1-536b5028dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c8e9487-6834-4e1a-ac2b-59ce0a3a5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5195dcf-861f-4310-b184-5c09bde789c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C) 16,32,16\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # Projection layer going back into the residual pathway\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37f1d7aa-eb44-4c59-8919-2c468deb5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))    # Communication\n",
    "        x = x + self.ffwd(self.ln2(x))  # Computation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e74df518-ee78-4c1e-bba8-025fe466b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple language model\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b77384dd-1753-4bee-ac4e-f79145c69181",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "448e58c1-5f84-4719-ab83-5220468734e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59de1bf-1604-46f0-9b52-c1dfe30408ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train Loss: 4.094468593597412, Val Loss: 4.092559814453125\n",
      "Step: 10, Train Loss: 3.3876137733459473, Val Loss: 3.415436029434204\n",
      "Step: 20, Train Loss: 3.2497808933258057, Val Loss: 3.2953882217407227\n",
      "Step: 30, Train Loss: 3.126052141189575, Val Loss: 3.1798043251037598\n",
      "Step: 40, Train Loss: 3.0196192264556885, Val Loss: 3.048623561859131\n",
      "Step: 50, Train Loss: 2.925534725189209, Val Loss: 2.941763401031494\n",
      "Step: 60, Train Loss: 2.8433682918548584, Val Loss: 2.8496041297912598\n",
      "Step: 70, Train Loss: 2.780139207839966, Val Loss: 2.7929749488830566\n",
      "Step: 80, Train Loss: 2.718958854675293, Val Loss: 2.7352917194366455\n",
      "Step: 90, Train Loss: 2.686169147491455, Val Loss: 2.677408456802368\n",
      "Step: 100, Train Loss: 2.6529603004455566, Val Loss: 2.6579787731170654\n",
      "Step: 110, Train Loss: 2.6223952770233154, Val Loss: 2.6180925369262695\n",
      "Step: 120, Train Loss: 2.605808734893799, Val Loss: 2.5961344242095947\n",
      "Step: 130, Train Loss: 2.5905394554138184, Val Loss: 2.5866754055023193\n",
      "Step: 140, Train Loss: 2.5743677616119385, Val Loss: 2.5702853202819824\n",
      "Step: 150, Train Loss: 2.5613369941711426, Val Loss: 2.5570764541625977\n",
      "Step: 160, Train Loss: 2.5468649864196777, Val Loss: 2.5520453453063965\n",
      "Step: 170, Train Loss: 2.5438058376312256, Val Loss: 2.545786142349243\n",
      "Step: 180, Train Loss: 2.5228383541107178, Val Loss: 2.5330660343170166\n",
      "Step: 190, Train Loss: 2.5241575241088867, Val Loss: 2.523898124694824\n",
      "Step: 200, Train Loss: 2.514425754547119, Val Loss: 2.519381523132324\n",
      "Step: 210, Train Loss: 2.506253719329834, Val Loss: 2.507753610610962\n",
      "Step: 220, Train Loss: 2.490630865097046, Val Loss: 2.4995343685150146\n",
      "Step: 230, Train Loss: 2.4860360622406006, Val Loss: 2.4773945808410645\n",
      "Step: 240, Train Loss: 2.4617629051208496, Val Loss: 2.4696295261383057\n",
      "Step: 250, Train Loss: 2.4528563022613525, Val Loss: 2.4671435356140137\n",
      "Step: 260, Train Loss: 2.450792074203491, Val Loss: 2.4442484378814697\n",
      "Step: 270, Train Loss: 2.4555752277374268, Val Loss: 2.447002649307251\n",
      "Step: 280, Train Loss: 2.434709310531616, Val Loss: 2.4404942989349365\n",
      "Step: 290, Train Loss: 2.430053472518921, Val Loss: 2.436086416244507\n",
      "Step: 300, Train Loss: 2.416165828704834, Val Loss: 2.4158761501312256\n",
      "Step: 310, Train Loss: 2.407716989517212, Val Loss: 2.418617010116577\n",
      "Step: 320, Train Loss: 2.3953495025634766, Val Loss: 2.4074666500091553\n",
      "Step: 330, Train Loss: 2.397430658340454, Val Loss: 2.399718999862671\n",
      "Step: 340, Train Loss: 2.39444637298584, Val Loss: 2.395836353302002\n",
      "Step: 350, Train Loss: 2.3824422359466553, Val Loss: 2.3918042182922363\n",
      "Step: 360, Train Loss: 2.3709425926208496, Val Loss: 2.376511335372925\n",
      "Step: 370, Train Loss: 2.3757550716400146, Val Loss: 2.3754751682281494\n",
      "Step: 380, Train Loss: 2.3666269779205322, Val Loss: 2.3819918632507324\n",
      "Step: 390, Train Loss: 2.3569774627685547, Val Loss: 2.3653311729431152\n",
      "Step: 400, Train Loss: 2.348149061203003, Val Loss: 2.3610949516296387\n",
      "Step: 410, Train Loss: 2.3572845458984375, Val Loss: 2.358908176422119\n",
      "Step: 420, Train Loss: 2.339003801345825, Val Loss: 2.3398571014404297\n",
      "Step: 430, Train Loss: 2.3217613697052, Val Loss: 2.3392653465270996\n",
      "Step: 440, Train Loss: 2.3265562057495117, Val Loss: 2.333448886871338\n",
      "Step: 450, Train Loss: 2.317169427871704, Val Loss: 2.3326311111450195\n",
      "Step: 460, Train Loss: 2.315119743347168, Val Loss: 2.3246304988861084\n",
      "Step: 470, Train Loss: 2.3102564811706543, Val Loss: 2.322331190109253\n",
      "Step: 480, Train Loss: 2.3027892112731934, Val Loss: 2.312415361404419\n",
      "Step: 490, Train Loss: 2.3068599700927734, Val Loss: 2.3161942958831787\n",
      "Step: 500, Train Loss: 2.3022539615631104, Val Loss: 2.312441825866699\n",
      "Step: 510, Train Loss: 2.2933874130249023, Val Loss: 2.308912754058838\n",
      "Step: 520, Train Loss: 2.291618585586548, Val Loss: 2.307152032852173\n",
      "Step: 530, Train Loss: 2.2720937728881836, Val Loss: 2.2996702194213867\n",
      "Step: 540, Train Loss: 2.2701375484466553, Val Loss: 2.2851011753082275\n",
      "Step: 550, Train Loss: 2.281980276107788, Val Loss: 2.2895445823669434\n",
      "Step: 560, Train Loss: 2.2588582038879395, Val Loss: 2.266225814819336\n",
      "Step: 570, Train Loss: 2.2650437355041504, Val Loss: 2.286808490753174\n",
      "Step: 580, Train Loss: 2.251682996749878, Val Loss: 2.2729287147521973\n",
      "Step: 590, Train Loss: 2.2430264949798584, Val Loss: 2.2661101818084717\n",
      "Step: 600, Train Loss: 2.2382545471191406, Val Loss: 2.2622475624084473\n",
      "Step: 610, Train Loss: 2.229424238204956, Val Loss: 2.24495005607605\n",
      "Step: 620, Train Loss: 2.229395866394043, Val Loss: 2.2444024085998535\n",
      "Step: 630, Train Loss: 2.219972848892212, Val Loss: 2.246438503265381\n",
      "Step: 640, Train Loss: 2.223825693130493, Val Loss: 2.239298105239868\n",
      "Step: 650, Train Loss: 2.2125062942504883, Val Loss: 2.235661029815674\n",
      "Step: 660, Train Loss: 2.215538263320923, Val Loss: 2.2306907176971436\n",
      "Step: 670, Train Loss: 2.218740940093994, Val Loss: 2.242227792739868\n",
      "Step: 680, Train Loss: 2.2021689414978027, Val Loss: 2.227116584777832\n",
      "Step: 690, Train Loss: 2.192296028137207, Val Loss: 2.2221245765686035\n",
      "Step: 700, Train Loss: 2.191791296005249, Val Loss: 2.2210142612457275\n",
      "Step: 710, Train Loss: 2.1880860328674316, Val Loss: 2.224133014678955\n",
      "Step: 720, Train Loss: 2.1744494438171387, Val Loss: 2.2130091190338135\n",
      "Step: 730, Train Loss: 2.183352470397949, Val Loss: 2.212221622467041\n",
      "Step: 740, Train Loss: 2.181011915206909, Val Loss: 2.208679437637329\n",
      "Step: 750, Train Loss: 2.1764445304870605, Val Loss: 2.201622486114502\n",
      "Step: 760, Train Loss: 2.157935380935669, Val Loss: 2.1893444061279297\n",
      "Step: 770, Train Loss: 2.1596508026123047, Val Loss: 2.1825852394104004\n",
      "Step: 780, Train Loss: 2.162848949432373, Val Loss: 2.2018814086914062\n",
      "Step: 790, Train Loss: 2.1685454845428467, Val Loss: 2.196997880935669\n",
      "Step: 800, Train Loss: 2.1616551876068115, Val Loss: 2.192063570022583\n",
      "Step: 810, Train Loss: 2.14461612701416, Val Loss: 2.1806719303131104\n",
      "Step: 820, Train Loss: 2.1515843868255615, Val Loss: 2.180978536605835\n",
      "Step: 830, Train Loss: 2.138115406036377, Val Loss: 2.1765451431274414\n",
      "Step: 840, Train Loss: 2.147902250289917, Val Loss: 2.189931631088257\n",
      "Step: 850, Train Loss: 2.1329970359802246, Val Loss: 2.166741132736206\n",
      "Step: 860, Train Loss: 2.132533311843872, Val Loss: 2.1650283336639404\n",
      "Step: 870, Train Loss: 2.129678726196289, Val Loss: 2.1583259105682373\n",
      "Step: 880, Train Loss: 2.122426748275757, Val Loss: 2.160945177078247\n",
      "Step: 890, Train Loss: 2.128838539123535, Val Loss: 2.164945125579834\n",
      "Step: 900, Train Loss: 2.116809368133545, Val Loss: 2.1430399417877197\n",
      "Step: 910, Train Loss: 2.112089157104492, Val Loss: 2.1540579795837402\n",
      "Step: 920, Train Loss: 2.118863582611084, Val Loss: 2.1514928340911865\n",
      "Step: 930, Train Loss: 2.106478214263916, Val Loss: 2.151343584060669\n",
      "Step: 940, Train Loss: 2.108488082885742, Val Loss: 2.1399502754211426\n",
      "Step: 950, Train Loss: 2.0984902381896973, Val Loss: 2.138090133666992\n",
      "Step: 960, Train Loss: 2.102503776550293, Val Loss: 2.1387970447540283\n",
      "Step: 970, Train Loss: 2.1003479957580566, Val Loss: 2.1424171924591064\n",
      "Step: 980, Train Loss: 2.1006083488464355, Val Loss: 2.1257107257843018\n",
      "Step: 990, Train Loss: 2.0932302474975586, Val Loss: 2.1345484256744385\n",
      "Step: 1000, Train Loss: 2.0936057567596436, Val Loss: 2.135162115097046\n",
      "Step: 1010, Train Loss: 2.0776751041412354, Val Loss: 2.118753671646118\n",
      "Step: 1020, Train Loss: 2.0759286880493164, Val Loss: 2.118304491043091\n",
      "Step: 1030, Train Loss: 2.079937219619751, Val Loss: 2.1182820796966553\n",
      "Step: 1040, Train Loss: 2.0716679096221924, Val Loss: 2.1053953170776367\n",
      "Step: 1050, Train Loss: 2.0794503688812256, Val Loss: 2.1272494792938232\n",
      "Step: 1060, Train Loss: 2.0687460899353027, Val Loss: 2.1120548248291016\n",
      "Step: 1070, Train Loss: 2.0719478130340576, Val Loss: 2.1130049228668213\n",
      "Step: 1080, Train Loss: 2.066483497619629, Val Loss: 2.105349063873291\n",
      "Step: 1090, Train Loss: 2.0661559104919434, Val Loss: 2.1071457862854004\n",
      "Step: 1100, Train Loss: 2.055203437805176, Val Loss: 2.1001179218292236\n",
      "Step: 1110, Train Loss: 2.054746627807617, Val Loss: 2.1128156185150146\n",
      "Step: 1120, Train Loss: 2.0453450679779053, Val Loss: 2.093851089477539\n",
      "Step: 1130, Train Loss: 2.0518577098846436, Val Loss: 2.1007449626922607\n",
      "Step: 1140, Train Loss: 2.042654514312744, Val Loss: 2.0985376834869385\n",
      "Step: 1150, Train Loss: 2.030745506286621, Val Loss: 2.0958147048950195\n",
      "Step: 1160, Train Loss: 2.0250940322875977, Val Loss: 2.081848382949829\n",
      "Step: 1170, Train Loss: 2.028991460800171, Val Loss: 2.0866870880126953\n",
      "Step: 1180, Train Loss: 2.0380659103393555, Val Loss: 2.0994415283203125\n",
      "Step: 1190, Train Loss: 2.0475528240203857, Val Loss: 2.119743824005127\n",
      "Step: 1200, Train Loss: 2.031477451324463, Val Loss: 2.083247423171997\n",
      "Step: 1210, Train Loss: 2.0256991386413574, Val Loss: 2.0956716537475586\n",
      "Step: 1220, Train Loss: 2.0161516666412354, Val Loss: 2.0849874019622803\n",
      "Step: 1230, Train Loss: 2.0103213787078857, Val Loss: 2.0721535682678223\n",
      "Step: 1240, Train Loss: 2.0125725269317627, Val Loss: 2.0655674934387207\n",
      "Step: 1250, Train Loss: 2.019416570663452, Val Loss: 2.0749802589416504\n",
      "Step: 1260, Train Loss: 2.002061367034912, Val Loss: 2.0678844451904297\n",
      "Step: 1270, Train Loss: 2.0049517154693604, Val Loss: 2.0834290981292725\n",
      "Step: 1280, Train Loss: 2.0036306381225586, Val Loss: 2.0544192790985107\n",
      "Step: 1290, Train Loss: 1.9959982633590698, Val Loss: 2.049682378768921\n",
      "Step: 1300, Train Loss: 1.9990023374557495, Val Loss: 2.0600852966308594\n",
      "Step: 1310, Train Loss: 2.0086183547973633, Val Loss: 2.0606822967529297\n",
      "Step: 1320, Train Loss: 2.0002787113189697, Val Loss: 2.06270170211792\n",
      "Step: 1330, Train Loss: 1.994875192642212, Val Loss: 2.0550544261932373\n",
      "Step: 1340, Train Loss: 1.9917411804199219, Val Loss: 2.070781946182251\n",
      "Step: 1350, Train Loss: 1.9861372709274292, Val Loss: 2.054239273071289\n",
      "Step: 1360, Train Loss: 1.988669753074646, Val Loss: 2.047797679901123\n",
      "Step: 1370, Train Loss: 1.9830756187438965, Val Loss: 2.051678419113159\n",
      "Step: 1380, Train Loss: 1.9898053407669067, Val Loss: 2.0523009300231934\n",
      "Step: 1390, Train Loss: 1.9721543788909912, Val Loss: 2.0408122539520264\n",
      "Step: 1400, Train Loss: 1.974000096321106, Val Loss: 2.0471010208129883\n",
      "Step: 1410, Train Loss: 1.9820979833602905, Val Loss: 2.0475265979766846\n",
      "Step: 1420, Train Loss: 1.9806300401687622, Val Loss: 2.0372307300567627\n",
      "Step: 1430, Train Loss: 1.9807418584823608, Val Loss: 2.050382137298584\n",
      "Step: 1440, Train Loss: 1.9682176113128662, Val Loss: 2.033107042312622\n",
      "Step: 1450, Train Loss: 1.9665920734405518, Val Loss: 2.0387604236602783\n",
      "Step: 1460, Train Loss: 1.9649947881698608, Val Loss: 2.0345067977905273\n",
      "Step: 1470, Train Loss: 1.960599422454834, Val Loss: 2.035876989364624\n",
      "Step: 1480, Train Loss: 1.9568287134170532, Val Loss: 2.0385024547576904\n",
      "Step: 1490, Train Loss: 1.9577516317367554, Val Loss: 2.033949851989746\n",
      "Step: 1500, Train Loss: 1.9474142789840698, Val Loss: 2.030102491378784\n",
      "Step: 1510, Train Loss: 1.9579458236694336, Val Loss: 2.031512975692749\n",
      "Step: 1520, Train Loss: 1.9362339973449707, Val Loss: 2.020768642425537\n",
      "Step: 1530, Train Loss: 1.9387884140014648, Val Loss: 2.0182459354400635\n",
      "Step: 1540, Train Loss: 1.949691653251648, Val Loss: 2.0155463218688965\n",
      "Step: 1550, Train Loss: 1.9536712169647217, Val Loss: 2.0191996097564697\n",
      "Step: 1560, Train Loss: 1.9427061080932617, Val Loss: 2.0298969745635986\n",
      "Step: 1570, Train Loss: 1.9354864358901978, Val Loss: 2.015979528427124\n",
      "Step: 1580, Train Loss: 1.9518499374389648, Val Loss: 2.0337331295013428\n",
      "Step: 1590, Train Loss: 1.9548940658569336, Val Loss: 2.0254065990448\n",
      "Step: 1600, Train Loss: 1.9489325284957886, Val Loss: 2.032033681869507\n",
      "Step: 1610, Train Loss: 1.932245135307312, Val Loss: 2.0259881019592285\n",
      "Step: 1620, Train Loss: 1.9295066595077515, Val Loss: 2.012575626373291\n",
      "Step: 1630, Train Loss: 1.93461012840271, Val Loss: 2.0076000690460205\n",
      "Step: 1640, Train Loss: 1.92128586769104, Val Loss: 1.9946494102478027\n",
      "Step: 1650, Train Loss: 1.9218121767044067, Val Loss: 2.003516912460327\n",
      "Step: 1660, Train Loss: 1.9256300926208496, Val Loss: 2.0017507076263428\n",
      "Step: 1670, Train Loss: 1.9113507270812988, Val Loss: 2.013573169708252\n",
      "Step: 1680, Train Loss: 1.9090278148651123, Val Loss: 2.0040454864501953\n",
      "Step: 1690, Train Loss: 1.9049627780914307, Val Loss: 1.9867331981658936\n",
      "Step: 1700, Train Loss: 1.9094330072402954, Val Loss: 2.0049853324890137\n",
      "Step: 1710, Train Loss: 1.9071544408798218, Val Loss: 1.9829708337783813\n",
      "Step: 1720, Train Loss: 1.9097307920455933, Val Loss: 1.9943500757217407\n",
      "Step: 1730, Train Loss: 1.915687084197998, Val Loss: 1.9861998558044434\n",
      "Step: 1740, Train Loss: 1.906650185585022, Val Loss: 1.9953429698944092\n",
      "Step: 1750, Train Loss: 1.9229966402053833, Val Loss: 2.0047693252563477\n",
      "Step: 1760, Train Loss: 1.9027737379074097, Val Loss: 1.9869836568832397\n",
      "Step: 1770, Train Loss: 1.9007779359817505, Val Loss: 1.9903827905654907\n",
      "Step: 1780, Train Loss: 1.90016770362854, Val Loss: 1.9984737634658813\n",
      "Step: 1790, Train Loss: 1.890204668045044, Val Loss: 1.9917492866516113\n",
      "Step: 1800, Train Loss: 1.901247262954712, Val Loss: 1.9815475940704346\n",
      "Step: 1810, Train Loss: 1.89107084274292, Val Loss: 1.984647512435913\n",
      "Step: 1820, Train Loss: 1.8946146965026855, Val Loss: 1.986116647720337\n",
      "Step: 1830, Train Loss: 1.9036036729812622, Val Loss: 1.9928866624832153\n",
      "Step: 1840, Train Loss: 1.896634817123413, Val Loss: 1.9820473194122314\n",
      "Step: 1850, Train Loss: 1.8800768852233887, Val Loss: 1.9702441692352295\n",
      "Step: 1860, Train Loss: 1.889748215675354, Val Loss: 1.965744972229004\n",
      "Step: 1870, Train Loss: 1.8910255432128906, Val Loss: 1.986611008644104\n",
      "Step: 1880, Train Loss: 1.8844048976898193, Val Loss: 1.9780868291854858\n",
      "Step: 1890, Train Loss: 1.893884539604187, Val Loss: 1.9887192249298096\n",
      "Step: 1900, Train Loss: 1.9027208089828491, Val Loss: 1.988244891166687\n",
      "Step: 1910, Train Loss: 1.8845562934875488, Val Loss: 1.980777621269226\n",
      "Step: 1920, Train Loss: 1.8702832460403442, Val Loss: 1.9790681600570679\n",
      "Step: 1930, Train Loss: 1.8684356212615967, Val Loss: 1.9701659679412842\n",
      "Step: 1940, Train Loss: 1.8726593255996704, Val Loss: 1.9707119464874268\n",
      "Step: 1950, Train Loss: 1.879138469696045, Val Loss: 1.9829554557800293\n",
      "Step: 1960, Train Loss: 1.8907448053359985, Val Loss: 1.9981623888015747\n",
      "Step: 1970, Train Loss: 1.8648773431777954, Val Loss: 1.9612818956375122\n",
      "Step: 1980, Train Loss: 1.8620017766952515, Val Loss: 1.9634857177734375\n",
      "Step: 1990, Train Loss: 1.8779973983764648, Val Loss: 1.9897154569625854\n",
      "Step: 2000, Train Loss: 1.8625520467758179, Val Loss: 1.9797604084014893\n",
      "Step: 2010, Train Loss: 1.8572382926940918, Val Loss: 1.9637221097946167\n",
      "Step: 2020, Train Loss: 1.8659831285476685, Val Loss: 1.9834163188934326\n",
      "Step: 2030, Train Loss: 1.8591759204864502, Val Loss: 1.9568127393722534\n",
      "Step: 2040, Train Loss: 1.8731491565704346, Val Loss: 1.9588229656219482\n",
      "Step: 2050, Train Loss: 1.8581241369247437, Val Loss: 1.960279941558838\n",
      "Step: 2060, Train Loss: 1.8517307043075562, Val Loss: 1.9680438041687012\n",
      "Step: 2070, Train Loss: 1.8551346063613892, Val Loss: 1.9816774129867554\n",
      "Step: 2080, Train Loss: 1.849881887435913, Val Loss: 1.9615501165390015\n",
      "Step: 2090, Train Loss: 1.857020616531372, Val Loss: 1.9639201164245605\n",
      "Step: 2100, Train Loss: 1.853523850440979, Val Loss: 1.9531810283660889\n",
      "Step: 2110, Train Loss: 1.8527891635894775, Val Loss: 1.9637643098831177\n",
      "Step: 2120, Train Loss: 1.8546600341796875, Val Loss: 1.9542747735977173\n",
      "Step: 2130, Train Loss: 1.8423569202423096, Val Loss: 1.9524528980255127\n",
      "Step: 2140, Train Loss: 1.8335047960281372, Val Loss: 1.95753812789917\n",
      "Step: 2150, Train Loss: 1.8423583507537842, Val Loss: 1.9575799703598022\n",
      "Step: 2160, Train Loss: 1.8523105382919312, Val Loss: 1.9593260288238525\n",
      "Step: 2170, Train Loss: 1.844183087348938, Val Loss: 1.977657675743103\n",
      "Step: 2180, Train Loss: 1.8387184143066406, Val Loss: 1.980880856513977\n",
      "Step: 2190, Train Loss: 1.830668330192566, Val Loss: 1.9574655294418335\n",
      "Step: 2200, Train Loss: 1.8356776237487793, Val Loss: 1.959531307220459\n",
      "Step: 2210, Train Loss: 1.8498355150222778, Val Loss: 1.9580579996109009\n",
      "Step: 2220, Train Loss: 1.844753623008728, Val Loss: 1.9605909585952759\n",
      "Step: 2230, Train Loss: 1.8445794582366943, Val Loss: 1.9325463771820068\n",
      "Step: 2240, Train Loss: 1.8257038593292236, Val Loss: 1.9446616172790527\n",
      "Step: 2250, Train Loss: 1.8332664966583252, Val Loss: 1.9296612739562988\n",
      "Step: 2260, Train Loss: 1.834100365638733, Val Loss: 1.9398106336593628\n",
      "Step: 2270, Train Loss: 1.8331174850463867, Val Loss: 1.9408804178237915\n",
      "Step: 2280, Train Loss: 1.840631365776062, Val Loss: 1.9507038593292236\n",
      "Step: 2290, Train Loss: 1.8277992010116577, Val Loss: 1.944685697555542\n",
      "Step: 2300, Train Loss: 1.8281581401824951, Val Loss: 1.939703106880188\n",
      "Step: 2310, Train Loss: 1.8358986377716064, Val Loss: 1.9558937549591064\n",
      "Step: 2320, Train Loss: 1.8183752298355103, Val Loss: 1.936768651008606\n",
      "Step: 2330, Train Loss: 1.8206161260604858, Val Loss: 1.9323315620422363\n",
      "Step: 2340, Train Loss: 1.8116511106491089, Val Loss: 1.9375473260879517\n",
      "Step: 2350, Train Loss: 1.8201004266738892, Val Loss: 1.9382766485214233\n",
      "Step: 2360, Train Loss: 1.8238128423690796, Val Loss: 1.9418039321899414\n",
      "Step: 2370, Train Loss: 1.813459873199463, Val Loss: 1.944426417350769\n",
      "Step: 2380, Train Loss: 1.8169304132461548, Val Loss: 1.9398936033248901\n",
      "Step: 2390, Train Loss: 1.8125696182250977, Val Loss: 1.9313544034957886\n",
      "Step: 2400, Train Loss: 1.8168072700500488, Val Loss: 1.930597186088562\n",
      "Step: 2410, Train Loss: 1.8316067457199097, Val Loss: 1.9400291442871094\n",
      "Step: 2420, Train Loss: 1.8118658065795898, Val Loss: 1.9196285009384155\n",
      "Step: 2430, Train Loss: 1.8082462549209595, Val Loss: 1.9208232164382935\n",
      "Step: 2440, Train Loss: 1.8202612400054932, Val Loss: 1.9247863292694092\n",
      "Step: 2450, Train Loss: 1.806082010269165, Val Loss: 1.923500418663025\n",
      "Step: 2460, Train Loss: 1.8124961853027344, Val Loss: 1.9215152263641357\n",
      "Step: 2470, Train Loss: 1.8072292804718018, Val Loss: 1.9166061878204346\n",
      "Step: 2480, Train Loss: 1.8149328231811523, Val Loss: 1.928501009941101\n",
      "Step: 2490, Train Loss: 1.7965002059936523, Val Loss: 1.9181307554244995\n",
      "Step: 2500, Train Loss: 1.8087074756622314, Val Loss: 1.9260553121566772\n",
      "Step: 2510, Train Loss: 1.8051769733428955, Val Loss: 1.925615668296814\n",
      "Step: 2520, Train Loss: 1.8155931234359741, Val Loss: 1.915442943572998\n",
      "Step: 2530, Train Loss: 1.7892653942108154, Val Loss: 1.9095836877822876\n",
      "Step: 2540, Train Loss: 1.79298734664917, Val Loss: 1.9060778617858887\n",
      "Step: 2550, Train Loss: 1.7962650060653687, Val Loss: 1.9162673950195312\n",
      "Step: 2560, Train Loss: 1.781947374343872, Val Loss: 1.9147273302078247\n",
      "Step: 2570, Train Loss: 1.7958565950393677, Val Loss: 1.9291632175445557\n",
      "Step: 2580, Train Loss: 1.7917163372039795, Val Loss: 1.922237515449524\n",
      "Step: 2590, Train Loss: 1.7964835166931152, Val Loss: 1.9174655675888062\n",
      "Step: 2600, Train Loss: 1.8109664916992188, Val Loss: 1.9263136386871338\n",
      "Step: 2610, Train Loss: 1.8005948066711426, Val Loss: 1.9211456775665283\n",
      "Step: 2620, Train Loss: 1.7937958240509033, Val Loss: 1.9144392013549805\n",
      "Step: 2630, Train Loss: 1.7954051494598389, Val Loss: 1.9164913892745972\n",
      "Step: 2640, Train Loss: 1.8013492822647095, Val Loss: 1.9250152111053467\n",
      "Step: 2650, Train Loss: 1.7922885417938232, Val Loss: 1.9153767824172974\n",
      "Step: 2660, Train Loss: 1.7850825786590576, Val Loss: 1.91825270652771\n",
      "Step: 2670, Train Loss: 1.781456708908081, Val Loss: 1.9148504734039307\n",
      "Step: 2680, Train Loss: 1.789014458656311, Val Loss: 1.9182562828063965\n",
      "Step: 2690, Train Loss: 1.7831718921661377, Val Loss: 1.9180328845977783\n",
      "Step: 2700, Train Loss: 1.7824145555496216, Val Loss: 1.9093999862670898\n",
      "Step: 2710, Train Loss: 1.785314917564392, Val Loss: 1.9138178825378418\n",
      "Step: 2720, Train Loss: 1.796616554260254, Val Loss: 1.9152880907058716\n",
      "Step: 2730, Train Loss: 1.787327527999878, Val Loss: 1.920896291732788\n",
      "Step: 2740, Train Loss: 1.7812716960906982, Val Loss: 1.899723768234253\n",
      "Step: 2750, Train Loss: 1.7839237451553345, Val Loss: 1.9197238683700562\n",
      "Step: 2760, Train Loss: 1.7862446308135986, Val Loss: 1.9142955541610718\n",
      "Step: 2770, Train Loss: 1.773995041847229, Val Loss: 1.908660888671875\n",
      "Step: 2780, Train Loss: 1.7683970928192139, Val Loss: 1.9034684896469116\n",
      "Step: 2790, Train Loss: 1.774930477142334, Val Loss: 1.8948930501937866\n",
      "Step: 2800, Train Loss: 1.7748452425003052, Val Loss: 1.9111098051071167\n",
      "Step: 2810, Train Loss: 1.7840819358825684, Val Loss: 1.9101582765579224\n",
      "Step: 2820, Train Loss: 1.7714749574661255, Val Loss: 1.9032913446426392\n",
      "Step: 2830, Train Loss: 1.759048342704773, Val Loss: 1.8976460695266724\n",
      "Step: 2840, Train Loss: 1.7687948942184448, Val Loss: 1.905099630355835\n",
      "Step: 2850, Train Loss: 1.7805333137512207, Val Loss: 1.9132708311080933\n",
      "Step: 2860, Train Loss: 1.7660582065582275, Val Loss: 1.9082826375961304\n",
      "Step: 2870, Train Loss: 1.7567152976989746, Val Loss: 1.9072351455688477\n",
      "Step: 2880, Train Loss: 1.7659504413604736, Val Loss: 1.9224603176116943\n",
      "Step: 2890, Train Loss: 1.7664101123809814, Val Loss: 1.9065279960632324\n",
      "Step: 2900, Train Loss: 1.763034462928772, Val Loss: 1.9143835306167603\n",
      "Step: 2910, Train Loss: 1.7742104530334473, Val Loss: 1.8903690576553345\n",
      "Step: 2920, Train Loss: 1.7599365711212158, Val Loss: 1.9077377319335938\n",
      "Step: 2930, Train Loss: 1.7585681676864624, Val Loss: 1.902395486831665\n",
      "Step: 2940, Train Loss: 1.7740784883499146, Val Loss: 1.9079536199569702\n",
      "Step: 2950, Train Loss: 1.7589343786239624, Val Loss: 1.904755711555481\n",
      "Step: 2960, Train Loss: 1.7610154151916504, Val Loss: 1.9001436233520508\n",
      "Step: 2970, Train Loss: 1.7695828676223755, Val Loss: 1.9039735794067383\n",
      "Step: 2980, Train Loss: 1.7529380321502686, Val Loss: 1.8946174383163452\n",
      "Step: 2990, Train Loss: 1.7616609334945679, Val Loss: 1.890377402305603\n",
      "Step: 3000, Train Loss: 1.7619612216949463, Val Loss: 1.905879259109497\n",
      "Step: 3010, Train Loss: 1.7522799968719482, Val Loss: 1.8960692882537842\n",
      "Step: 3020, Train Loss: 1.7657361030578613, Val Loss: 1.9105380773544312\n",
      "Step: 3030, Train Loss: 1.7588560581207275, Val Loss: 1.9095263481140137\n",
      "Step: 3040, Train Loss: 1.7547943592071533, Val Loss: 1.8981364965438843\n",
      "Step: 3050, Train Loss: 1.7523857355117798, Val Loss: 1.8929780721664429\n",
      "Step: 3060, Train Loss: 1.7526642084121704, Val Loss: 1.8949884176254272\n",
      "Step: 3070, Train Loss: 1.751542329788208, Val Loss: 1.876061201095581\n",
      "Step: 3080, Train Loss: 1.758034110069275, Val Loss: 1.8981537818908691\n",
      "Step: 3090, Train Loss: 1.7459030151367188, Val Loss: 1.9089570045471191\n",
      "Step: 3100, Train Loss: 1.7610821723937988, Val Loss: 1.9101240634918213\n",
      "Step: 3110, Train Loss: 1.7394769191741943, Val Loss: 1.9006423950195312\n",
      "Step: 3120, Train Loss: 1.7535439729690552, Val Loss: 1.898962140083313\n",
      "Step: 3130, Train Loss: 1.7402794361114502, Val Loss: 1.8957610130310059\n",
      "Step: 3140, Train Loss: 1.7613998651504517, Val Loss: 1.888492465019226\n",
      "Step: 3150, Train Loss: 1.7371140718460083, Val Loss: 1.8712294101715088\n",
      "Step: 3160, Train Loss: 1.7499511241912842, Val Loss: 1.903481125831604\n",
      "Step: 3170, Train Loss: 1.7437047958374023, Val Loss: 1.9084115028381348\n",
      "Step: 3180, Train Loss: 1.7407819032669067, Val Loss: 1.9038941860198975\n",
      "Step: 3190, Train Loss: 1.744896411895752, Val Loss: 1.905218482017517\n",
      "Step: 3200, Train Loss: 1.7452300786972046, Val Loss: 1.892081618309021\n",
      "Step: 3210, Train Loss: 1.7386574745178223, Val Loss: 1.891205906867981\n",
      "Step: 3220, Train Loss: 1.7503657341003418, Val Loss: 1.8889508247375488\n",
      "Step: 3230, Train Loss: 1.7365490198135376, Val Loss: 1.8702219724655151\n",
      "Step: 3240, Train Loss: 1.7491360902786255, Val Loss: 1.8931317329406738\n",
      "Step: 3250, Train Loss: 1.7493382692337036, Val Loss: 1.9033743143081665\n",
      "Step: 3260, Train Loss: 1.7425744533538818, Val Loss: 1.8931677341461182\n",
      "Step: 3270, Train Loss: 1.7370507717132568, Val Loss: 1.8861629962921143\n",
      "Step: 3280, Train Loss: 1.7457939386367798, Val Loss: 1.9032325744628906\n",
      "Step: 3290, Train Loss: 1.7315798997879028, Val Loss: 1.8854858875274658\n",
      "Step: 3300, Train Loss: 1.7341852188110352, Val Loss: 1.8820725679397583\n",
      "Step: 3310, Train Loss: 1.7402592897415161, Val Loss: 1.8840739727020264\n",
      "Step: 3320, Train Loss: 1.7347303628921509, Val Loss: 1.8645466566085815\n",
      "Step: 3330, Train Loss: 1.7448114156723022, Val Loss: 1.8853726387023926\n",
      "Step: 3340, Train Loss: 1.7233920097351074, Val Loss: 1.8891699314117432\n",
      "Step: 3350, Train Loss: 1.7332453727722168, Val Loss: 1.8766909837722778\n",
      "Step: 3360, Train Loss: 1.729263424873352, Val Loss: 1.8733597993850708\n",
      "Step: 3370, Train Loss: 1.7286245822906494, Val Loss: 1.8693660497665405\n",
      "Step: 3380, Train Loss: 1.7377771139144897, Val Loss: 1.8775184154510498\n",
      "Step: 3390, Train Loss: 1.7284126281738281, Val Loss: 1.8785805702209473\n",
      "Step: 3400, Train Loss: 1.7278403043746948, Val Loss: 1.8667569160461426\n",
      "Step: 3410, Train Loss: 1.7142895460128784, Val Loss: 1.8681129217147827\n",
      "Step: 3420, Train Loss: 1.7253456115722656, Val Loss: 1.8702718019485474\n",
      "Step: 3430, Train Loss: 1.7286858558654785, Val Loss: 1.8682196140289307\n",
      "Step: 3440, Train Loss: 1.715786337852478, Val Loss: 1.8687325716018677\n",
      "Step: 3450, Train Loss: 1.7315255403518677, Val Loss: 1.8758763074874878\n",
      "Step: 3460, Train Loss: 1.7157536745071411, Val Loss: 1.869388461112976\n",
      "Step: 3470, Train Loss: 1.7247658967971802, Val Loss: 1.8849704265594482\n",
      "Step: 3480, Train Loss: 1.7243074178695679, Val Loss: 1.8785529136657715\n",
      "Step: 3490, Train Loss: 1.7191842794418335, Val Loss: 1.8717565536499023\n",
      "Step: 3500, Train Loss: 1.7163479328155518, Val Loss: 1.8658499717712402\n",
      "Step: 3510, Train Loss: 1.717769980430603, Val Loss: 1.875074863433838\n",
      "Step: 3520, Train Loss: 1.7180322408676147, Val Loss: 1.8725907802581787\n",
      "Step: 3530, Train Loss: 1.7151576280593872, Val Loss: 1.852636456489563\n",
      "Step: 3540, Train Loss: 1.7149397134780884, Val Loss: 1.8569599390029907\n",
      "Step: 3550, Train Loss: 1.7146817445755005, Val Loss: 1.8586297035217285\n",
      "Step: 3560, Train Loss: 1.7232115268707275, Val Loss: 1.8505882024765015\n",
      "Step: 3570, Train Loss: 1.719749093055725, Val Loss: 1.8743605613708496\n",
      "Step: 3580, Train Loss: 1.7209123373031616, Val Loss: 1.8576337099075317\n",
      "Step: 3590, Train Loss: 1.714769721031189, Val Loss: 1.869682788848877\n",
      "Step: 3600, Train Loss: 1.716829776763916, Val Loss: 1.85933256149292\n",
      "Step: 3610, Train Loss: 1.7195794582366943, Val Loss: 1.8651460409164429\n",
      "Step: 3620, Train Loss: 1.7117516994476318, Val Loss: 1.8573830127716064\n",
      "Step: 3630, Train Loss: 1.7183817625045776, Val Loss: 1.8548035621643066\n",
      "Step: 3640, Train Loss: 1.7171438932418823, Val Loss: 1.867437720298767\n",
      "Step: 3650, Train Loss: 1.707414984703064, Val Loss: 1.8585875034332275\n",
      "Step: 3660, Train Loss: 1.7193019390106201, Val Loss: 1.8655978441238403\n",
      "Step: 3670, Train Loss: 1.6998262405395508, Val Loss: 1.8647695779800415\n",
      "Step: 3680, Train Loss: 1.709945797920227, Val Loss: 1.8555095195770264\n",
      "Step: 3690, Train Loss: 1.715157151222229, Val Loss: 1.8617172241210938\n",
      "Step: 3700, Train Loss: 1.7176769971847534, Val Loss: 1.8658405542373657\n",
      "Step: 3710, Train Loss: 1.7054270505905151, Val Loss: 1.8677505254745483\n",
      "Step: 3720, Train Loss: 1.7089831829071045, Val Loss: 1.8682347536087036\n",
      "Step: 3730, Train Loss: 1.7086975574493408, Val Loss: 1.8576016426086426\n",
      "Step: 3740, Train Loss: 1.713417649269104, Val Loss: 1.8660906553268433\n",
      "Step: 3750, Train Loss: 1.7073256969451904, Val Loss: 1.8540844917297363\n",
      "Step: 3760, Train Loss: 1.7050468921661377, Val Loss: 1.855116844177246\n",
      "Step: 3770, Train Loss: 1.703385353088379, Val Loss: 1.8629313707351685\n",
      "Step: 3780, Train Loss: 1.7089046239852905, Val Loss: 1.8459080457687378\n",
      "Step: 3790, Train Loss: 1.699273943901062, Val Loss: 1.8558682203292847\n",
      "Step: 3800, Train Loss: 1.7037408351898193, Val Loss: 1.8579461574554443\n",
      "Step: 3810, Train Loss: 1.7001755237579346, Val Loss: 1.8542813062667847\n",
      "Step: 3820, Train Loss: 1.7122327089309692, Val Loss: 1.8464053869247437\n",
      "Step: 3830, Train Loss: 1.7043451070785522, Val Loss: 1.8496791124343872\n",
      "Step: 3840, Train Loss: 1.7019702196121216, Val Loss: 1.8412355184555054\n",
      "Step: 3850, Train Loss: 1.7104414701461792, Val Loss: 1.8354017734527588\n",
      "Step: 3860, Train Loss: 1.700099229812622, Val Loss: 1.8413140773773193\n",
      "Step: 3870, Train Loss: 1.7090942859649658, Val Loss: 1.8476165533065796\n",
      "Step: 3880, Train Loss: 1.7095122337341309, Val Loss: 1.8554404973983765\n",
      "Step: 3890, Train Loss: 1.7023800611495972, Val Loss: 1.8531904220581055\n",
      "Step: 3900, Train Loss: 1.6935057640075684, Val Loss: 1.8557640314102173\n",
      "Step: 3910, Train Loss: 1.7071702480316162, Val Loss: 1.8513171672821045\n",
      "Step: 3920, Train Loss: 1.690068244934082, Val Loss: 1.8715230226516724\n",
      "Step: 3930, Train Loss: 1.6953679323196411, Val Loss: 1.853468418121338\n",
      "Step: 3940, Train Loss: 1.6822689771652222, Val Loss: 1.8534414768218994\n",
      "Step: 3950, Train Loss: 1.7087182998657227, Val Loss: 1.8651788234710693\n",
      "Step: 3960, Train Loss: 1.6946138143539429, Val Loss: 1.8422045707702637\n",
      "Step: 3970, Train Loss: 1.7085009813308716, Val Loss: 1.851565957069397\n",
      "Step: 3980, Train Loss: 1.689444899559021, Val Loss: 1.8576217889785767\n",
      "Step: 3990, Train Loss: 1.6889230012893677, Val Loss: 1.8495380878448486\n",
      "Step: 4000, Train Loss: 1.6983778476715088, Val Loss: 1.8551803827285767\n",
      "Step: 4010, Train Loss: 1.699529767036438, Val Loss: 1.8610097169876099\n",
      "Step: 4020, Train Loss: 1.7011033296585083, Val Loss: 1.8511215448379517\n",
      "Step: 4030, Train Loss: 1.6989316940307617, Val Loss: 1.8461596965789795\n",
      "Step: 4040, Train Loss: 1.6972736120224, Val Loss: 1.8544456958770752\n",
      "Step: 4050, Train Loss: 1.6895931959152222, Val Loss: 1.8366754055023193\n",
      "Step: 4060, Train Loss: 1.685336709022522, Val Loss: 1.8447657823562622\n",
      "Step: 4070, Train Loss: 1.6802949905395508, Val Loss: 1.8499478101730347\n",
      "Step: 4080, Train Loss: 1.6850073337554932, Val Loss: 1.8500699996948242\n",
      "Step: 4090, Train Loss: 1.6928203105926514, Val Loss: 1.839074730873108\n",
      "Step: 4100, Train Loss: 1.6786648035049438, Val Loss: 1.852408766746521\n",
      "Step: 4110, Train Loss: 1.6879875659942627, Val Loss: 1.8601573705673218\n",
      "Step: 4120, Train Loss: 1.702918291091919, Val Loss: 1.8648333549499512\n",
      "Step: 4130, Train Loss: 1.684281587600708, Val Loss: 1.8484801054000854\n",
      "Step: 4140, Train Loss: 1.6991082429885864, Val Loss: 1.856540322303772\n",
      "Step: 4150, Train Loss: 1.6790369749069214, Val Loss: 1.8485209941864014\n",
      "Step: 4160, Train Loss: 1.6881545782089233, Val Loss: 1.8520879745483398\n",
      "Step: 4170, Train Loss: 1.686334252357483, Val Loss: 1.8526676893234253\n",
      "Step: 4180, Train Loss: 1.691693902015686, Val Loss: 1.8430899381637573\n",
      "Step: 4190, Train Loss: 1.6851006746292114, Val Loss: 1.856481671333313\n",
      "Step: 4200, Train Loss: 1.681692361831665, Val Loss: 1.8652961254119873\n",
      "Step: 4210, Train Loss: 1.6843785047531128, Val Loss: 1.8413326740264893\n",
      "Step: 4220, Train Loss: 1.6780145168304443, Val Loss: 1.8412067890167236\n",
      "Step: 4230, Train Loss: 1.6867918968200684, Val Loss: 1.8476852178573608\n",
      "Step: 4240, Train Loss: 1.6745144128799438, Val Loss: 1.8504730463027954\n",
      "Step: 4250, Train Loss: 1.6801438331604004, Val Loss: 1.8349233865737915\n",
      "Step: 4260, Train Loss: 1.6754405498504639, Val Loss: 1.847212314605713\n",
      "Step: 4270, Train Loss: 1.6858162879943848, Val Loss: 1.852388858795166\n",
      "Step: 4280, Train Loss: 1.6813408136367798, Val Loss: 1.8539178371429443\n",
      "Step: 4290, Train Loss: 1.6878914833068848, Val Loss: 1.846238374710083\n",
      "Step: 4300, Train Loss: 1.6925359964370728, Val Loss: 1.8469489812850952\n",
      "Step: 4310, Train Loss: 1.6781830787658691, Val Loss: 1.8438668251037598\n",
      "Step: 4320, Train Loss: 1.6926090717315674, Val Loss: 1.8375533819198608\n",
      "Step: 4330, Train Loss: 1.6886460781097412, Val Loss: 1.8528043031692505\n",
      "Step: 4340, Train Loss: 1.678674340248108, Val Loss: 1.8286566734313965\n",
      "Step: 4350, Train Loss: 1.6815730333328247, Val Loss: 1.8325223922729492\n",
      "Step: 4360, Train Loss: 1.6706106662750244, Val Loss: 1.8411602973937988\n",
      "Step: 4370, Train Loss: 1.6654365062713623, Val Loss: 1.8221858739852905\n",
      "Step: 4380, Train Loss: 1.6794440746307373, Val Loss: 1.8375945091247559\n",
      "Step: 4390, Train Loss: 1.6779223680496216, Val Loss: 1.8396635055541992\n",
      "Step: 4400, Train Loss: 1.6777064800262451, Val Loss: 1.824388027191162\n",
      "Step: 4410, Train Loss: 1.6684893369674683, Val Loss: 1.8280810117721558\n",
      "Step: 4420, Train Loss: 1.6783642768859863, Val Loss: 1.8435429334640503\n",
      "Step: 4430, Train Loss: 1.6919060945510864, Val Loss: 1.8416807651519775\n",
      "Step: 4440, Train Loss: 1.6676034927368164, Val Loss: 1.8362302780151367\n",
      "Step: 4450, Train Loss: 1.6783018112182617, Val Loss: 1.8356046676635742\n",
      "Step: 4460, Train Loss: 1.6723837852478027, Val Loss: 1.8375613689422607\n",
      "Step: 4470, Train Loss: 1.6735016107559204, Val Loss: 1.8326114416122437\n",
      "Step: 4480, Train Loss: 1.673417329788208, Val Loss: 1.8394616842269897\n",
      "Step: 4490, Train Loss: 1.6697115898132324, Val Loss: 1.8223284482955933\n",
      "Step: 4500, Train Loss: 1.6685214042663574, Val Loss: 1.8368654251098633\n",
      "Step: 4510, Train Loss: 1.666783332824707, Val Loss: 1.8288036584854126\n",
      "Step: 4520, Train Loss: 1.6727664470672607, Val Loss: 1.8252886533737183\n",
      "Step: 4530, Train Loss: 1.6757560968399048, Val Loss: 1.8247060775756836\n",
      "Step: 4540, Train Loss: 1.662959337234497, Val Loss: 1.8265044689178467\n",
      "Step: 4550, Train Loss: 1.6773419380187988, Val Loss: 1.8374806642532349\n",
      "Step: 4560, Train Loss: 1.657942771911621, Val Loss: 1.8203270435333252\n",
      "Step: 4570, Train Loss: 1.6632705926895142, Val Loss: 1.833093523979187\n",
      "Step: 4580, Train Loss: 1.669166922569275, Val Loss: 1.8244433403015137\n",
      "Step: 4590, Train Loss: 1.6677939891815186, Val Loss: 1.8193641901016235\n",
      "Step: 4600, Train Loss: 1.6661460399627686, Val Loss: 1.8298770189285278\n",
      "Step: 4610, Train Loss: 1.6733832359313965, Val Loss: 1.8413547277450562\n",
      "Step: 4620, Train Loss: 1.6677743196487427, Val Loss: 1.8228954076766968\n",
      "Step: 4630, Train Loss: 1.6794792413711548, Val Loss: 1.8386074304580688\n",
      "Step: 4640, Train Loss: 1.6446490287780762, Val Loss: 1.8166521787643433\n",
      "Step: 4650, Train Loss: 1.658497929573059, Val Loss: 1.8181366920471191\n",
      "Step: 4660, Train Loss: 1.6713647842407227, Val Loss: 1.828739047050476\n",
      "Step: 4670, Train Loss: 1.6723541021347046, Val Loss: 1.8325777053833008\n",
      "Step: 4680, Train Loss: 1.6664313077926636, Val Loss: 1.818920612335205\n",
      "Step: 4690, Train Loss: 1.659957766532898, Val Loss: 1.8200567960739136\n",
      "Step: 4700, Train Loss: 1.6540484428405762, Val Loss: 1.829473614692688\n",
      "Step: 4710, Train Loss: 1.6764956712722778, Val Loss: 1.8277157545089722\n",
      "Step: 4720, Train Loss: 1.6672245264053345, Val Loss: 1.8224354982376099\n",
      "Step: 4730, Train Loss: 1.6621181964874268, Val Loss: 1.827470302581787\n",
      "Step: 4740, Train Loss: 1.6616934537887573, Val Loss: 1.8030630350112915\n",
      "Step: 4750, Train Loss: 1.6716749668121338, Val Loss: 1.8358213901519775\n",
      "Step: 4760, Train Loss: 1.6646714210510254, Val Loss: 1.8257193565368652\n",
      "Step: 4770, Train Loss: 1.6574325561523438, Val Loss: 1.8129868507385254\n",
      "Step: 4780, Train Loss: 1.6510969400405884, Val Loss: 1.824001669883728\n",
      "Step: 4790, Train Loss: 1.6584864854812622, Val Loss: 1.8374754190444946\n",
      "Step: 4800, Train Loss: 1.650862455368042, Val Loss: 1.8297702074050903\n",
      "Step: 4810, Train Loss: 1.6561503410339355, Val Loss: 1.810306429862976\n",
      "Step: 4820, Train Loss: 1.6574711799621582, Val Loss: 1.827012062072754\n",
      "Step: 4830, Train Loss: 1.654147982597351, Val Loss: 1.814876914024353\n",
      "Step: 4840, Train Loss: 1.6507328748703003, Val Loss: 1.8205417394638062\n",
      "Step: 4850, Train Loss: 1.6670268774032593, Val Loss: 1.8319923877716064\n",
      "Step: 4860, Train Loss: 1.6541916131973267, Val Loss: 1.8148113489151\n",
      "Step: 4870, Train Loss: 1.6592581272125244, Val Loss: 1.83039128780365\n",
      "Step: 4880, Train Loss: 1.6568403244018555, Val Loss: 1.809340238571167\n",
      "Step: 4890, Train Loss: 1.6584466695785522, Val Loss: 1.8297747373580933\n",
      "Step: 4900, Train Loss: 1.6455763578414917, Val Loss: 1.815825343132019\n",
      "Step: 4910, Train Loss: 1.6553746461868286, Val Loss: 1.831947922706604\n",
      "Step: 4920, Train Loss: 1.6606639623641968, Val Loss: 1.819434404373169\n",
      "Step: 4930, Train Loss: 1.6503770351409912, Val Loss: 1.8217015266418457\n",
      "Step: 4940, Train Loss: 1.6553646326065063, Val Loss: 1.8194525241851807\n",
      "Step: 4950, Train Loss: 1.654128909111023, Val Loss: 1.8135097026824951\n",
      "Step: 4960, Train Loss: 1.6579419374465942, Val Loss: 1.808821678161621\n",
      "Step: 4970, Train Loss: 1.6637996435165405, Val Loss: 1.8254643678665161\n",
      "Step: 4980, Train Loss: 1.651442289352417, Val Loss: 1.8202388286590576\n",
      "Step: 4990, Train Loss: 1.663093090057373, Val Loss: 1.8129525184631348\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train Loss: 4.172012805938721, Val Loss: 4.174652576446533\n",
      "Step: 10, Train Loss: 3.4045569896698, Val Loss: 3.437119483947754\n",
      "Step: 20, Train Loss: 3.225302219390869, Val Loss: 3.2651376724243164\n",
      "Step: 30, Train Loss: 3.0958411693573, Val Loss: 3.1236846446990967\n",
      "Step: 40, Train Loss: 2.9855661392211914, Val Loss: 2.9988417625427246\n",
      "Step: 50, Train Loss: 2.8883626461029053, Val Loss: 2.9178216457366943\n",
      "Step: 60, Train Loss: 2.823512077331543, Val Loss: 2.857663869857788\n",
      "Step: 70, Train Loss: 2.7628214359283447, Val Loss: 2.7943880558013916\n",
      "Step: 80, Train Loss: 2.732734441757202, Val Loss: 2.751629114151001\n",
      "Step: 90, Train Loss: 2.6920905113220215, Val Loss: 2.6991260051727295\n",
      "Step: 100, Train Loss: 2.6488089561462402, Val Loss: 2.6591904163360596\n",
      "Step: 110, Train Loss: 2.6442954540252686, Val Loss: 2.6444790363311768\n",
      "Step: 120, Train Loss: 2.6173171997070312, Val Loss: 2.619143009185791\n",
      "Step: 130, Train Loss: 2.5939199924468994, Val Loss: 2.587609052658081\n",
      "Step: 140, Train Loss: 2.570146083831787, Val Loss: 2.574490547180176\n",
      "Step: 150, Train Loss: 2.5535149574279785, Val Loss: 2.5647919178009033\n",
      "Step: 160, Train Loss: 2.5385661125183105, Val Loss: 2.5484158992767334\n",
      "Step: 170, Train Loss: 2.526869535446167, Val Loss: 2.5416126251220703\n",
      "Step: 180, Train Loss: 2.5226616859436035, Val Loss: 2.5293221473693848\n",
      "Step: 190, Train Loss: 2.503904342651367, Val Loss: 2.510132312774658\n",
      "Step: 200, Train Loss: 2.5023293495178223, Val Loss: 2.522465467453003\n",
      "Step: 210, Train Loss: 2.483059883117676, Val Loss: 2.499549627304077\n",
      "Step: 220, Train Loss: 2.4781229496002197, Val Loss: 2.4891674518585205\n",
      "Step: 230, Train Loss: 2.466978073120117, Val Loss: 2.4778707027435303\n",
      "Step: 240, Train Loss: 2.464528799057007, Val Loss: 2.468122959136963\n",
      "Step: 250, Train Loss: 2.458900213241577, Val Loss: 2.4752631187438965\n",
      "Step: 260, Train Loss: 2.4477193355560303, Val Loss: 2.457984209060669\n",
      "Step: 270, Train Loss: 2.4417638778686523, Val Loss: 2.4503347873687744\n",
      "Step: 280, Train Loss: 2.445662260055542, Val Loss: 2.4793148040771484\n",
      "Step: 290, Train Loss: 2.420441150665283, Val Loss: 2.42392635345459\n",
      "Step: 300, Train Loss: 2.414072036743164, Val Loss: 2.4228479862213135\n",
      "Step: 310, Train Loss: 2.4119668006896973, Val Loss: 2.4144933223724365\n",
      "Step: 320, Train Loss: 2.400134801864624, Val Loss: 2.4115734100341797\n",
      "Step: 330, Train Loss: 2.392932176589966, Val Loss: 2.410398006439209\n",
      "Step: 340, Train Loss: 2.388298273086548, Val Loss: 2.406517505645752\n",
      "Step: 350, Train Loss: 2.3829777240753174, Val Loss: 2.4031524658203125\n",
      "Step: 360, Train Loss: 2.369371175765991, Val Loss: 2.3811826705932617\n",
      "Step: 370, Train Loss: 2.3681066036224365, Val Loss: 2.3734679222106934\n",
      "Step: 380, Train Loss: 2.3550336360931396, Val Loss: 2.36564564704895\n",
      "Step: 390, Train Loss: 2.3493971824645996, Val Loss: 2.3668630123138428\n",
      "Step: 400, Train Loss: 2.3407044410705566, Val Loss: 2.365149736404419\n",
      "Step: 410, Train Loss: 2.337552070617676, Val Loss: 2.3539886474609375\n",
      "Step: 420, Train Loss: 2.338326930999756, Val Loss: 2.345217227935791\n",
      "Step: 430, Train Loss: 2.324213981628418, Val Loss: 2.3473360538482666\n",
      "Step: 440, Train Loss: 2.3181464672088623, Val Loss: 2.3483307361602783\n",
      "Step: 450, Train Loss: 2.316972017288208, Val Loss: 2.3410730361938477\n",
      "Step: 460, Train Loss: 2.317119598388672, Val Loss: 2.332014799118042\n",
      "Step: 470, Train Loss: 2.3062691688537598, Val Loss: 2.3140993118286133\n",
      "Step: 480, Train Loss: 2.3106143474578857, Val Loss: 2.3267598152160645\n",
      "Step: 490, Train Loss: 2.3033642768859863, Val Loss: 2.3122243881225586\n",
      "Step: 500, Train Loss: 2.2870562076568604, Val Loss: 2.3010544776916504\n",
      "Step: 510, Train Loss: 2.2821662425994873, Val Loss: 2.3048605918884277\n",
      "Step: 520, Train Loss: 2.2728700637817383, Val Loss: 2.292970657348633\n",
      "Step: 530, Train Loss: 2.269221305847168, Val Loss: 2.2946207523345947\n",
      "Step: 540, Train Loss: 2.2714226245880127, Val Loss: 2.287444829940796\n",
      "Step: 550, Train Loss: 2.253465175628662, Val Loss: 2.282735824584961\n",
      "Step: 560, Train Loss: 2.2617766857147217, Val Loss: 2.290656805038452\n",
      "Step: 570, Train Loss: 2.254586935043335, Val Loss: 2.2677528858184814\n",
      "Step: 580, Train Loss: 2.2523200511932373, Val Loss: 2.2766273021698\n",
      "Step: 590, Train Loss: 2.2452807426452637, Val Loss: 2.2646164894104004\n",
      "Step: 600, Train Loss: 2.233147621154785, Val Loss: 2.246835708618164\n",
      "Step: 610, Train Loss: 2.233222246170044, Val Loss: 2.244006395339966\n",
      "Step: 620, Train Loss: 2.235220193862915, Val Loss: 2.2541322708129883\n",
      "Step: 630, Train Loss: 2.22029185295105, Val Loss: 2.230558156967163\n",
      "Step: 640, Train Loss: 2.216831684112549, Val Loss: 2.239928722381592\n",
      "Step: 650, Train Loss: 2.218501329421997, Val Loss: 2.221231460571289\n",
      "Step: 660, Train Loss: 2.204157590866089, Val Loss: 2.2222354412078857\n",
      "Step: 670, Train Loss: 2.206188678741455, Val Loss: 2.225964307785034\n",
      "Step: 680, Train Loss: 2.1967105865478516, Val Loss: 2.2236738204956055\n",
      "Step: 690, Train Loss: 2.1961300373077393, Val Loss: 2.218252658843994\n",
      "Step: 700, Train Loss: 2.1787497997283936, Val Loss: 2.198421001434326\n",
      "Step: 710, Train Loss: 2.1773009300231934, Val Loss: 2.2078325748443604\n",
      "Step: 720, Train Loss: 2.1797378063201904, Val Loss: 2.193584680557251\n",
      "Step: 730, Train Loss: 2.1711313724517822, Val Loss: 2.1896047592163086\n",
      "Step: 740, Train Loss: 2.1835379600524902, Val Loss: 2.2024855613708496\n",
      "Step: 750, Train Loss: 2.1655848026275635, Val Loss: 2.1914620399475098\n",
      "Step: 760, Train Loss: 2.1724963188171387, Val Loss: 2.1956629753112793\n",
      "Step: 770, Train Loss: 2.1552622318267822, Val Loss: 2.1888489723205566\n",
      "Step: 780, Train Loss: 2.165175437927246, Val Loss: 2.1930501461029053\n",
      "Step: 790, Train Loss: 2.155477523803711, Val Loss: 2.187682628631592\n",
      "Step: 800, Train Loss: 2.147706985473633, Val Loss: 2.1777541637420654\n",
      "Step: 810, Train Loss: 2.1521518230438232, Val Loss: 2.170558214187622\n",
      "Step: 820, Train Loss: 2.137050151824951, Val Loss: 2.1681337356567383\n",
      "Step: 830, Train Loss: 2.139374017715454, Val Loss: 2.1646037101745605\n",
      "Step: 840, Train Loss: 2.1296579837799072, Val Loss: 2.1543357372283936\n",
      "Step: 850, Train Loss: 2.1243181228637695, Val Loss: 2.158087968826294\n",
      "Step: 860, Train Loss: 2.116663932800293, Val Loss: 2.1615307331085205\n",
      "Step: 870, Train Loss: 2.131713628768921, Val Loss: 2.1518049240112305\n",
      "Step: 880, Train Loss: 2.1270177364349365, Val Loss: 2.150175094604492\n",
      "Step: 890, Train Loss: 2.120729923248291, Val Loss: 2.1465864181518555\n",
      "Step: 900, Train Loss: 2.1166677474975586, Val Loss: 2.1515965461730957\n",
      "Step: 910, Train Loss: 2.119103193283081, Val Loss: 2.1451804637908936\n",
      "Step: 920, Train Loss: 2.1066653728485107, Val Loss: 2.139279365539551\n",
      "Step: 930, Train Loss: 2.10746431350708, Val Loss: 2.1408259868621826\n",
      "Step: 940, Train Loss: 2.1094770431518555, Val Loss: 2.129594564437866\n",
      "Step: 950, Train Loss: 2.106429100036621, Val Loss: 2.127408981323242\n",
      "Step: 960, Train Loss: 2.0971152782440186, Val Loss: 2.13140606880188\n",
      "Step: 970, Train Loss: 2.097012519836426, Val Loss: 2.1331491470336914\n",
      "Step: 980, Train Loss: 2.0795235633850098, Val Loss: 2.1183290481567383\n",
      "Step: 990, Train Loss: 2.083566188812256, Val Loss: 2.1192944049835205\n",
      "Step: 1000, Train Loss: 2.078672170639038, Val Loss: 2.110156536102295\n",
      "Step: 1010, Train Loss: 2.0585744380950928, Val Loss: 2.1138410568237305\n",
      "Step: 1020, Train Loss: 2.0605669021606445, Val Loss: 2.1053390502929688\n",
      "Step: 1030, Train Loss: 2.0572328567504883, Val Loss: 2.1147658824920654\n",
      "Step: 1040, Train Loss: 2.0619332790374756, Val Loss: 2.1107616424560547\n",
      "Step: 1050, Train Loss: 2.073183536529541, Val Loss: 2.1053526401519775\n",
      "Step: 1060, Train Loss: 2.060307741165161, Val Loss: 2.1117100715637207\n",
      "Step: 1070, Train Loss: 2.053922176361084, Val Loss: 2.1124649047851562\n",
      "Step: 1080, Train Loss: 2.06733775138855, Val Loss: 2.108832359313965\n",
      "Step: 1090, Train Loss: 2.0537874698638916, Val Loss: 2.0947511196136475\n",
      "Step: 1100, Train Loss: 2.0504305362701416, Val Loss: 2.090428352355957\n",
      "Step: 1110, Train Loss: 2.048126459121704, Val Loss: 2.107797861099243\n",
      "Step: 1120, Train Loss: 2.0486016273498535, Val Loss: 2.0866994857788086\n",
      "Step: 1130, Train Loss: 2.0352623462677, Val Loss: 2.097151279449463\n",
      "Step: 1140, Train Loss: 2.0410964488983154, Val Loss: 2.0835764408111572\n",
      "Step: 1150, Train Loss: 2.0331220626831055, Val Loss: 2.078894853591919\n",
      "Step: 1160, Train Loss: 2.032142400741577, Val Loss: 2.0834908485412598\n",
      "Step: 1170, Train Loss: 2.0350751876831055, Val Loss: 2.068214178085327\n",
      "Step: 1180, Train Loss: 2.041377305984497, Val Loss: 2.0809173583984375\n",
      "Step: 1190, Train Loss: 2.0285511016845703, Val Loss: 2.089625597000122\n",
      "Step: 1200, Train Loss: 2.0162863731384277, Val Loss: 2.0779192447662354\n",
      "Step: 1210, Train Loss: 2.017214059829712, Val Loss: 2.062251329421997\n",
      "Step: 1220, Train Loss: 2.0226335525512695, Val Loss: 2.0676867961883545\n",
      "Step: 1230, Train Loss: 2.0253970623016357, Val Loss: 2.0645217895507812\n",
      "Step: 1240, Train Loss: 2.023625373840332, Val Loss: 2.0716402530670166\n",
      "Step: 1250, Train Loss: 2.0062646865844727, Val Loss: 2.061488389968872\n",
      "Step: 1260, Train Loss: 2.0067288875579834, Val Loss: 2.059799909591675\n",
      "Step: 1270, Train Loss: 2.000985622406006, Val Loss: 2.0508034229278564\n",
      "Step: 1280, Train Loss: 2.0023181438446045, Val Loss: 2.061389684677124\n",
      "Step: 1290, Train Loss: 2.0084540843963623, Val Loss: 2.06523060798645\n",
      "Step: 1300, Train Loss: 2.002707004547119, Val Loss: 2.067722797393799\n",
      "Step: 1310, Train Loss: 1.988524317741394, Val Loss: 2.0436525344848633\n",
      "Step: 1320, Train Loss: 1.9961919784545898, Val Loss: 2.044856309890747\n",
      "Step: 1330, Train Loss: 1.9944499731063843, Val Loss: 2.0572197437286377\n",
      "Step: 1340, Train Loss: 1.980252742767334, Val Loss: 2.0380687713623047\n",
      "Step: 1350, Train Loss: 1.9963628053665161, Val Loss: 2.052109718322754\n",
      "Step: 1360, Train Loss: 1.9772140979766846, Val Loss: 2.0397531986236572\n",
      "Step: 1370, Train Loss: 1.9785270690917969, Val Loss: 2.0415985584259033\n",
      "Step: 1380, Train Loss: 1.985169529914856, Val Loss: 2.053772449493408\n",
      "Step: 1390, Train Loss: 1.9916598796844482, Val Loss: 2.0529160499572754\n",
      "Step: 1400, Train Loss: 1.9682724475860596, Val Loss: 2.048506736755371\n",
      "Step: 1410, Train Loss: 1.9741058349609375, Val Loss: 2.0375709533691406\n",
      "Step: 1420, Train Loss: 1.9657659530639648, Val Loss: 2.033592700958252\n",
      "Step: 1430, Train Loss: 1.9669796228408813, Val Loss: 2.033165693283081\n",
      "Step: 1440, Train Loss: 1.9644896984100342, Val Loss: 2.030200242996216\n",
      "Step: 1450, Train Loss: 1.9674851894378662, Val Loss: 2.0418012142181396\n",
      "Step: 1460, Train Loss: 1.9661418199539185, Val Loss: 2.024883508682251\n",
      "Step: 1470, Train Loss: 1.9652190208435059, Val Loss: 2.0233309268951416\n",
      "Step: 1480, Train Loss: 1.9658509492874146, Val Loss: 2.0363106727600098\n",
      "Step: 1490, Train Loss: 1.9587634801864624, Val Loss: 2.03802490234375\n",
      "Step: 1500, Train Loss: 1.953336477279663, Val Loss: 2.044049024581909\n",
      "Step: 1510, Train Loss: 1.9549221992492676, Val Loss: 2.027247667312622\n",
      "Step: 1520, Train Loss: 1.9646029472351074, Val Loss: 2.0280649662017822\n",
      "Step: 1530, Train Loss: 1.949479103088379, Val Loss: 2.0280959606170654\n",
      "Step: 1540, Train Loss: 1.9487162828445435, Val Loss: 2.017230272293091\n",
      "Step: 1550, Train Loss: 1.9467653036117554, Val Loss: 2.007293462753296\n",
      "Step: 1560, Train Loss: 1.93172287940979, Val Loss: 2.0086257457733154\n",
      "Step: 1570, Train Loss: 1.9398713111877441, Val Loss: 2.0163700580596924\n",
      "Step: 1580, Train Loss: 1.9382013082504272, Val Loss: 2.012444257736206\n",
      "Step: 1590, Train Loss: 1.9377336502075195, Val Loss: 2.0157647132873535\n",
      "Step: 1600, Train Loss: 1.937351107597351, Val Loss: 2.0055229663848877\n",
      "Step: 1610, Train Loss: 1.9211585521697998, Val Loss: 2.008899211883545\n",
      "Step: 1620, Train Loss: 1.9318132400512695, Val Loss: 1.9953796863555908\n",
      "Step: 1630, Train Loss: 1.9256439208984375, Val Loss: 2.0001285076141357\n",
      "Step: 1640, Train Loss: 1.9262737035751343, Val Loss: 2.008251428604126\n",
      "Step: 1650, Train Loss: 1.9207186698913574, Val Loss: 2.0079715251922607\n",
      "Step: 1660, Train Loss: 1.926692008972168, Val Loss: 2.0038857460021973\n",
      "Step: 1670, Train Loss: 1.9231857061386108, Val Loss: 2.0025482177734375\n",
      "Step: 1680, Train Loss: 1.9184646606445312, Val Loss: 2.0015337467193604\n",
      "Step: 1690, Train Loss: 1.915736436843872, Val Loss: 2.0106847286224365\n",
      "Step: 1700, Train Loss: 1.9268771409988403, Val Loss: 2.010122776031494\n",
      "Step: 1710, Train Loss: 1.9119269847869873, Val Loss: 2.0076427459716797\n",
      "Step: 1720, Train Loss: 1.9152568578720093, Val Loss: 2.0192477703094482\n",
      "Step: 1730, Train Loss: 1.9181923866271973, Val Loss: 2.004596471786499\n",
      "Step: 1740, Train Loss: 1.917752981185913, Val Loss: 1.988869071006775\n",
      "Step: 1750, Train Loss: 1.9065043926239014, Val Loss: 1.9967563152313232\n",
      "Step: 1760, Train Loss: 1.8976207971572876, Val Loss: 2.0003998279571533\n",
      "Step: 1770, Train Loss: 1.9043039083480835, Val Loss: 1.9879437685012817\n",
      "Step: 1780, Train Loss: 1.9032111167907715, Val Loss: 1.9857902526855469\n",
      "Step: 1790, Train Loss: 1.901215672492981, Val Loss: 1.9954532384872437\n",
      "Step: 1800, Train Loss: 1.9028902053833008, Val Loss: 1.9801418781280518\n",
      "Step: 1810, Train Loss: 1.8907629251480103, Val Loss: 1.9863603115081787\n",
      "Step: 1820, Train Loss: 1.8913177251815796, Val Loss: 1.992801547050476\n",
      "Step: 1830, Train Loss: 1.897312879562378, Val Loss: 1.992436408996582\n",
      "Step: 1840, Train Loss: 1.8946460485458374, Val Loss: 1.9867138862609863\n",
      "Step: 1850, Train Loss: 1.8899314403533936, Val Loss: 1.9841281175613403\n",
      "Step: 1860, Train Loss: 1.8901599645614624, Val Loss: 1.9833381175994873\n",
      "Step: 1870, Train Loss: 1.8758763074874878, Val Loss: 1.987542986869812\n",
      "Step: 1880, Train Loss: 1.883558988571167, Val Loss: 1.9783962965011597\n",
      "Step: 1890, Train Loss: 1.8780900239944458, Val Loss: 1.9714181423187256\n",
      "Step: 1900, Train Loss: 1.874087929725647, Val Loss: 1.9882856607437134\n",
      "Step: 1910, Train Loss: 1.8848720788955688, Val Loss: 1.975854754447937\n",
      "Step: 1920, Train Loss: 1.8838906288146973, Val Loss: 1.979090929031372\n",
      "Step: 1930, Train Loss: 1.8790369033813477, Val Loss: 1.9852361679077148\n",
      "Step: 1940, Train Loss: 1.8635836839675903, Val Loss: 1.9813799858093262\n",
      "Step: 1950, Train Loss: 1.8663437366485596, Val Loss: 1.9755929708480835\n",
      "Step: 1960, Train Loss: 1.8721795082092285, Val Loss: 1.9905754327774048\n",
      "Step: 1970, Train Loss: 1.869539499282837, Val Loss: 1.9689898490905762\n",
      "Step: 1980, Train Loss: 1.8656518459320068, Val Loss: 1.9662315845489502\n",
      "Step: 1990, Train Loss: 1.859743356704712, Val Loss: 1.9746862649917603\n",
      "Step: 2000, Train Loss: 1.8731907606124878, Val Loss: 1.977137804031372\n",
      "Step: 2010, Train Loss: 1.852771282196045, Val Loss: 1.9810196161270142\n",
      "Step: 2020, Train Loss: 1.85813307762146, Val Loss: 1.9668792486190796\n",
      "Step: 2030, Train Loss: 1.8539907932281494, Val Loss: 1.9640603065490723\n",
      "Step: 2040, Train Loss: 1.8545773029327393, Val Loss: 1.9707200527191162\n",
      "Step: 2050, Train Loss: 1.851202130317688, Val Loss: 1.9716160297393799\n",
      "Step: 2060, Train Loss: 1.8474732637405396, Val Loss: 1.95820951461792\n",
      "Step: 2070, Train Loss: 1.8535549640655518, Val Loss: 1.9736802577972412\n",
      "Step: 2080, Train Loss: 1.8525468111038208, Val Loss: 1.974202275276184\n",
      "Step: 2090, Train Loss: 1.853691816329956, Val Loss: 1.9647337198257446\n",
      "Step: 2100, Train Loss: 1.8377941846847534, Val Loss: 1.9531205892562866\n",
      "Step: 2110, Train Loss: 1.8549695014953613, Val Loss: 1.9698455333709717\n",
      "Step: 2120, Train Loss: 1.8439334630966187, Val Loss: 1.966398000717163\n",
      "Step: 2130, Train Loss: 1.850447654724121, Val Loss: 1.9737286567687988\n",
      "Step: 2140, Train Loss: 1.8480137586593628, Val Loss: 1.9588629007339478\n",
      "Step: 2150, Train Loss: 1.8300542831420898, Val Loss: 1.9491722583770752\n",
      "Step: 2160, Train Loss: 1.8487576246261597, Val Loss: 1.9739875793457031\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training loop\n",
    "def train_model(n_embd, n_head, n_layer):\n",
    "    global model\n",
    "    model = LanguageModel().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for step in range(max_iters):\n",
    "        X, Y = get_batch('train')\n",
    "        logits, loss = model(X, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            train_loss = losses['train']\n",
    "            val_loss = losses['val']\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            print(f'Step: {step}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "    plt.plot(range(0, max_iters, eval_interval), train_losses, label=f'n_embd={n_embd}, n_head={n_head}, n_layer={n_layer}')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title('Perplexity over Training Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run training loop with different hyperparameters\n",
    "train_model(64, 4, 4)\n",
    "train_model(128, 4, 4)\n",
    "train_model(64, 8, 4)\n",
    "train_model(64, 4, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a69f03-7927-412e-a7cb-afd90d348850",
   "metadata": {},
   "source": [
    "# Note\n",
    "I was not able to run the entire of code because the limit time and number of cpu I can select. I provided my code as required above, and some prelim results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience/conda-2023-10-04",
   "language": "python",
   "name": "conda-2023-10-03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
